,question,summary,link,answers
0,,,,
1,Dump a NumPy array into a csv file,Is there a way to dump a NumPy array into a CSV file? I have a 2D NumPy array and need to dump it in human-readable format.,https://stackoverflow.com/questions/6081008/dump-a-numpy-array-into-a-csv-file,"['Is there a way to dump a NumPy array into a CSV file? I have a 2D NumPy array and need to dump it in human-readable format.', 'numpy.savetxt saves an array to a text file.', ""You can use pandas. It does take some extra memory so it's not always possible, but it's very fast and easy to use."", 'tofile is a convenient function to do this:', 'Writing record arrays as CSV files with headers requires a bit more work.', 'As already discussed, the best way to dump the array into a CSV file is by using .savetxt(...)method. However, there are certain things we should know to do it properly.', 'I believe you can also accomplish this quite simply as follows:', 'if you want to write in column:', 'In Python we use csv.writer() module to write data into csv files. This module is similar to the csv.reader() module.', 'If you want to save your numpy array (e.g. your_array = np.array([[1,2],[3,4]])) to one cell, you could convert it first with your_array.tolist().', 'You can also do it with pure python without using any modules.']"
2,How can the Euclidean distance be calculated with NumPy?,"I have two points in 3D:
(xa, ya, za)
(xb, yb, zb)

And I want to calculate the distance:
dist = sqrt((xa-xb)^2 + (ya-yb)^2 + (za-zb)^2)

What's the best way to do this with NumPy, or with Python in ...",https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy,"['I have two points in 3D:', 'Use numpy.linalg.norm:', ""There's a function for that in SciPy. It's called Euclidean."", ""For anyone interested in computing multiple distances at once, I've done a little comparison using perfplot (a small project of mine)."", 'I want to expound on the simple answer with various performance notes. np.linalg.norm will do perhaps more than you need:', 'Another instance of this problem solving method:', 'Starting Python 3.8, the math module directly provides the dist function, which returns the euclidean distance between two points (given as tuples or lists of coordinates):', ""It can be done like the following. I don't know how fast it is, but it's not using NumPy."", ""I find a 'dist' function in matplotlib.mlab, but I don't think it's handy enough."", 'A nice one-liner:', 'I like np.dot (dot product):', 'You can just subtract the vectors and then innerproduct.', 'Having a and b as you defined them, you can use also:', ""Here's some concise code for Euclidean distance in Python given two points represented as lists in Python."", ""With Python 3.8, it's very easy."", 'Since Python 3.8 the math module includes the function math.dist().\nSee here https://docs.python.org/3.8/library/math.html#math.dist.', 'Calculate the Euclidean distance for multidimensional space:', 'You can easily use the formula', ""Find difference of two matrices first. Then, apply element wise multiplication with numpy's multiply command. After then, find summation of the element wise multiplied new matrix. Finally, find square root of the summation."", 'You first change list to numpy array and do like this: print(np.linalg.norm(np.array(a) - np.array(b))). Second method directly from python list as: print(np.linalg.norm(np.subtract(a,b)))', 'The other answers work for floating point numbers, but do not correctly compute the distance for integer dtypes which are subject to overflow and underflow. Note that even scipy.distance.euclidean has this issue:']"
3,How do I get indices of N maximum values in a NumPy array?,"NumPy proposes a way to get the index of the maximum value of an array via np.argmax.

I would like a similar thing, but returning the indexes of the N maximum values.

For instance, if I have an ...",https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array,"['NumPy proposes a way to get the index of the maximum value of an array via np.argmax.', ""The simplest I've been able to come up with is:"", 'Newer NumPy versions (1.8 and up) have a function called argpartition for this. To get the indices of the four largest elements, do', 'Simpler yet:', 'Use:', ""If you happen to be working with a multidimensional array then you'll need to flatten and unravel the indices:"", ""If you don't care about the order of the K-th largest elements you can use argpartition, which should perform better than a full sort through argsort."", 'For multidimensional arrays you can use the axis keyword in order to apply the partitioning along the expected axis.', 'This will be faster than a full sort depending on the size of your original array and the size of your selection:', ""Method np.argpartition only returns the k largest indices, performs a local sort, and is faster than np.argsort(performing a full sort) when array is quite large. But the returned indices are NOT in ascending/descending order. Let's say with an example:"", 'Use:', 'Use:', 'bottleneck has a partial sort function, if the expense of sorting the entire array just to get the N largest values is too great.', 'The following is a very easy way to see the maximum elements and its positions. Here axis is the domain; axis = 0 means column wise maximum number and axis = 1 means row wise max number for the 2D case. And for higher dimensions it depends upon you.', 'I found it most intuitive to use np.unique.', 'I think the most time efficiency way is manually iterate through the array and keep a k-size min-heap, as other people have mentioned.', 'This code works for a numpy 2D matrix array:']"
4,Convert pandas dataframe to NumPy array,"I am interested in knowing how to convert a pandas dataframe into a NumPy array.

dataframe:

import numpy as np
import pandas as pd

index = [1, 2, 3, 4, 5, 6, 7]
a = [np.nan, np.nan, np.nan, 0.1, 0....",https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array,"['I am interested in knowing how to convert a pandas dataframe into a NumPy array.', 'To convert a pandas dataframe (df) to a numpy ndarray, use this code:', ""It's time to deprecate your usage of values and as_matrix()."", 'Note: The .as_matrix() method used in this answer is deprecated. Pandas 0.23.4 warns:', 'I would just chain the DataFrame.reset_index() and DataFrame.values functions to get the Numpy representation of the dataframe, including the index:', 'You can use the to_records method, but have to play around a bit with the dtypes if they are not what you want from the get go. In my case, having copied your DF from a string, the index type is string (represented by an object dtype in pandas):', ""It seems like df.to_records() will work for you. The exact feature you're looking for was requested and to_records pointed to as an alternative."", 'Try this:', 'Here is my approach to making a structure array from a pandas DataFrame.', 'A Simpler Way for Example DataFrame:', 'Two ways to convert the data-frame to its Numpy-array representation.', 'Just had a similar problem when exporting from dataframe to arcgis table and stumbled on a solution from usgs (https://my.usgs.gov/confluence/display/cdi/pandas.DataFrame+to+ArcGIS+Table).\nIn short your problem has a similar solution:', 'I went through the answers above. The ""as_matrix()"" method works but its obsolete now. For me, What worked was "".to_numpy()"".', ""Further to meteore's answer, I found the code"", 'Try this:', 'A simple way to convert dataframe to numpy array:']"
5,How to access the ith column of a NumPy multidimensional array?,"Suppose I have:

test = numpy.array([[1, 2], [3, 4], [5, 6]])
test[i] gets me ith line of the array (eg [1, 2]). How can I access the ith column? (eg [1, 3, 5]). Also, would this be an expensive ...",https://stackoverflow.com/questions/4455076/how-to-access-the-ith-column-of-a-numpy-multidimensional-array,"['Suppose I have:', 'Similarly,', 'And if you want to access more than one column at a time you could do:', ""this command gives you a row vector, if you just want to loop over it, it's fine, but if you want to hstack with some other array with dimension 3xN, you will have"", 'You could also transpose and return a row:', 'Although the question has been answered, let me mention some nuances.', 'To get several and indepent columns, just:', 'Then you can select the 2nd - 4th column this way:']"
6,What are the advantages of NumPy over regular Python lists?,"What are the advantages of NumPy over regular Python lists?

I have approximately 100 financial markets series, and I am going to create a cube array of 100x100x100 = 1 million cells. I will be ...",https://stackoverflow.com/questions/993984/what-are-the-advantages-of-numpy-over-regular-python-lists,"['What are the advantages of NumPy over regular Python lists?', ""NumPy's arrays are more compact than Python lists -- a list of lists as you describe, in Python, would take at least 20 MB or so, while a NumPy 3D array with single-precision floats in the cells would fit in 4 MB. Access in reading and writing items is also faster with NumPy."", 'NumPy is not just more efficient; it is also more convenient. You get a lot of vector and matrix operations for free, which sometimes allow one to avoid unnecessary work. And they are also efficiently implemented.', ""Alex mentioned memory efficiency, and Roberto mentions convenience, and these are both good points. For a few more ideas, I'll mention speed and functionality."", ""Here's a nice answer from the FAQ on the scipy.org website:"", 'All have highlighted almost all major differences between numpy array and python list, I will just brief them out here:']"
7,Is there a NumPy function to return the first index of something in an array?,"I know there is a method for a Python list to return the first index of something:

>>> l = [1, 2, 3]
>>> l.index(2)
1
Is there something like that for NumPy arrays?",https://stackoverflow.com/questions/432112/is-there-a-numpy-function-to-return-the-first-index-of-something-in-an-array,"['I know there is a method for a Python list to return the first index of something:', 'Yes, given an array, array, and a value, item to search for, you can use np.where as:', 'If you need the index of the first occurrence of only one value, you can use nonzero (or where, which amounts to the same thing in this case):', 'You can also convert a NumPy array to list in the air and get its index. For example,', 'Just to add a very performant and handy numba alternative based on np.ndenumerate to find the first index:', ""If you're going to use this as an index into something else, you can use boolean indices if the arrays are broadcastable; you don't need explicit indices.  The absolute simplest way to do this is to simply index based on a truth value."", 'l.index(x) returns the smallest i such that i is the index of the first occurrence of x in the list.', 'For one-dimensional sorted arrays, it would be much more simpler and efficient O(log(n)) to use numpy.searchsorted which returns a NumPy integer (position). For example,', 'To index on any criteria, you can so something like the following:', ""For 1D arrays, I'd recommend np.flatnonzero(array == value)[0], which is equivalent to both np.nonzero(array == value)[0][0] and np.where(array == value)[0][0] but avoids the ugliness of unboxing a 1-element tuple."", 'An alternative to selecting the first element from np.where() is to use a generator expression together with enumerate, such as:', 'There are lots of operations in NumPy that could perhaps be put together to accomplish this. This will return indices of elements equal to item:', 'The numpy_indexed package (disclaimer, I am its author) contains a vectorized equivalent of list.index for numpy.ndarray; that is:', 'Note: this is for python 2.7 version']"
8,What does -1 mean in numpy reshape?,"A numpy matrix can be reshaped into a vector using reshape function with parameter -1. But I don't know what -1 means here.

For example: 

a = numpy.matrix([[1, 2, 3, 4], [5, 6, 7, 8]])
b = numpy....",https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape,"[""A numpy matrix can be reshaped into a vector using reshape function with parameter -1. But I don't know what -1 means here."", ""The criterion to satisfy for providing the new shape is that 'The new shape should be compatible with the original shape'"", 'Used to reshape an array.', 'According to the documentation:', 'check the below link for more info.\nhttps://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html', 'It is fairly easy to understand. The ""-1"" stands for ""unknown dimension"" which can should be infered from another dimension. \nIn this case, if you set your matrix like this:', 'Long story short: you set some dimensions and let NumPy set the remaining(s).', 'It simply means that you are not sure about what number of rows  or columns you can give and you are asking numpy to suggest number of column or rows to get reshaped in.', 'The final outcome of the conversion is that the number of elements in the final array is same as that of the initial array or data frame.']"
9,How do I read CSV data into a record array in NumPy?,"I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's read.table(), read.delim(), and read.csv() family imports data to R's data frame?

...",https://stackoverflow.com/questions/3518778/how-do-i-read-csv-data-into-a-record-array-in-numpy,"[""I wonder if there is a direct way to import the contents of a CSV file into a record array, much in the way that R's read.table(), read.delim(), and read.csv() family imports data to R's data frame?"", ""You can use Numpy's genfromtxt() method to do so, by setting the delimiter kwarg to a comma."", 'I would recommend the read_csv function from the pandas library:', 'I timed the', 'You can also try recfromcsv() which can guess data types and return a properly formatted record array.', 'As I tried both ways using NumPy and Pandas, using pandas has a lot of advantages:', 'You can use this code to send CSV file data into an array:', 'I tried this:', 'Using numpy.loadtxt', 'This is the easiest way:', 'I would suggest using tables (pip3 install tables). You can save your .csv file to .h5 using pandas (pip3 install pandas),', 'This work as a charm...']"
10,How to count the occurrence of certain item in an ndarray?,"In Python, I have an ndarray y
that is printed as array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])

I'm trying to count how many 0s and how many 1s are there in this array. 

But when I type y.count(0) or ...",https://stackoverflow.com/questions/28663856/how-to-count-the-occurrence-of-certain-item-in-an-ndarray,"['In Python, I have an ndarray y\nthat is printed as array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])', 'Non-numpy way:', 'What about using numpy.count_nonzero, something like', ""Personally, I'd go for:\n(y == 0).sum() and (y == 1).sum()"", 'For your case you could also look into numpy.bincount', 'Convert your array y to list l and then do l.count(1) and l.count(0)', 'If you know that they are just 0 and 1:', ""If you know exactly which number you're looking for, you can use the following;"", 'Honestly I find it easiest to convert to a pandas Series or DataFrame:', 'What about len(y[y==0]) and len(y[y==1]) ?', 'No one suggested to use numpy.bincount(input, minlength) with minlength = np.size(input), but it seems to be a good solution, and definitely the fastest:', ""I'd use np.where:"", 'y.tolist().count(val)', 'Yet another simple solution might be to use numpy.count_nonzero():', 'To count the number of occurrences, you can use np.unique(array, return_counts=True):', 'take advantage of the methods offered by a Series:', 'A general and simple answer would be:', 'You can use dictionary comprehension to create a neat one-liner. More about dictionary comprehension can be found here', 'You have a special array with only 1 and 0 here. So a trick is to use', 'Try this:', 'If you are interested in the fastest execution, you know in advance which value(s) to look for, and your array is 1D, or you are otherwise interested in the result on the flattened array (in which case the input of the function should be np.flatten(arr) rather than just arr), then Numba is your friend:', 'It involves one more step, but a more flexible solution which would also work for 2d arrays and more complicated filters is to create a boolean mask and then use .sum() on the mask.', 'This can be done easily in the following method', 'Since your ndarray contains only 0 and 1, \nyou can use sum() to get the occurrence of 1s \nand len()-sum() to get the occurrence of 0s.', ""Just copied  Seppo Enarvi's comment here which deserves to be a proper answer"", ""If you don't want to use numpy or a collections module you can use a dictionary:"", 'For generic entries:', 'here I have something, through which you can count the number of occurrence of a particular number:\naccording to your code', 'if you are dealing with very large arrays using generators could be an option. The nice thing here it that this approach works fine for both arrays and lists and you dont need any additional package. Additionally, you are not using that much memory.', 'This funktion returns the number of occurences of a variable in an array:', 'Numpy has a module for this. Just a small hack. Put your input array as bins.']"
11,Simple Digit Recognition OCR in OpenCV-Python,"I am trying to implement a ""Digit Recognition OCR"" in OpenCV-Python (cv2). It is just for learning purposes. I would like to learn both KNearest and SVM features in OpenCV. 

I have 100 samples (i.e. ...",https://stackoverflow.com/questions/9413216/simple-digit-recognition-ocr-in-opencv-python,"['I am trying to implement a ""Digit Recognition OCR"" in OpenCV-Python (cv2). It is just for learning purposes. I would like to learn both KNearest and SVM features in OpenCV.', 'Well, I decided to workout myself on my question to solve above problem. What I wanted is to implement a simpl OCR using KNearest or SVM features in OpenCV. And below is what I did and how. ( it is just for learning how to use KNearest for simple OCR purposes).', 'For those who interested in C++ code can refer below code. \nThanks Abid Rahman for the nice explanation.']"
12,Most efficient way to map function over numpy array,"What is the most efficient way to map a function over a numpy array? The way I've been doing it in my current project is as follows:

import numpy as np 

x = np.array([1, 2, 3, 4, 5])

# Obtain array ...",https://stackoverflow.com/questions/35215161/most-efficient-way-to-map-function-over-numpy-array,"[""What is the most efficient way to map a function over a numpy array? The way I've been doing it in my current project is as follows:"", ""I've tested all suggested methods plus np.array(map(f, x)) with perfplot (a small project of mine)."", 'How about using numpy.vectorize.', 'As noted by @user2357112, a ""direct"" method of applying the function is always the fastest and simplest way to map a function over Numpy arrays:', 'There are numexpr, numba and cython around, the goal of this answer is to take these possibilities into consideration.', 'Arithmetic operations on arrays are automatically applied elementwise, with efficient C-level loops that avoid all the interpreter overhead that would apply to a Python-level loop or comprehension.', 'It seems no one has mentioned a built-in factory method of producing ufunc in numpy package: np.frompyfunc which I have tested again np.vectorize and have outperformed it by about 20~30%. Of course it will perform well as prescribed C code or even numba(which I have not tested), but it can a better alternative than np.vectorize', 'Edit: the original answer was misleading, np.sqrt was applied directly to the array, just with a small overhead.', 'I believe in newer version( I use 1.13) of numpy you can simply call the function by passing the numpy array to the fuction that you wrote for scalar type, it will automatically apply the function call to each element over the numpy array and return you another numpy array', 'As mentioned in this post, just use generator expressions like so:', 'All above answers compares well, but if you need to use custom function for mapping, and you have numpy.ndarray, and you need to retain the shape of array.', 'Use numpy.fromfunction(function, shape, **kwargs)']"
13,Numpy array dimensions,"I'm currently trying to learn Numpy and Python. Given the following array:

import numpy as np
a = np.array([[1,2],[1,2]])
Is there a function that returns the dimensions of a (e.g.a is a 2 by 2 ...",https://stackoverflow.com/questions/3061761/numpy-array-dimensions,"[""I'm currently trying to learn Numpy and Python. Given the following array:"", 'It is .shape:', 'By convention, in Python world, the shortcut for numpy is np, so:', 'Also works if the input is not a numpy array but a list of lists', 'You can use .shape', 'You can use .ndim for dimension and .shape to know the exact dimension', 'The shape method requires that a be a Numpy ndarray. But Numpy can also calculate the shape of iterables of pure python objects:', 'a.shape is just a limited version of np.info(). Check this out:']"
14,Pandas read_csv low_memory and dtype options,"When calling

df = pd.read_csv('somefile.csv')
I get:
  /Users/josh/anaconda/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py:1130:
  DtypeWarning: Columns (4,5,7,16) have mixed types.  ...",https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options,"['When calling', 'The low_memory option is not properly deprecated, but it should be, since it does not actually do anything differently[source]', 'Try:', 'This should solve the issue. I got exactly the same error, when reading 1.8M rows from a CSV.', 'As mentioned earlier by firelynx if dtype is explicitly specified and there is mixed data that is not compatible with that dtype then loading will crash. I used a converter like this as a workaround to change the values with incompatible data type so that the data could still be loaded.', ""I had a similar issue with a ~400MB file. Setting low_memory=False did the trick for me. Do the simple things first,I would check that your dataframe isn't bigger than your system memory, reboot, clear the RAM before proceeding. If you're still running into errors, its worth making sure your .csv file is ok, take a quick look in Excel and make sure there's no obvious corruption. Broken original data can wreak havoc..."", 'It worked for me with low_memory = False while importing a DataFrame. That is all the change that worked for me:', 'I was facing a similar issue when processing a huge csv file (6 million rows). I had three issues:\n1. the file contained strange characters (fixed using encoding)\n2. the datatype was not specified (fixed using dtype property)\n3. Using the above I still faced an issue which was related with the file_format that could not be defined based on the filename (fixed using try .. except..)', ""According to the pandas documentation, specifying low_memory=False as long as the engine='c' (which is the default) is a reasonable solution to this problem."", 'As the error says, you should specify the datatypes when using the read_csv() method.\nSo, you should write']"
15,What are the differences between numpy arrays and matrices? Which one should I use?,"What are the advantages and disadvantages of each?

From what I've seen, either one can work as a replacement for the other if need be, so should I bother using both or should I stick to just one of ...",https://stackoverflow.com/questions/4151128/what-are-the-differences-between-numpy-arrays-and-matrices-which-one-should-i-u,"['What are the advantages and disadvantages of each?', ""As per the official documents, it's not anymore advisable to use matrix class since it will be removed in the future."", 'Numpy matrices are strictly 2-dimensional, while numpy arrays (ndarrays) are\nN-dimensional.  Matrix objects are a subclass of ndarray, so they inherit all\nthe attributes and methods of ndarrays.', 'Scipy.org recommends that you use arrays:', ""Just to add one case to unutbu's list."", 'As others have mentioned, perhaps the main advantage of matrix was that it provided a convenient notation for matrix multiplication.']"
16,Sorting arrays in NumPy by column,"How can I sort an array in NumPy by the nth column?

For example,

a = array([[9, 2, 3],
           [4, 5, 6],
           [7, 0, 5]])
I'd like to sort rows by the second column, such that I get back:
...",https://stackoverflow.com/questions/2828059/sorting-arrays-in-numpy-by-column,"['How can I sort an array in NumPy by the nth column?', ""@steve's answer is actually the most elegant way of doing it."", 'I suppose this works: a[a[:,1].argsort()]', ""You can sort on multiple columns as per Steve Tjoa's method by using a stable sort like mergesort and sorting the indices from the least significant to the most significant columns:"", 'From the Python documentation wiki, I think you can do:', ""In case someone wants to make use of sorting at a critical part of their programs here's a performance comparison for the different proposals:"", ""From the NumPy mailing list, here's another solution:"", 'I had a similar problem.', 'A little more complicated lexsort example - descending on the 1st column, secondarily ascending on the 2nd.  The tricks with lexsort are that it sorts on rows (hence the .T), and gives priority to the last.', 'Desired output is [[6,5,4,3,2],[11,10,9,8,7],[16,15,14,13,12],[21,20,19,18,17]]', ""Here is another solution considering all columns (more compact way of J.J's answer);"", 'Simply using sort, use coloumn number based on which you want to sort.', 'It is an old question but if you need to generalize this to a higher than 2 dimension arrays, here is the solution than can be easily generalized:', '#for sorting along column 1']"
17,"pandas create new column based on values from other columns / apply a function of multiple columns, row-wise","I want to apply my custom function (it uses an if-else ladder) to these six columns (ERI_Hispanic, ERI_AmerInd_AKNatv, ERI_Asian, ERI_Black_Afr.Amer, ERI_HI_PacIsl, ERI_White) in each row of my ...",https://stackoverflow.com/questions/26886653/pandas-create-new-column-based-on-values-from-other-columns-apply-a-function-o,"['I want to apply my custom function (it uses an if-else ladder) to these six columns (ERI_Hispanic, ERI_AmerInd_AKNatv, ERI_Asian, ERI_Black_Afr.Amer, ERI_HI_PacIsl, ERI_White) in each row of my dataframe.', ""OK, two steps to this - first is to write a function that does the translation you want - I've put an example together based on your pseudo-code:"", ""Since this is the first Google result for 'pandas new column from others', here's a simple example:"", 'The answers above are perfectly valid, but a vectorized solution exists, in the form of numpy.select.  This allows you to define conditions, then define outputs for those conditions, much more efficiently than using apply:', '.apply() takes in a function as the first parameter; pass in the label_race function as so:', 'try this,']"
18,Find nearest value in numpy array,"Is there a numpy-thonic way, e.g. function, to find the nearest value in an array? 

Example:

np.find_nearest( array, value )",https://stackoverflow.com/questions/2566412/find-nearest-value-in-numpy-array,"['Is there a numpy-thonic way, e.g. function, to find the nearest value in an array?', 'IF your array is sorted and is very large, this is a much faster solution:', 'With slight modification, the answer above works with arrays of arbitrary dimension (1d, 2d, 3d, ...):', 'Summary of answer: If one has a sorted array then the bisection code (given below) performs the fastest. ~100-1000 times faster for large arrays, and ~2-100 times faster for small arrays. It does not require numpy either. \nIf you have an unsorted array then if array is large, one should consider first using an O(n logn) sort and then bisection, and if array is small then method 2 seems the fastest.', ""Here's an extension to find the nearest vector in an array of vectors."", ""If you don't want to use numpy this will do it:"", 'Here\'s a version that will handle a non-scalar ""values"" array:', 'Here is a version with scipy for @Ari Onasafari, answer ""to find the nearest vector in an array of vectors""', ""Here is a fast vectorized version of @Dimitri's solution if you have many values to search for (values can be multi-dimensional array):"", ""For large arrays, the (excellent) answer given by @Demitri is far faster than the answer currently marked as best. I've adapted his exact algorithm in the following two ways:"", ""This is a vectorized version of unutbu's answer:"", 'I think the most pythonic way would be:', 'All the answers are beneficial to gather the information to write efficient code. However, I have written a small Python script to optimize for various cases. It will be the best case if the provided array is sorted. If one searches the index of the nearest point of a specified value, then bisect module is the most time efficient. When one search the indices correspond to an array, the numpy searchsorted is most efficient.', 'Maybe helpful for ndarrays:', 'For 2d array, to determine the i, j position of nearest element:']"
19,How to pretty-print a numpy.array without scientific notation and with given precision?,"I'm curious, whether there is any way to print formatted numpy.arrays, e.g., in a way similar to this:

x = 1.23456
print '%.3f' % x
If I want to print the numpy.array of floats, it prints several ...",https://stackoverflow.com/questions/2891790/how-to-pretty-print-a-numpy-array-without-scientific-notation-and-with-given-pre,"[""I'm curious, whether there is any way to print formatted numpy.arrays, e.g., in a way similar to this:"", 'You can use set_printoptions to set the precision of the output:', 'You can get a subset of the np.set_printoptions functionality from the np.array_str command, which applies only to a single print statement.', 'Unutbu gave a really complete answer (they got a +1 from me too), but here is a lo-tech alternative:', 'FYI Numpy 1.15 (release date pending) will include a context manager for setting print options locally. This means that the following will work the same as the corresponding example in the accepted answer (by unutbu and Neil G) without having to write your own context manager. E.g., using their example:', ""The gem that makes it all too easy to obtain the result as a string (in today's numpy versions) is hidden in denis answer:\nnp.array2string"", 'Years later, another one is below. But for everyday use I just', ""And here is what I use, and it's pretty uncomplicated:"", 'Was surprised to not see around method mentioned - means no messing with print options.', 'The numpy arrays have the method round(precision) which return a new numpy array with elements rounded accordingly.', 'I often want different columns to have different formats.  Here is how I print a simple 2D array using some variety in the formatting by converting (slices of) my NumPy array to a tuple:', 'I find that the usual float format {:9.5f} works properly -- suppressing small-value e-notations -- when displaying a list or an array using a loop.  But that format sometimes fails to suppress its e-notation when a formatter has several items in a single print statement.  For example:', 'I use', 'numpy.char.mod may also be useful, depending on the details of your application e.g.:numpy.char.mod(\'Value=%4.2f\', numpy.arange(5, 10, 0.1)) will return a string array with elements ""Value=5.00"", ""Value=5.10"" etc. (as a somewhat contrived example).', 'Yet another option is to use the decimal module:']"
20,Pandas conditional creation of a series/dataframe column,"I have a dataframe along the lines of the below:
    Type       Set
1    A          Z
2    B          Z           
3    B          X
4    C          Y

I want to add another column to the dataframe (...",https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column,"['I have a dataframe along the lines of the below:', 'If you only have two choices to select from:', 'List comprehension is another way to create another column conditionally. If you are working with object dtypes in columns, like in your example, list comprehensions typically outperform most other methods.', 'Another way in which this could be achieved is', ""Here's yet another way to skin this cat, using a dictionary to map new values onto the keys in the list:"", 'The following is slower than the approaches timed here, but we can compute the extra column based on the contents of more than one column, and more than two values can be computed for the extra column.', 'Maybe this has been possible with newer updates of Pandas (tested with pandas=1.0.5), but I think the following is the shortest and maybe best answer for the question, so far. You can use the .loc method and use one condition or several depending on your need.', 'One liner with .apply() method is following:', ""If you're working with massive data, a memoized approach would be best:""]"
21,"Difference between numpy.array shape (R, 1) and (R,)","In numpy, some of the operations return in shape (R, 1) but some return (R,). This will make matrix multiplication more tedious since explicit reshape is required. For example, given a matrix M, if we ...",https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r,"['In numpy, some of the operations return in shape (R, 1) but some return (R,). This will make matrix multiplication more tedious since explicit reshape is required. For example, given a matrix M, if we want to do numpy.dot(M[:,0], numpy.ones((1, R))) where R is the number of rows (of course, the same issue also occurs column-wise). We will get matrices are not aligned error since M[:,0] is in shape (R,) but numpy.ones((1, R)) is in shape (1, R).', 'You write, ""I know literally it\'s list of numbers and list of lists where all list contains only a number"" but that\'s a bit of an unhelpful way to think about it.', ""The difference between (R,) and (1,R) is literally the number of indices that you need to use.  ones((1,R)) is a 2-D array that happens to have only one row.  ones(R) is a vector.  Generally if it doesn't make sense for the variable to have more than one row/column, you should be using a vector, not a matrix with a singleton dimension."", 'The shape is a tuple. If there is only 1 dimension the shape will be one number and just blank after a comma. For 2+ dimensions, there will be a number after all the commas.', 'For its base array class, 2d arrays are no more special than 1d or 3d ones.  There are some operations the preserve the dimensions, some that reduce them, other combine or even expand them.', ""1) The reason not to prefer a shape of (R, 1) over (R,) is that it unnecessarily complicates things. Besides, why would it be preferable to have shape (R, 1) by default for a length-R vector instead of (1, R)? It's better to keep it simple and be explicit when you require additional dimensions."", 'There are a lot of good answers here already. But for me it was hard to find some example, where the shape or array can break all the program.']"
22,What is the purpose of meshgrid in Python / NumPy?,"Can someone explain to me what is the purpose of meshgrid function in Numpy? I know it creates some kind of grid of coordinates for plotting, but I can't really see the direct benefit of it.

I am ...",https://stackoverflow.com/questions/36013063/what-is-the-purpose-of-meshgrid-in-python-numpy,"[""Can someone explain to me what is the purpose of meshgrid function in Numpy? I know it creates some kind of grid of coordinates for plotting, but I can't really see the direct benefit of it."", 'The purpose of meshgrid is to create a rectangular grid out of an array of x values and an array of y values.', 'Courtesy of Microsoft Excel:', 'Actually the purpose of np.meshgrid is already mentioned in the documentation:', 'Suppose you have a function:', 'meshgrid helps in creating a rectangular grid from two 1-D arrays of all pairs of points from the two arrays.', ""Given possible x values, xs, (think of them as the tick-marks on the x-axis of a plot) and possible y values, ys, meshgrid generates the corresponding set of (x, y) grid points---analogous to set((x, y) for x in xs for y in yx).  For example, if xs=[1,2,3] and ys=[4,5,6], we'd get the set of coordinates {(1,4), (2,4), (3,4), (1,5), (2,5), (3,5), (1,6), (2,6), (3,6)}."", 'Short answer']"
23,How do I create an empty array/matrix in NumPy?,"I can't figure out how to use an array or matrix in the way that I would normally use a list. I want to create an empty array (or matrix) and then add one column (or row) to it at a time.

At the ...",https://stackoverflow.com/questions/568962/how-do-i-create-an-empty-array-matrix-in-numpy,"[""I can't figure out how to use an array or matrix in the way that I would normally use a list. I want to create an empty array (or matrix) and then add one column (or row) to it at a time."", 'You have the wrong mental model for using NumPy efficiently. NumPy arrays are stored in contiguous blocks of memory. If you want to add rows or columns to an existing array, the entire array needs to be copied to a new block of memory, creating gaps for the new elements to be stored. This is very inefficient if done repeatedly to build an array.', 'A NumPy array is a very different data structure from a list and is designed to be used in different ways.  Your use of hstack is potentially very inefficient... every time you call it, all the data in the existing array is copied into a new one. (The append function will have the same issue.)  If you want to build up your matrix one column at a time, you might be best off to keep it in a list until it is finished, and only then convert it into an array.', ""To create an empty multidimensional array in NumPy (e.g. a 2D array m*n to store your matrix), in case you don't know m how many rows you will append and don't care about the computational cost Stephen Simmons mentioned (namely re-buildinging the array at each append), you can squeeze to 0 the dimension to which you want to append to: X = np.empty(shape=[0, n])."", ""I looked into this a lot because I needed to use a numpy.array as a set in one of my school projects and I needed to be initialized empty... I didn't found any relevant answer here on Stack Overflow, so I started doodling something."", 'You can use the append function.  For rows:', 'Here is some workaround to make numpys look more like Lists', ""If you absolutely don't know the final size of the array, you can increment the size of the array like this:"", 'You can apply it to build any kind of array, like zeros:', ""Depending on what you are using this for, you may need to specify the data type (see 'dtype')."", 'I think you want to handle most of the work with lists then use the result as a matrix. Maybe this is a way ;', 'I think you can create empty numpy array like:', 'For creating an empty NumPy array without defining its shape there is to way:', 'Perhaps what you are looking for is something like this:']"
24,How to convert 2D float numpy array to 2D int numpy array?,"How to convert real numpy array to int numpy array?
Tried using map directly to array but it did not work.",https://stackoverflow.com/questions/10873824/how-to-convert-2d-float-numpy-array-to-2d-int-numpy-array,"['How to convert real numpy array to int numpy array?\nTried using map directly to array but it did not work.', 'Use the astype method.', 'Some numpy functions for how to control the rounding: rint, floor,trunc, ceil. depending how  u wish to round the floats, up, down, or to the nearest int.', 'you can use np.int_:', ""If you're not sure your input is going to be a Numpy array, you can use asarray with dtype=int instead of astype:""]"
25,Converting NumPy array into Python List structure?,"How do I convert a NumPy array to a Python List (for example [[1,2,3],[4,5,6]] ), and do it reasonably fast?",https://stackoverflow.com/questions/1966207/converting-numpy-array-into-python-list-structure,"['How do I convert a NumPy array to a Python List (for example [[1,2,3],[4,5,6]] ), and do it reasonably fast?', 'Use tolist():', 'The numpy .tolist method produces nested lists if the numpy array shape is 2D.', 'tolist() works fine even if encountered a nested array, say a pandas DataFrame;', 'c = np.array([[1,2,3],[4,5,6]])', 'someList = [list(map(int, input().split())) for i in range(N)]']"
26,What is the difference between Numpy's array() and asarray() functions?,What is the difference between Numpy's array() and asarray() functions? When should you use one rather than the other? They seem to generate identical output for all the inputs I can think of.,https://stackoverflow.com/questions/14415741/what-is-the-difference-between-numpys-array-and-asarray-functions,"[""What is the difference between Numpy's array() and asarray() functions? When should you use one rather than the other? They seem to generate identical output for all the inputs I can think of."", ""Since other questions are being redirected to this one which ask about asanyarray or other array creation routines, it's probably worth having a brief summary of what each of them does."", 'The definition of asarray is:', 'The difference can be demonstrated by this example:', 'The differences are mentioned quite clearly in the documentation of array and asarray. The differences lie in the argument list and hence the action of the function depending on those parameters.', ""Here's a simple example that can demonstrate the difference."", 'asarray(x) is like array(x, copy=False)']"
27,How to add an extra column to a NumPy array,"Let’s say I have a NumPy array, a:
a = np.array([
    [1, 2, 3],
    [2, 3, 4]
    ])
And I would like to add a column of zeros to get an array, b:
b = np.array([
    [1, 2, 3, 0],
    [2, 3, 4, 0]
...",https://stackoverflow.com/questions/8486294/how-to-add-an-extra-column-to-a-numpy-array,"['Let’s say I have a NumPy array, a:', 'I think a more straightforward solution and faster to boot is to do the following:', 'np.r_[ ... ] and np.c_[ ... ]\nare useful alternatives to vstack and hstack,\nwith square brackets [] instead of round ().\nA couple of examples:', 'Use numpy.append:', 'One way, using hstack, is:', 'I find the following most elegant:', 'I was also interested in this question and compared the speed of', 'I think:', 'np.concatenate also works', 'Assuming M is a (100,3) ndarray and y is a (100,) ndarray append can be used as follows:', ""I like JoshAdel's answer because of the focus on performance. A minor performance improvement is to avoid the overhead of initializing with zeros, only to be overwritten. This has a measurable difference when N is large, empty is used instead of zeros, and the column of zeros is written as a separate step:"", 'np.insert also serves the purpose.', ""Numpy's np.append method takes three parameters, the first two are 2D numpy arrays and the 3rd is an axis parameter instructing along which axis to append:"", 'A bit late to the party, but nobody posted this answer yet, so for the sake of completeness: you can do this with list comprehensions, on a plain Python array:', 'For me, the next way looks pretty intuitive and simple.', 'In my case, I had to add a column of ones to a NumPy array', 'There is a function specifically for this. It is called numpy.pad', 'I liked this:']"
28,"Converting between datetime, Timestamp and datetime64","How do I convert a numpy.datetime64 object to a datetime.datetime (or Timestamp)?

In the following code, I create a datetime, timestamp and datetime64 objects.

import datetime
import numpy as np
...",https://stackoverflow.com/questions/13703720/converting-between-datetime-timestamp-and-datetime64,"['How do I convert a numpy.datetime64 object to a datetime.datetime (or Timestamp)?', 'To convert numpy.datetime64 to datetime object that represents time in UTC on numpy-1.8:', 'You can just use the pd.Timestamp constructor. The following diagram may be useful for this and related questions.', 'Welcome to hell.', ""I think there could be a more consolidated effort in an answer to better explain the relationship between Python's datetime module, numpy's datetime64/timedelta64 and pandas' Timestamp/Timedelta objects."", 'For DatetimeIndex, the tolist returns a list of datetime objects. For a single datetime64 object it returns a single datetime object.', 'If you want to convert an entire pandas series of datetimes to regular python datetimes, you can also use .to_pydatetime().', 'One option is to use str, and then to_datetime (or similar):', ""This post has been up for 4 years and I still struggled with this conversion problem - so the issue is still active in 2017 in some sense. I was somewhat shocked that the numpy documentation does not readily offer a simple conversion algorithm but that's another story."", ""I've come back to this answer more times than I can count, so I decided to throw together a quick little class, which converts a Numpy datetime64 value to Python datetime value. I hope it helps others out there."", 'use this function to get pythons native datetime object', 'Some solutions work well for me but numpy will deprecate some parameters. \nThe solution that work better for me is to read the date as a pandas datetime and excract explicitly the year, month and day of a pandas object.\nThe following code works for the most common situation.', 'indeed, all of these datetime types can be difficult, and potentially problematic (must keep careful track of timezone information). here\'s what i have done, though i admit that i am concerned that at least part of it is ""not by design"". also, this can be made a bit more compact as needed.\nstarting with a numpy.datetime64 dt_a:']"
29,Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?,"I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:

data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])
...",https://stackoverflow.com/questions/20763012/creating-a-pandas-dataframe-from-a-numpy-array-how-do-i-specify-the-index-colum,"['I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:', 'You need to specify data, index and columns to DataFrame constructor, as in:', 'Here is an easy to understand solution', 'I agree with Joris; it seems like you should be doing this differently, like with numpy record arrays. Modifying ""option 2"" from this great answer, you could do it like this:', 'This can be done simply by using from_records of pandas DataFrame', '', ""Adding to @behzad.nouri 's answer - we can create a helper routine to handle this common scenario:"", 'Here simple example to create pandas dataframe by using numpy array.', 'I think this is a simple and intuitive method:', ""It's not so short, but maybe can help you.""]"
30,What does numpy.random.seed(0) do?,"What does np.random.seed do in the below code from a Scikit-Learn tutorial? I'm not very familiar with NumPy's random state generator stuff, so I'd really appreciate a layman's terms explanation of ...",https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do,"[""What does np.random.seed do in the below code from a Scikit-Learn tutorial? I'm not very familiar with NumPy's random state generator stuff, so I'd really appreciate a layman's terms explanation of this."", 'np.random.seed(0) makes the random numbers predictable', ""If you set the np.random.seed(a_fixed_number) every time you call the numpy's other random function, the result will be the same:"", 'As noted, numpy.random.seed(0) sets the random seed to 0, so the pseudo random numbers you get from random will start from the same point. This can be good for debuging in some cases.  HOWEVER, after some reading, this seems to be the wrong way to go at it, if you have threads because it is not thread safe.', 'I have used this very often in neural networks. It is well known that when we start training a neural network we randomly initialise the weights. The model is trained on these weights on a particular dataset. After number of epochs you get trained set of weights.', 'Imagine you are showing someone how to code something with a bunch of ""random"" numbers. By using numpy seed they can use the same seed number and get the same set of ""random"" numbers.', 'A random seed specifies the start point when a computer generates a random number sequence.', ""All the answers above show the implementation of np.random.seed() in code. I'll try my best to explain briefly why it actually happens. Computers are machines that are designed based on predefined algorithms. Any output from a computer is the result of the algorithm implemented on the input. So when we request a computer to generate random numbers, sure they are random but the computer did not just come up with them randomly!"", 'I hope to give a really short answer:', 'All the random numbers generated after setting particular seed value are same across all the platforms/systems.', 'There is a nice explanation in Numpy docs:\nhttps://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.RandomState.html\nit refers to Mersenne Twister pseudo-random number generator.  More details on the algorithm here: https://en.wikipedia.org/wiki/Mersenne_Twister', 'This produces the following output:\narray([5, 0, 3, 3, 7])\nAgain,if we run the same code we will get the same result.']"
31,What is the difference between flatten and ravel functions in numpy?,"import numpy as np
y = np.array(((1,2,3),(4,5,6),(7,8,9)))
OUTPUT:
print(y.flatten())
[1   2   3   4   5   6   7   8   9]
print(y.ravel())
[1   2   3   4   5   6   7   8   9]
Both function return the ...",https://stackoverflow.com/questions/28930465/what-is-the-difference-between-flatten-and-ravel-functions-in-numpy,"['Both function return the same list.\nThen what is the need of two different functions performing same job.', 'The current API is that:', 'As explained here a key difference is that:', 'Here is the correct namespace for the functions:']"
32,How do I check which version of NumPy I'm using?,"How can I check which version of NumPy I'm using?

(FYI this question has been edited because both the question and answer are not platform specific.)",https://stackoverflow.com/questions/1520234/how-do-i-check-which-version-of-numpy-im-using,"[""How can I check which version of NumPy I'm using?"", 'From the command line, you can simply issue:', 'Run:', 'You can also check if your version is using MKL with:', 'We can use pip freeze to get any Python package version without opening the Python shell.', ""If you're using NumPy from the Anaconda distribution, then you can just do:"", 'You can try this:', 'You can get numpy version using Terminal or a Python code.', 'For Python 3.X print syntax:', 'Just a slight solution change for checking the version of numpy with Python,', 'In a Python shell:', 'Pure Python line that can be executed from the terminal (both 2.X and 3.X versions):', 'It is good to know the version of numpy you run, but strictly speaking if you just need to have specific version on your system you can write like this:', 'Simply']"
33,What does axis in pandas mean?,"Here is my code to generate a dataframe:

import pandas as pd
import numpy as np

dff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))
then I got the dataframe:

+------------+---------+-------...",https://stackoverflow.com/questions/22149584/what-does-axis-in-pandas-mean,"['Here is my code to generate a dataframe:', ""It specifies the axis along which the means are computed. By default axis=0. This is consistent with the numpy.mean usage when axis is specified explicitly (in numpy.mean, axis==None by default, which computes the mean value over the flattened array) , in which axis=0 along the rows (namely, index in pandas), and axis=1 along the columns. For added clarity, one may choose to specify axis='index' (instead of axis=0) or axis='columns' (instead of axis=1)."", 'These answers do help explain this, but it still isn\'t perfectly intuitive for a non-programmer (i.e. someone like me who is learning Python for the first time in context of data science coursework). I still find using the terms ""along"" or ""for each"" wrt to rows and columns to be confusing.', ""Let's visualize (you gonna remember always),"", 'axis refers to the dimension of the array, in the case of pd.DataFrames axis=0 is the dimension that points downwards and axis=1 the one that points to the right.', 'The easiest way for me to understand is to talk about whether you are calculating a statistic for each column (axis = 0) or each row (axis = 1). If you calculate a statistic, say a mean, with axis = 0 you will get that statistic for each column. So if each observation is a row and each variable is in a column, you would get the mean of each variable. If you set axis = 1 then you will calculate your statistic for each row. In our example, you would get the mean for each observation across all of your variables (perhaps you want the average of related measures).', ""Let's look at the table from Wiki. This is an IMF estimate of GDP from 2010 to 2019 for top ten countries."", 'Axis in view of programming is the position in the shape tuple. Here is an example:', ""The designer of pandas, Wes McKinney, used to work intensively on finance data. Think of columns as stock names and index as daily prices. You can then guess what the default behavior is (i.e., axis=0) with respect to this finance data. axis=1 can be simply thought as 'the other direction'."", 'The problem with using axis= properly is for its use for 2 main different cases:', 'one of easy ways to remember axis 1 (columns), vs axis 0 (rows) is the output you expect.', ""This is based on @Safak's answer.\nThe best way to understand the axes in pandas/numpy is to create a 3d array and check the result of the sum function along the 3 different axes."", 'I understand this way :', 'axis = 0 means up to down \naxis = 1 means left to right', ""I will explicitly avoid using 'row-wise' or 'along the columns', since people may interpret them in exactly the wrong way."", '', 'My thinking : Axis = n, where n = 0, 1, etc. means that the matrix is collapsed (folded) along that axis. So in a 2D matrix, when you collapse along 0 (rows), you are really operating on one column at a time. Similarly for higher order matrices.', ""I'm a newbie to pandas. But this is how I understand axis in pandas:"", 'I think there is an another way to understand it.', 'I have been trying to figure out the axis for the last hour as well. The language in all the above answers, and also the documentation is not at all helpful.', 'Many answers here helped me a lot!', 'Arrays are designed with so-called axis=0 and rows positioned vertically versus axis=1 and columns positioned horizontally. Axis refers to the dimension of the array.']"
34,Concatenating two one-dimensional NumPy arrays,"I have two simple one-dimensional arrays in NumPy. I should be able to concatenate them using numpy.concatenate. But I get this error for the code below:

TypeError: only length-1 arrays can be ...",https://stackoverflow.com/questions/9236926/concatenating-two-one-dimensional-numpy-arrays,"['I have two simple one-dimensional arrays in NumPy. I should be able to concatenate them using numpy.concatenate. But I get this error for the code below:', 'The line should be:', 'There are several possibilities for concatenating 1D arrays, e.g.,', 'The first parameter to concatenate should itself be a sequence of arrays to concatenate:', 'An alternative ist to use the short form of ""concatenate"" which is either ""r_[...]"" or ""c_[...]"" as shown in the example code beneath (see http://wiki.scipy.org/NumPy_for_Matlab_Users for additional information):', 'Here are more approaches for doing this by using numpy.ravel(), numpy.array(), utilizing the fact that 1D arrays can be unpacked into plain elements:', 'Some more facts from the numpy docs :']"
35,NumPy array is not JSON serializable,"After creating a NumPy array, and saving it as a Django context variable, I receive the following error when loading the webpage:

array([   0,  239,  479,  717,  952, 1192, 1432, 1667], dtype=int64) ...",https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable,"['After creating a NumPy array, and saving it as a Django context variable, I receive the following error when loading the webpage:', 'I regularly ""jsonify"" np.arrays. Try using the "".tolist()"" method on the arrays first, like this:', 'Store as JSON a numpy.ndarray or any nested-list composition.', 'You can use Pandas:', 'I found the best solution if you have nested numpy arrays in a dictionary:', 'Use the json.dumps default kwarg:', ""This is not supported by default, but you can make it work quite easily! There are several things you'll want to encode if you want the exact same data back:"", 'You could also use default argument for example:', 'I had a similar problem with a nested dictionary with some numpy.ndarrays in it.', 'use NumpyEncoder it will process json dump successfully.without throwing - NumPy array is not JSON serializable', 'Also, some very interesting information further on lists vs. arrays in Python ~> Python List vs. Array - when to use?', 'Here is an implementation that work for me and removed all nans (assuming these are simple object (list or dict)):', ""This is a different answer, but this might help to help people who are trying to save data and then read it again.\nThere is hickle which is faster than pickle and easier.\nI tried to save and read it in pickle dump but while reading there were lot of problems and wasted an hour and still didn't find solution though I was working on my own data to create a chat bot."", 'May do simple for loop with checking types:', 'TypeError: array([[0.46872085, 0.67374235, 1.0218339 , 0.13210179, 0.5440686 , 0.9140083 , 0.58720225, 0.2199381 ]], dtype=float32) is not JSON serializable']"
36,Most efficient way to reverse a numpy array,"Believe it or not, after profiling my current code, the repetitive operation of numpy array reversion ate a giant chunk of the running time. What I have right now is the common view-based method:

...",https://stackoverflow.com/questions/6771428/most-efficient-way-to-reverse-a-numpy-array,"['Believe it or not, after profiling my current code, the repetitive operation of numpy array reversion ate a giant chunk of the running time. What I have right now is the common view-based method:', 'When you create reversed_arr you are creating a view into the original array.  You can then change the original array, and the view will update to reflect the changes.', ""As mentioned above, a[::-1] really only creates a view, so it's a constant-time operation (and as such doesn't take longer as the array grows). If you need the array to be contiguous (for example because you're performing many vector operations with it), ascontiguousarray is about as fast as flipud/fliplr:"", 'Because this seems to not be marked as answered yet... The Answer of Thomas Arildsen should be the proper one: just use', 'np.fliplr() flips the array left to right.', 'I will expand on the earlier answer about np.fliplr(). Here is some code that demonstrates constructing a 1d array, transforming it into a 2d array, flipping it, then converting back into a 1d array. time.clock() will be used to keep time, which is presented in terms of seconds.', 'Expanding on what others have said I will give a short example.', 'In order to have it working with negative numbers and a long list you can do the following:']"
37,How to take column-slices of dataframe in pandas,"I load some machine learning data from a CSV file. The first 2 columns are observations and the remaining columns are features.

Currently, I do the following:

data = pandas.read_csv('mydata.csv')
...",https://stackoverflow.com/questions/10665889/how-to-take-column-slices-of-dataframe-in-pandas,"['I load some machine learning data from a CSV file. The first 2 columns are observations and the remaining columns are features.', 'See the deprecation in the docs', 'Note: .ix has been deprecated since Pandas v0.20. You should instead use .loc or .iloc, as appropriate.', 'Lets use the titanic dataset from the seaborn package as an example', 'Also, Given a DataFrame', 'You can slice along the columns of a DataFrame by referring to the names of each column in a list, like so:', 'And if you came here looking for slicing two ranges of columns and combining them together (like me) you can do something like', ""Here's how you could use different methods to do selective column slicing, including selective label based, index based and the selective ranges based column slicing."", 'Its equivalent', 'if Data frame look like that:', ""Another way to get a subset of columns from your DataFrame, assuming you want all the rows, would be to do:\ndata[['a','b']] and data[['c','d','e']]\nIf you want to use numerical column indexes you can do:\ndata[data.columns[:2]] and data[data.columns[2:]]""]"
38,Saving a Numpy array as an image,"I have a matrix in the type of a Numpy array.  How would I write it to disk it as an image?  Any format works (png, jpeg, bmp...).  One important constraint is that PIL is not present.",https://stackoverflow.com/questions/902761/saving-a-numpy-array-as-an-image,"['I have a matrix in the type of a Numpy array.  How would I write it to disk it as an image?  Any format works (png, jpeg, bmp...).  One important constraint is that PIL is not present.', ""You can use PyPNG. It's a pure Python (no dependencies) open source PNG encoder/decoder and it supports writing NumPy arrays as images."", 'This uses PIL, but maybe some might find it useful:', ""An answer using PIL (just in case it's useful)."", 'With matplotlib:', 'Pure Python (2 & 3), a snippet without 3rd party dependencies.', ""There's opencv for python (documentation here)."", 'If you have matplotlib, you can do:', ""You can use 'skimage' library in Python"", 'scipy.misc gives deprecation warning about imsave function and suggests usage of imageio instead.', ""Addendum to @ideasman42's answer:"", 'for saving a numpy array as image, U have several choices:', 'For those looking for a direct fully working example:', ""matplotlib svn has a new function to save images as just an image -- no axes etc.  it's a very simple function to backport too, if you don't want to install svn (copied straight from image.py in matplotlib svn, removed the docstring for brevity):"", ""The world probably doesn't need yet another package for writing a numpy array to a PNG file, but for those who can't get enough, I recently put up numpngw on github:"", 'Assuming you want a grayscale image:', 'Imageio is a Python library that provides an easy interface to read and write a wide range of image data, including animated images, video, volumetric data, and scientific formats. It is cross-platform, runs on Python 2.7 and 3.4+, and is easy to install.', ""If you happen to use [Py]Qt already, you may be interested in qimage2ndarray.  Starting with version 1.4 (just released), PySide is supported as well, and there will be a tiny imsave(filename, array) function similar to scipy's, but using Qt instead of PIL.  With 1.3, just use something like the following:"", 'If you are working in python environment Spyder, then it cannot get more easier than to just right click the array in variable explorer, and then choose Show Image option.', 'Use cv2.imwrite.', 'With pygame']"
39,How to convert a PIL Image into a numpy array?,"Alright, I'm toying around with converting a PIL image object back and forth to a numpy array so I can do some faster pixel by pixel transformations than PIL's PixelAccess object would allow.  I've ...",https://stackoverflow.com/questions/384759/how-to-convert-a-pil-image-into-a-numpy-array,"[""Alright, I'm toying around with converting a PIL image object back and forth to a numpy array so I can do some faster pixel by pixel transformations than PIL's PixelAccess object would allow.  I've figured out how to place the pixel information in a useful 3D numpy array by way of:"", ""You're not saying how exactly putdata() is not behaving. I'm assuming you're doing"", 'Open I as an array:', 'I am using Pillow 4.1.1 (the successor of PIL) in Python 3.5. The conversion between Pillow and numpy is straightforward.', 'You need to convert your image to a numpy array this way:', 'Convert Numpy to PIL image  and PIL to Numpy', 'The example, I have used today:', 'If your image is stored in a Blob format (i.e. in a database) you can use the same technique explained by Billal Begueradj to convert your image from Blobs to a byte array.', 'You can transform the image into numpy \nby parsing the image into numpy() function after squishing out the features( unnormalization)']"
40,"Comparing two NumPy arrays for equality, element-wise","What is the simplest way to compare two NumPy arrays for equality (where equality is defined as: A = B iff for all indices i: A[i] == B[i])?

Simply using == gives me a boolean array:

 >>> ...",https://stackoverflow.com/questions/10580676/comparing-two-numpy-arrays-for-equality-element-wise,"['What is the simplest way to compare two NumPy arrays for equality (where equality is defined as: A = B iff for all indices i: A[i]\xa0== B[i])?', 'test if all values of array (A==B) are True.', 'The (A==B).all() solution is very neat, but there are some built-in functions for this task. Namely array_equal, allclose and array_equiv.', 'If you want to check if two arrays have the same shape AND elements you should  use np.array_equal as it is the method recommended in the documentation.', ""Let's measure the performance by using the following piece of code."", 'Usually two arrays will have some small numeric errors,', 'Now use np.array_equal. From documentation:']"
41,numpy: most efficient frequency counts for unique values in an array,"In numpy / scipy, is there an efficient way to get frequency counts for unique values in an array?

Something along these lines:

x = array( [1,1,1,2,2,2,5,25,1,1] )
y = freq_count( x )
print y

>&...",https://stackoverflow.com/questions/10741346/numpy-most-efficient-frequency-counts-for-unique-values-in-an-array,"['In numpy / scipy, is there an efficient way to get frequency counts for unique values in an array?', 'Take a look at np.bincount:', 'As of Numpy 1.9, the easiest and fastest method is to simply use numpy.unique, which now has a return_counts keyword argument:', 'Update: The method mentioned in the original answer is deprecated, we should use the new way instead:', 'I was also interested in this, so I did a little performance comparison (using perfplot, a pet project of mine). Result:', 'Using pandas module:', ""This is by far the most general and performant solution; surprised it hasn't been posted yet."", 'numpy.bincount is the probably the best choice. If your array contains anything besides small dense integers it might be useful to wrap it something like this:', 'Even though it has already been answered, I suggest a different approach that makes use of numpy.histogram. Such function given a sequence it returns the frequency of its elements grouped in bins.', 'This gives you:\n{1: 5, 2: 3, 5: 1, 25: 1}', ""To count unique non-integers - similar to Eelco Hoogendoorn's answer but considerably faster (factor of 5 on my machine), I used weave.inline to combine numpy.unique with a bit of c-code;"", ""Old question, but I'd like to provide my own solution which turn out to be the fastest, use normal list instead of np.array as input (or transfer to list firstly), based on my bench test."", 'some thing like this should do it:', 'multi-dimentional frequency count, i.e. counting arrays.', 'Most of simple problems get complicated because simple functionality like order() in R that gives a statistical result in both and descending order is missing in various python libraries. But if we devise our thinking that all such statistical ordering and parameters in python are easily found in pandas, we can can result sooner than looking in 100 different places. Also, development of R and pandas go hand-in-hand because they were created for same purpose. To solve this problem I use following code that gets me by anywhere:']"
42,What is the difference between ndarray and array in numpy?,What is the difference between ndarray and array in Numpy? And where can I find the implementations in the numpy source code?,https://stackoverflow.com/questions/15879315/what-is-the-difference-between-ndarray-and-array-in-numpy,"['What is the difference between ndarray and array in Numpy? And where can I find the implementations in the numpy source code?', 'numpy.array is just a convenience function to create an ndarray; it is not a class itself.', 'numpy.array is a function that returns a numpy.ndarray.  There is no object type numpy.array.', 'Just a few lines of example code to show the difference between numpy.array and numpy.ndarray', 'numpy.ndarray() is a class, while numpy.array() is a method / function to create ndarray.', 'I think with np.array() you can only create C like though you mention the order, when you check using np.isfortran() it says false. but with np.ndarrray() when you specify the order it creates based on the order provided.']"
43,Converting numpy dtypes to native python types,"If I have a numpy dtype, how do I automatically convert it to its closest python data type?  For example,

numpy.float32 -> ""python float""
numpy.float64 -> ""python float""
numpy.uint32  -> ""...",https://stackoverflow.com/questions/9452775/converting-numpy-dtypes-to-native-python-types,"['If I have a numpy dtype, how do I automatically convert it to its closest python data type?  For example,', 'Use val.item() to convert most NumPy values to a native Python type:', ""found myself having mixed set of numpy types and standard python. as all numpy types derive from numpy.generic, here's how you can convert everything to python standard types:"", 'If you want to convert (numpy.array OR numpy scalar OR native type OR numpy.darray) TO native type you can simply do :', 'How about:', 'tolist() is a more general approach to accomplish this. It works in any primitive dtype and also in arrays or matrices.', 'You can also call the item() method of the object you want to convert:', 'I think you can just write general type convert function like so:', 'numpy holds that information in a mapping exposed as typeDict so you could do something like the below::', 'Sorry to come late to the partly, but I was looking at a problem of converting numpy.float64 to regular Python float only. I saw 3 ways of doing that:', 'My approach is a bit forceful, but seems to play nice for all cases:', ""A side note about array scalars for those who don't need automatic conversion and know the numpy dtype of the value:"", 'Translate the whole ndarray instead one unit data object:']"
44,Relationship between SciPy and NumPy,"SciPy appears to provide most (but not all [1]) of NumPy's functions in its own namespace. In other words, if there's a function named numpy.foo, there's almost certainly a scipy.foo. Most of the time,...",https://stackoverflow.com/questions/6200910/relationship-between-scipy-and-numpy,"[""SciPy appears to provide most (but not all [1]) of NumPy's functions in its own namespace. In other words, if there's a function named numpy.foo, there's almost certainly a scipy.foo. Most of the time, the two appear to be exactly the same, oftentimes even pointing to the same function object."", 'Last time I checked it, the scipy __init__ method executes a', 'From the SciPy Reference Guide:', 'It seems from the SciPy FAQ that some functions from NumPy are here for historical reasons while it should\nonly be in SciPy:', 'There is a short comment at the end of the introduction to SciPy documentation:', 'From Wikipedia ( http://en.wikipedia.org/wiki/NumPy#History ):', 'Regarding the linalg package - the scipy functions will call lapack and blas, which are available in highly optimised versions on many platforms and offer very good performance, particularly for operations on reasonably large dense matrices.  On the other hand, they are not easy libraries to compile, requiring a fortran compiler and many platform specific tweaks to get full performance. Therefore, numpy provides simple implementations of many common linear algebra functions which are often good enough for many purposes.', ""From Lectures on 'Quantitative Economics'"", 'In addition to the SciPy FAQ describing the duplication is mainly for backwards compatibility, it is further clarified in the NumPy documentation to say that']"
45,Better way to shuffle two numpy arrays in unison,"I have two numpy arrays of different shapes, but with the same length (leading dimension). I want to shuffle each of them, such that corresponding elements continue to correspond -- i.e. shuffle them ...",https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison,"['I have two numpy arrays of different shapes, but with the same length (leading dimension). I want to shuffle each of them, such that corresponding elements continue to correspond -- i.e. shuffle them in unison with respect to their leading indices.', 'Your ""scary"" solution does not appear scary to me.  Calling shuffle() for two sequences of the same length results in the same number of calls to the random number generator, and these are the only ""random"" elements in the shuffle algorithm.  By resetting the state, you ensure that the calls to the random number generator will give the same results in the second call to shuffle(), so the whole algorithm will generate the same permutation.', ""Your can use NumPy's array indexing:"", 'To learn more, see http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html', 'Very simple solution:', 'James wrote in 2015 an sklearn solution which is helpful. But he added a random state variable, which is not needed. In the below code, the random state from numpy is automatically assumed.', 'Shuffle any number of arrays together, in-place, using only NumPy.', 'you can make an array like:', 'There is a well-known function that can handle this:', 'One way in which in-place shuffling  can be done for connected lists is using a seed (it could be random) and using numpy.random.shuffle to do the shuffling.', 'This seems like a very simple solution:', 'Say we have two arrays: a and b.', 'If you want to avoid copying arrays, then I would suggest that instead of generating a permutation list, you go through every element in the array, and randomly swap it to another position in the array', ""With an example, this is what I'm doing:"", ""I extended python's random.shuffle() to take a second arg:"", 'Just use numpy...']"
46,How to implement the Softmax function in Python,"From the Udacity's deep learning class, the softmax of y_i is simply the exponential divided by the sum of exponential of the whole Y vector:
Where S(y_i) is the softmax function of y_i and e is the ...",https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python,"[""From the Udacity's deep learning class, the softmax of y_i is simply the exponential divided by the sum of exponential of the whole Y vector:"", ""They're both correct, but yours is preferred from the point of view of numerical stability."", '(Well... much confusion here, both in the question and in the answers...)', ""So, this is really a comment to desertnaut's answer but I can't comment on it yet due to my reputation. As he pointed out, your version is only correct if your input consists of a single sample. If your input consists of several samples, it is wrong. However, desertnaut's solution is also wrong. The problem is that once he takes a 1-dimensional input and then he takes a 2-dimensional input. Let me show this to you."", 'I would say that while both are correct mathematically, implementation-wise, first one is better. When computing softmax, the intermediate values may become very large. Dividing two large numbers can be numerically unstable. These notes (from Stanford) mention a normalization trick which is essentially what you are doing.', 'sklearn also offers implementation of softmax', 'From mathematical point of view both sides are equal.', 'EDIT. As of version 1.2.0, scipy includes softmax as a special function:', 'Here you can find out why they used - max.', 'A more concise version is:', 'To offer an alternative solution, consider the cases where your arguments are extremely large in magnitude such that exp(x) would underflow (in the negative case) or overflow (in the positive case). Here you want to remain in log space as long as possible, exponentiating only at the end where you can trust the result will be well-behaved.', 'I needed something compatible with the output of a dense layer from Tensorflow.', 'I would suggest this:', 'In order to maintain for numerical stability, max(x) should be subtracted. The following is the code for softmax function;', 'Already answered in much detail in above answers. max is subtracted to avoid overflow. I am adding here one more implementation in python3.', ""Everybody seems to post their solution so I'll post mine:"", 'Based on all the responses and CS231n notes, allow me to summarise:', 'I would like to supplement a little bit more understanding of the problem. Here it is correct of subtracting max of the array. But if you run the code in the other post, you would find it is not giving you right answer when the array is 2D  or higher dimensions.', 'Goal was to achieve similar results using Numpy and Tensorflow. The only change from original answer is axis parameter for np.sum api.', 'Here is generalized solution using numpy and comparision for correctness with tensorflow ans scipy:', 'The softmax function is an activation function that turns numbers into probabilities which sum to one. The softmax function outputs a vector that represents the probability distributions of a list of outcomes. It is also a core element used in deep learning classification tasks.', 'This also works with np.reshape.', 'The purpose of the softmax function is to preserve the ratio of the vectors as opposed to squashing the end-points with a sigmoid as the values saturate (i.e. tend to +/- 1 (tanh) or from 0 to 1 (logistical)). This is because it preserves more information about the rate of change at the end-points and thus is more applicable to neural nets with 1-of-N Output Encoding (i.e. if we squashed the end-points it would be harder to differentiate the 1-of-N output class because we can\'t tell which one is the ""biggest"" or ""smallest"" because they got squished.); also it makes the total output sum to 1, and the clear winner will be closer to 1 while other numbers that are close to each other will sum to 1/p, where p is the number of output neurons with similar values.', 'This generalizes and assumes you are normalizing the trailing dimension.']"
47,NumPy array initialization (fill with identical values),"I need to create a NumPy array of length n, each element of which is v.

Is there anything better than:

a = empty(n)
for i in range(n):
    a[i] = v
I know zeros and ones would work for v = 0, 1. I ...",https://stackoverflow.com/questions/5891410/numpy-array-initialization-fill-with-identical-values,"['I need to create a NumPy array of length n, each element of which is v.', 'NumPy\xa01.8 introduced np.full(), which is a more direct method than empty() followed by fill() for creating an array filled with a certain value:', 'Updated for Numpy 1.7.0:(Hat-tip to @Rolf Bartstra.)', 'I believe fill is the fastest way to do this.', 'I had', ""Apparently, not only the absolute speeds but also the speed order (as reported by user1579844) are machine dependent; here's what I found:"", 'You can use numpy.tile, e.g. :', 'without numpy']"
48,Convert array of indices to 1-hot encoded numpy array,"Let's say I have a 1d numpy array
a = array([1,0,3])

I would like to encode this as a 2D one-hot array
b = array([[0,1,0,0], [1,0,0,0], [0,0,0,1]])

Is there a quick way to do this?  Quicker than ...",https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array,"[""Let's say I have a 1d numpy array"", 'Your array a defines the columns of the nonzero elements in the output array. You need to also define the rows and then use fancy indexing:', 'In case you are using keras, there is a built in utility for that:', 'Here is what I find useful:', 'You can use  sklearn.preprocessing.LabelBinarizer:', 'You can also use eye function of numpy:', 'Here is a function that converts a 1-D vector to a 2-D one-hot array.', 'You can use the following code for converting into a one-hot vector:', 'For 1-hot-encoding', 'I think the short answer is no. For a more generic case in n dimensions, I came up with this:', 'Just to elaborate on the excellent answer from K3---rnc, here is a more generic version:', 'I recently ran into a problem of same kind and found said solution which turned out to be only satisfying if you have numbers that go within a certain formation. For example if you want to one-hot encode following list:', 'Such type of encoding are usually part of numpy array. If you are using a numpy array like this :', 'clean and easy solution:', 'Link to documentation: neuraxle.steps.numpy.OneHotEncoder', 'Here is an example function that I wrote to do this based upon the answers above and my own use case:', 'I am adding for completion a simple function, using only numpy operators:', ""Here's a dimensionality-independent standalone solution."", 'Use the following code. It works best.', 'If using tensorflow, there is one_hot():']"
49,How do I convert a numpy array to (and display) an image?,"I have created an array thusly:

import numpy as np
data = np.zeros( (512,512,3), dtype=np.uint8)
data[256,256] = [255,0,0]
What I want this to do is display a single red dot in the center of a ...",https://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image,"['I have created an array thusly:', 'You could use PIL to create (and display) an image:', 'The following should work:', 'Shortest path is to use scipy, like this:', 'I know there are simpler answers but this one will give you understanding of how images are actually drawn from a numpy array.', ""Using pygame, you can open a window, get the surface as an array of pixels, and manipulate as you want from there. You'll need to copy your numpy array into the surface array, however, which will be much slower than doing actual graphics operations on the pygame surfaces themselves."", ""Using pillow's fromarray, for example:"", 'The Python Imaging Library can display images using Numpy arrays.  Take a look at this page for sample code:', ""Supplement for doing so with matplotlib. I found it handy doing computer vision tasks.  Let's say you got data with dtype = int32"", 'this could be a possible code solution:']"
50,ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(),"I just discovered a logical bug in my code which was causing all sorts of problems. I was inadvertently doing a bitwise AND instead of a logical AND.

I changed the code from:

r = mlab.csv2rec(...",https://stackoverflow.com/questions/10062954/valueerror-the-truth-value-of-an-array-with-more-than-one-element-is-ambiguous,"['I just discovered a logical bug in my code which was causing all sorts of problems. I was inadvertently doing a bitwise AND instead of a logical AND.', 'r is a numpy (rec)array. So r[""dt""] >= startdate is also a (boolean)\narray. For numpy arrays the & operation returns the elementwise-and of the two\nboolean arrays.', ""I had the same problem (i.e. indexing with multi-conditions, here it's finding data in a certain date range). The (a-b).any() or (a-b).all() seem not working, at least for me."", 'The reason for the exception is that and implicitly calls bool. First on the left operand and (if the left operand is True) then on the right operand. So x and y is equivalent to bool(x) and bool(y).', 'if you work with pandas what solved the issue for me was that i was trying to do calculations when I had NA values, the solution was to run:', 'This typed error-message also shows while an if-statement comparison is done where there is an array and for example a bool or int. See for example:', 'try this=>  numpy.array(r)   or     numpy.array(yourvariable)   followed by the command to compare whatever you wish to.']"
51,Removing nan values from an array,"I want to figure out how to remove nan values from my array. My array looks something like this: 

x = [1400, 1500, 1600, nan, nan, nan ,1700] #Not in this exact configuration
How can I remove the ...",https://stackoverflow.com/questions/11620914/removing-nan-values-from-an-array,"['I want to figure out how to remove nan values from my array. My array looks something like this:', ""If you're using numpy for your arrays, you can also use"", 'works both for lists and numpy array\nsince v!=v only for NaN', 'Try this:', ""For me the answer by @jmetz didn't work, however using pandas isnull() did."", 'Doing the above :', ""If you're using numpy"", 'As shown by others', 'The accepted answer changes shape for 2d arrays.\nI present a solution here, using the Pandas dropna() functionality.\nIt works for 1D and 2D arrays. In the 2D case you can choose weather to drop the row or column containing np.nan.', 'A simplest way is:', ""@jmetz's answer is probably the one most people need; however it yields a one-dimensional array, e.g. making it unusable to remove entire rows or columns in matrices."", 'This is my approach to filter ndarray ""X"" for NaNs and infs,']"
52,How to remove specific elements in a numpy array,"How can I remove some specific elements from a numpy array? Say I have

import numpy as np

a = np.array([1,2,3,4,5,6,7,8,9])
I then want to remove 3,4,7 from a. All I know is the index of the values ...",https://stackoverflow.com/questions/10996140/how-to-remove-specific-elements-in-a-numpy-array,"['How can I remove some specific elements from a numpy array? Say I have', 'Use numpy.delete() - returns a new array with sub-arrays along an axis deleted', 'There is a numpy built-in function to help with that.', ""A Numpy array is immutable, meaning you technically cannot delete an item from it. However, you can construct a new array without the values you don't want, like this:"", 'To delete by value :', 'Not being a numpy person, I took a shot with:', 'Using np.delete is the fastest way to do it, if we know the indices of the elements that we want to remove. However, for completeness, let me add another way of ""removing"" array elements using a boolean mask created with the help of np.isin. This method allows us to remove the elements by specifying them directly or by their indices:', ""If you don't know the index, you can't use logical_and"", 'Remove specific index(i removed 16 and 21 from matrix)', 'You can also use sets:', 'list comprehension could be an interesting approach as well.', ""In case you don't have the indices of the elements you want to remove, you can use the function in1d provided by numpy.""]"
53,How do I calculate percentiles with python/numpy?,"Is there a convenient way to calculate percentiles for a sequence or single-dimensional numpy array?

I am looking for something similar to Excel's percentile function.

I looked in NumPy's statistics ...",https://stackoverflow.com/questions/2374640/how-do-i-calculate-percentiles-with-python-numpy,"['Is there a convenient way to calculate percentiles for a sequence or single-dimensional numpy array?', ""You might be interested in the SciPy Stats package. It has the percentile function you're after and many other statistical goodies."", ""By the way, there is a pure-Python implementation of percentile function, in case one doesn't want to depend on scipy.  The function is copied below:"", ""Here's how to do it without numpy, using only python to calculate the percentile."", 'The definition of percentile I usually see expects as a result the value from the supplied list below which P percent of values are found... which means the result must be from the set, not an interpolation between set elements.  To get that, you can use a simpler function.', 'Starting Python 3.8, the standard library comes with the quantiles function  as part of the statistics module:', 'check for scipy.stats module:', 'To calculate the percentile of a series, run:', 'In case you need the answer to be a member of the input numpy array:', 'for a series: used describe functions', 'A convenient way to calculate percentiles for a one-dimensional numpy sequence or matrix is by using numpy.percentile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.percentile.html>. Example:']"
54,How to normalize an array in NumPy?,"I would like to have the norm of one NumPy array. More specifically, I am looking for an equivalent version of this function

def normalize(v):
    norm = np.linalg.norm(v)
    if norm == 0: 
       ...",https://stackoverflow.com/questions/21030391/how-to-normalize-an-array-in-numpy,"['I would like to have the norm of one NumPy array. More specifically, I am looking for an equivalent version of this function', ""If you're using scikit-learn you can use sklearn.preprocessing.normalize:"", ""I would agree that it were nice if such a function was part of the included batteries. But it isn't, as far as I know. Here is a version for arbitrary axes, and giving optimal performance."", ""You can specify ord to get the L1 norm.\nTo avoid zero division I use eps, but that's maybe not great."", 'This might also work for you', 'If you have multidimensional data and want each axis normalized to its max or its sum:', 'There is also the function unit_vector() to normalize vectors in the popular transformations module by Christoph Gohlke:', 'You mentioned sci-kit learn, so I want to share another solution.', ""If you're working with 3D vectors, you can do this concisely using the toolbelt vg. It's a light layer on top of numpy and it supports single values and stacked vectors."", 'Without sklearn and using just numpy.\nJust define a function:.', 'If you want to normalize n dimensional feature vectors stored in a 3D tensor, you could also use PyTorch:', 'If you work with multidimensional array following fast solution is possible.', ""If you don't need utmost precision, your function can be reduced to:""]"
55,How to install python modules without root access?,"I'm taking some university classes and have been given an 'instructional account', which is a school account I can ssh into to do work. I want to run my computationally intensive Numpy, matplotlib, ...",https://stackoverflow.com/questions/7465445/how-to-install-python-modules-without-root-access,"[""I'm taking some university classes and have been given an 'instructional account', which is a school account I can ssh into to do work. I want to run my computationally intensive Numpy, matplotlib, scipy code on that machine, but I cannot install these modules because I am not a system administrator."", 'In most situations the best solution is to rely on the so-called ""user site"" location (see the PEP for details) by running:', 'No permissions to access nor install easy_install?', ""You can run easy_install to install python packages in your home directory even without root access. There's a standard way to do this using site.USER_BASE which defaults to something like $HOME/.local or $HOME/Library/Python/2.7/bin and is included by default on the PYTHONPATH"", 'If you have to use a distutils setup.py script, there are some commandline options for forcing an installation destination. See http://docs.python.org/install/index.html#alternate-installation. If this problem repeats, you can setup a distutils configuration file, see http://docs.python.org/install/index.html#inst-config-files.', 'Important question. The server I use (Ubuntu 12.04) had easy_install3 but not pip3. This is how I installed Pip and then other packages to my home folder', 'I use JuJu which basically allows to have a really tiny linux distribution (containing just the package manager) inside your $HOME/.juju directory.', 'The best and easiest way is this command:', 'Install virtualenv locally (source of instructions):']"
56,Split (explode) pandas dataframe string entry to separate rows,I have a pandas dataframe in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be ...,https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows,"[""I have a pandas dataframe in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be split on ','). For example, a should become b:"", 'How about something like this:', 'UPDATE2: more generic vectorized function, which will work for multiple normal and multiple list columns', 'After painful experimentation to find something faster than the accepted answer, I got this to work. It ran around 100x faster on the dataset I tried it on.', 'Series and DataFrame methods define a .explode() method that explodes lists into separate rows. See the docs section on Exploding a list-like column.', ""Here's a function I wrote for this common task. It's more efficient than the Series/stack methods. Column order and names are retained."", 'Similar question as: pandas: How do I split text in a column into multiple rows?', ""Let's create a new dataframe d that has lists"", 'There is a possibility to split and explode the dataframe without changing the structure of dataframe', ""I came up with a solution for dataframes with arbitrary numbers of columns (while still only separating one column's entries at a time)."", 'Here is a fairly straightforward message that uses the split method from pandas str accessor and then uses NumPy to flatten each row into a single array.', 'I have been struggling with out-of-memory experience using various way to explode my lists so I prepared some benchmarks to help me decide which answers to upvote. I tested five scenarios with varying proportions of the list length to the number of lists. Sharing the results below:', ""Based on the excellent @DMulligan's solution, here is a generic vectorized (no loops) function which splits a column of a dataframe into multiple rows, and merges it back to the original dataframe. It also uses a great generic change_column_order function from this answer."", ""The string function split can take an option boolean argument 'expand'."", 'One-liner using split(___, expand=True) and the level and name arguments to reset_index():', ""Just used jiln's excellent answer from above, but needed to expand to split multiple columns. Thought I would share."", ""upgraded MaxU's answer with MultiIndex support"", 'I have come up with the following solution to this problem:', 'Another solution that uses python copy package', ""There are a lot of answers here but I'm surprised no one has mentioned the built in pandas explode function. Check out the link below:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas.DataFrame.explode"", 'My version of the solution to add to this collection! :-)', 'I had a similar problem, my solution was converting the dataframe to a list of dictionaries first, then do the transition. Here is the function:', 'Upon adding few bits and pieces from all the solutions on this page, I was able to get something like this(for someone who need to use it right away).\nparameters to the function are df(input dataframe) and key(column that has delimiter separated string). Just replace with your delimiter if that is different to semicolon "";"".']"
57,How to smooth a curve in the right way?,"Lets assume we have a dataset which might be given approximately by

import numpy as np
x = np.linspace(0,2*np.pi,100)
y = np.sin(x) + np.random.random(100) * 0.2
Therefore we have a variation of 20% ...",https://stackoverflow.com/questions/20618804/how-to-smooth-a-curve-in-the-right-way,"['Lets assume we have a dataset which might be given approximately by', 'I prefer a Savitzky-Golay filter. It uses least squares to regress a small window of your data onto a polynomial, then uses the polynomial to estimate the point in the center of the window. Finally the window is shifted forward by one data point and the process repeats. This continues until every point has been optimally adjusted relative to its neighbors. It works great even with noisy samples from non-periodic and non-linear sources.', 'A quick and dirty way to smooth data I use, based on a moving average box (by convolution):', 'If you are interested in a ""smooth"" version of a signal that is periodic (like your example), then a FFT is the right way to go. Take the fourier transform and subtract out the low-contributing frequencies:', 'Fitting a moving average to your data would smooth out the noise, see this this answer for how to do that.', 'Another option is to use KernelReg in statsmodels:', 'Check this out! There is a clear definition of smoothing of a 1D signal.', 'This Question is already thoroughly answered, so I think a runtime analysis of the proposed methods would be of interest (It was for me, anyway). I will also look at the behavior of the methods at the center and the edges of the noisy dataset.', 'For a project of mine, I needed to create intervals for time-series modeling, and to make the procedure more efficient I created tsmoothie: A python library for time-series smoothing and outlier detection in a vectorized way.', 'If you are plotting time series graph and if you have used mtplotlib for drawing graphs then use \nmedian method to smooth-en the  graph']"
58,Import Error: No module named numpy,"I have a very similar question to this question, but still one step behind. I have only one version of Python 3 installed on my Windows 7 (sorry) 64-bit system.

I installed numpy following this link -...",https://stackoverflow.com/questions/7818811/import-error-no-module-named-numpy,"['I have a very similar question to this question, but still one step behind. I have only one version of Python 3 installed on my Windows 7 (sorry) 64-bit system.', 'Support for Python 3 was added in NumPy version 1.5.0, so to begin with, you must download/install a newer version of NumPy.', 'You can simply use', 'Installing Numpy on Windows', 'I think there are something wrong with the installation of numpy.\nHere are my steps to solve this problem.', 'I also had this problem (Import Error: No module named numpy) but in my case it was a problem with my PATH variables in Mac OS X.  I had made an earlier edit to my .bash_profile file that caused the paths for my Anaconda installation (and others) to not be added properly.', 'You installed the Numpy Version for Python 2.6 - so you can only use it with Python 2.6. You have to install Numpy for Python 3.x, e.g. that one: http://sourceforge.net/projects/numpy/files/NumPy/1.6.1/numpy-1.6.1-win32-superpack-python3.2.exe/download', 'I had this problem too after I installed Numpy. I solved it by just closing the Python interpreter and reopening. It may be something else to try if anyone else has this problem, perhaps it will save a few minutes!', 'You should try to install numpy using one of those:', 'I had numpy installed on the same environment both by pip and by conda, and simply removing and reinstalling either was not enough.', 'Faced with same issue', 'I too faced the above problem with phyton 3 while setting up python for machine learning.', ""I'm not sure exactly why I was getting the error, but pip3 uninstall numpy then pip3 install numpy resolved the issue for me."", 'For those using python 2.7, should try:', 'Those who are using xonsh, do xpip install numpy.', 'You can try:', 'For installing NumPy via Anaconda(use below commands):', 'For me, on windows 10, I had unknowingly installed multiple python versions (One from PyCharm IDE and another from Windows store). I uninstalled the one from windows Store and just to be thorough, uninstalled numpy pip uninstall numpy and then installed it again pip install numpy. It worked in the terminal in PyCharm and also in command prompt.', ""this is the problem of the numpy's version, please check out $CAFFE_ROOT/python/requirement.txt. Then exec: sudo apt-get install python-numpy>=x.x.x, this problem will be sloved."", 'I got this even though I knew numpy was installed and unsuccessfully tried all the advice above. The fix for me was to remove the as np  and directly refer to modules . (python 3.4.8 on Centos)\n.', 'After trying many suggestions from various sites and similar questions, what worked for me was to uninstall all Python stuff and reinstall Anaconda only (see https://stackoverflow.com/a/38330088/1083292)', 'If it was working before reinstalling python would solve the issue.', 'solution for me - I installed numpy inside a virtual environment, but then running ipython was not inside virtual env:', 'I was trying to use NumPy in Intellij but was facing the same issue so, I figured out that NumPy also comes with pandas. So, I installed pandas with IntelliJ tip and later on was able to import NumPy. Might help someone someday!', 'As stated in other answers, this error may refer to using the wrong python version. In my case, my environment is Windows 10 + Cygwin. In my Windows environment variables, the PATH points to C:\\Python38 which is correct, but when I run my command like this:']"
59,How to create a numpy array of all True or all False?,"In Python, how do I create a numpy array of arbitrary shape filled with all True or all False?",https://stackoverflow.com/questions/21174961/how-to-create-a-numpy-array-of-all-true-or-all-false,"['In Python, how do I create a numpy array of arbitrary shape filled with all True or all False?', 'numpy already allows the creation of arrays of all ones or all zeros very easily:', 'ones and zeros, which create arrays full of ones and zeros respectively, take an optional dtype parameter:', ""If it doesn't have to be writeable you can create such an array with np.broadcast_to:"", 'Quickly ran a timeit to see, if there are any differences between the np.full and np.ones version.', 'numpy.full(Size, Scalar Value, Type). There is other arguments as well that can be passed, for documentation on that, check https://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html', ""benchmark for Michael Currie's answer""]"
60,numpy matrix vector multiplication [duplicate],"When I multiply two numpy arrays of sizes (n x n)*(n x 1), I get a matrix of size (n x n). Following normal matrix multiplication rules, a (n x 1) vector is expected, but I simply cannot find any ...",https://stackoverflow.com/questions/21562986/numpy-matrix-vector-multiplication,"[""When I multiply two numpy arrays of sizes (n x n)*(n x 1), I get a matrix of size (n x n). Following normal matrix multiplication rules, a (n x 1) vector is expected, but I simply cannot find any information about how this is done in Python's Numpy module."", 'Use numpy.dot or a.dot(b). See the documentation here.']"
61,Transposing a NumPy array,"I use Python and NumPy and have some problems with ""transpose"":

import numpy as np
a = np.array([5,4])
print(a)
print(a.T)
Invoking a.T is not transposing the array.  If a is for example [[],[]] ...",https://stackoverflow.com/questions/5954603/transposing-a-numpy-array,"['I use Python and NumPy and have some problems with ""transpose"":', 'It\'s working exactly as it\'s supposed to. The transpose of a 1D array is still a 1D array!  (If you\'re used to matlab, it fundamentally doesn\'t have a concept of a 1D array. Matlab\'s ""1D"" arrays are 2D.)', 'Use two bracket pairs instead of one. This creates a 2D array, which can be transposed, unlike the 1D array you create if you use one bracket pair.', 'For 1D arrays:', 'You can convert an existing vector into a matrix by wrapping it in an extra set of square brackets...', 'numpy 1D array --> column/row matrix:', ""To 'transpose' a 1d array to a 2d column, you can use numpy.vstack:"", 'You can only transpose a 2D array. You can use numpy.matrix to create a 2D array. This is three years late, but I am just adding to the possible set of solutions:', 'instead use arr[:,None] to create column vector', 'The transpose of', 'Another solution.... :-)', 'I am just consolidating the above post, hope it will help others to save some time:', 'As some of the comments above mentioned, the transpose of 1D arrays are 1D arrays, so one way to transpose a 1D array would be to convert the array to a matrix like so:', 'The name of the function in numpy is column_stack.', 'There is a method not described in the answers but described in the documentation for the numpy.ndarray.transpose method:', 'Basically what the transpose function does is to swap the shape and strides of the array:', ""The way I've learned to implement this in a compact and readable manner for 1-D arrays, so far:""]"
62,Find unique rows in numpy.array,"I need to find unique rows in a numpy.array.

For example:

>>> a # I have
array([[1, 1, 1, 0, 0, 0],
       [0, 1, 1, 1, 0, 0],
       [0, 1, 1, 1, 0, 0],
       [1, 1, 1, 0, 0, 0],
       [...",https://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy-array,"['I need to find unique rows in a numpy.array.', 'As of NumPy 1.13, one can simply choose the axis for selection of unique values in any N-dim array. To get unique rows, one can do:', 'Yet another possible solution', 'Another option to the use of structured arrays is using a view of a void type that joins the whole row into a single item:', ""If you want to avoid the memory expense of converting to a series of tuples or another similar data structure, you can exploit numpy's structured arrays."", 'np.unique when I run it on np.random.random(100).reshape(10,10) returns all the unique individual elements, but you want the unique rows, so first you need to put them into tuples:', 'np.unique works by sorting a flattened array, then looking at whether each item is equal to the previous. This can be done manually without flattening:', 'Here is another variation for @Greg pythonic answer', ""I've compared the suggested alternative for speed and found that, surprisingly, the void view unique solution is even a bit faster than numpy's native unique with the axis argument. If you're looking for speed, you'll want"", 'I didn’t like any of these answers because none handle floating-point arrays in a linear algebra or vector space sense, where two rows being “equal” means “within some 𝜀”. The one answer that has a tolerance threshold, https://stackoverflow.com/a/26867764/500207, took the threshold to be both element-wise and decimal precision, which works for some cases but isn’t as mathematically general as a true vector distance.', 'Why not use drop_duplicates from pandas:', 'The numpy_indexed package (disclaimer: I am its author) wraps the solution posted by Jaime in a nice and tested interface, plus many more features:', 'np.unique works given a list of tuples:', ""Based on the answer in this page I have written a function that replicates the capability of MATLAB's unique(input,'rows') function, with the additional feature to accept tolerance for checking the uniqueness. It also returns the indices such that c = data[ia,:] and data = c[ic,:]. Please report if you see any discrepancies or errors."", 'Beyond @Jaime excellent answer, another way to collapse a row is to uses a.strides[0] (assuming a is C-contiguous) which is equal to a.dtype.itemsize*a.shape[0]. Furthermore void(n) is a shortcut for dtype((void,n)). we arrive finally to this shortest version :', 'For general purpose like 3D or higher multidimensional nested arrays, try this:', ""None of these answers worked for me. I'm assuming as my unique rows contained strings and not numbers. However this answer from another thread did work:"", 'We can actually turn m x n numeric numpy array into m x 1 numpy string array, please try using the following function, it provides count, inverse_idx and etc, just like numpy.unique:', 'Lets get the entire numpy matrix as a list, then drop duplicates from this list, and finally return our unique list back into a numpy matrix:', 'The most straightforward solution is to make the rows a single item by making them strings. Each row then can be  compared as a whole for its uniqueness using numpy. This solution is generalize-able you just need to reshape and transpose your array for other combinations. Here is the solution for the problem provided.']"
63,How does numpy.newaxis work and when to use it?,"When I try
numpy.newaxis

the result gives me a 2-d plot frame with x-axis from 0 to 1. However, when I try using numpy.newaxis to slice a vector,
vector[0:4,]
[ 0.04965172  0.04979645  0.04994022  0....",https://stackoverflow.com/questions/29241056/how-does-numpy-newaxis-work-and-when-to-use-it,"['When I try', 'Simply put, numpy.newaxis is used to increase the dimension of the existing array by one more dimension, when used once. Thus,', 'The np.newaxis is just an alias for the Python constant None, which means that wherever you use np.newaxis you could also use None:', 'You started with a one-dimensional list of numbers.  Once you used numpy.newaxis, you turned it into a two-dimensional matrix, consisting of four rows of one column each.', 'newaxis object in the selection tuple serves to expand the dimensions of the resulting selection by one unit-length dimension.']"
64,Convert a tensor to numpy array in Tensorflow?,How to convert a tensor into a numpy array when using Tensorflow with Python bindings?,https://stackoverflow.com/questions/34097281/convert-a-tensor-to-numpy-array-in-tensorflow,"['How to convert a tensor into a numpy array when using Tensorflow with Python bindings?', 'Eager Execution is enabled by default, so just call .numpy() on the Tensor object.', 'Any tensor returned by Session.run or eval is a NumPy array.', 'To convert back from tensor to numpy array you can simply run .eval() on the transformed tensor.', 'You need to:', 'Maybe you can try，this method:', 'I have faced and solved the tensor->ndarray conversion in the specific case of tensors representing (adversarial) images, obtained with cleverhans library/tutorials.', 'A simple example could be,', 'I was searching for days for this command.', 'You can use keras backend function.', 'If you see there is a method _numpy(),\ne.g for an EagerTensor simply call the above method and you will get an ndarray.']"
65,Understanding NumPy's einsum,"I'm struggling to understand exactly how einsum works. I've looked at the documentation and a few examples, but it's not seeming to stick.

Here's an example we went over in class:

C = np.einsum(""ij,...",https://stackoverflow.com/questions/26089893/understanding-numpys-einsum,"[""I'm struggling to understand exactly how einsum works. I've looked at the documentation and a few examples, but it's not seeming to stick."", '(Note: this answer is based on a short blog post about einsum I wrote a while ago.)', ""Grasping the idea of numpy.einsum() is very easy if you understand it intuitively. As an example, let's start with a simple description involving matrix multiplication."", 'Lets make 2 arrays, with different, but compatible dimensions to highlight their interplay', 'I found NumPy: The tricks of the trade (Part II) instructive', ""When reading einsum equations, I've found it the most helpful to just be able to\nmentally boil them down to their imperative versions."", 'I think the simplest example is in tensorflow docs']"
66,Moving average or running mean,Is there a SciPy function or NumPy function or module for Python that calculates the running mean of a 1D array given a specific window?,https://stackoverflow.com/questions/13728392/moving-average-or-running-mean,"['Is there a SciPy function or NumPy function or module for Python that calculates the running mean of a 1D array given a specific window?', 'For a short, fast solution that does the whole thing in one loop, without dependencies, the code below works great.', 'UPD: more efficient solutions have been proposed by Alleo and jasaarim.', 'Convolution is much better than straightforward approach, but (I guess) it uses FFT and thus quite slow. However specially for computing the running mean the following approach works fine', 'Update: The example below shows the old pandas.rolling_mean function which has been removed in recent versions of pandas. A modern equivalent of the function call below would be', 'You can calculate a running mean with:', 'You can use scipy.ndimage.filters.uniform_filter1d:', 'or module for python that calculates', 'For a ready-to-use solution, see https://scipy-cookbook.readthedocs.io/items/SignalSmooth.html.\nIt provides running average with the flat window type. Note that this is a bit more sophisticated than the simple do-it-yourself convolve-method, since it tries to handle the problems at the beginning and the end of the data by reflecting it (which may or may not work in your case...).', ""I know this is an old question, but here is a solution that doesn't use any extra data structures or libraries. It is linear in the number of elements of the input list and I cannot think of any other way to make it more efficient (actually if anyone knows of a better way to allocate the result, please let me know)."", 'I feel this can be elegantly solved using bottleneck', ""I haven't yet checked how fast this is, but you could try:"", 'This answer contains solutions using the Python standard library for three different scenarios.', ""A bit late to the party, but I've made my own little function that does NOT wrap around the ends or pads with zeroes that are then used to find the average as well. As a further treat is, that it also re-samples the signal at linearly spaced points. Customize the code at will to get other features."", 'Instead of numpy or scipy, I would recommend pandas to do this more swiftly:', 'There is a comment by mab buried in one of the answers above which has this method.  bottleneck has move_mean which is a simple moving average:', 'Another approach to find moving average without using numpy, panda', 'This question is now even older than when NeXuS wrote about it last month, BUT I like how his code deals with edge cases. However, because it is a ""simple moving average,"" its results lag behind the data they apply to. I thought that dealing with edge cases in a more satisfying way than NumPy\'s modes valid, same, and full could be achieved by applying a similar approach to a convolution() based method.', 'There are many answers above about calculating a running mean. My answer adds two extra features:', 'Use Only Python Standard Library (Memory Efficient)', ""With @Aikude's variables, I wrote one-liner."", 'All the aforementioned solutions are poor because they lack', 'Although there are solutions for this question here, please take a look at my solution. It is very simple and working well.', ""From reading the other answers I don't think this is what the question asked for, but I got here with the need of keeping a running average of a list of values that was growing in size."", 'Another solution just using a standard library and deque:', 'For educational purposes, let me add two more Numpy solutions (which are slower than the cumsum solution):', 'How about a moving average filter? It is also a one-liner and has the advantage, that you can easily manipulate the window type if you need something else than the rectangle, ie. a N-long simple moving average of an array a:', 'If you do choose to roll your own, rather than use an existing library, please be conscious of floating point error and try to minimize its effects:']"
67,Create numpy matrix filled with NaNs,"I have the following code:

r = numpy.zeros(shape = (width, height, 9))
It creates a width x height x 9 matrix filled with zeros. Instead, I'd like to know if there's a function or way to initialize ...",https://stackoverflow.com/questions/1704823/create-numpy-matrix-filled-with-nans,"['I have the following code:', 'You rarely need loops for vector operations in numpy.\nYou can create an uninitialized array and assign to all entries at once:', 'Another option is to use numpy.full, an option available in NumPy 1.8+', 'I compared the suggested alternatives for speed and found that, for large enough vectors/matrices to fill, all alternatives except val * ones and array(n * [val]) are equally fast.', 'Are you familiar with numpy.nan?', ""You can always use multiplication if you don't immediately recall the .empty or .full methods:"", 'Another alternative is numpy.broadcast_to(val,n) which returns in constant time regardless of the size and is also the most memory efficient (it returns a view of the repeated element). The caveat is that the returned value is read-only.', 'As said, numpy.empty() is the way to go. However, for objects, fill() might not do exactly what you think it does:', 'Yet another possibility not yet mentioned here is to use NumPy tile:']"
68,np.mean() vs np.average() in Python NumPy?,"I notice that 

In [30]: np.mean([1, 2, 3])
Out[30]: 2.0

In [31]: np.average([1, 2, 3])
Out[31]: 2.0
However, there should be some differences, since after all they are two different functions.

...",https://stackoverflow.com/questions/20054243/np-mean-vs-np-average-in-python-numpy,"['I notice that', 'np.average takes an optional weight parameter.  If it is not supplied they are equivalent.  Take a look at the source code: Mean, Average', 'np.mean always computes an arithmetic mean, and has some additional options for input and output (e.g. what datatypes to use, where to place the result).', 'In some version of numpy there is another imporant difference that you must be aware:', ""In addition to the differences already noted, there's another extremely important difference that I just now discovered the hard way:  unlike np.mean, np.average doesn't allow the dtype keyword, which is essential for getting correct results in some cases.  I have a very large single-precision array that is accessed from an h5 file.  If I take the mean along axes 0 and 1, I get wildly incorrect results unless I specify dtype='float64':"", 'In your invocation, the two functions are the same.']"
69,Calculating Pearson correlation and significance in Python,"I am looking for a function that takes as input two lists, and returns the Pearson correlation, and the significance of the correlation.",https://stackoverflow.com/questions/3949226/calculating-pearson-correlation-and-significance-in-python,"['I am looking for a function that takes as input two lists, and returns the Pearson correlation, and the significance of the correlation.', 'You can have a look at scipy.stats:', ""The Pearson correlation can be calculated with numpy's corrcoef."", 'An alternative can be a native scipy function from linregress which calculates:', ""If you don't feel like installing scipy, I've used this quick hack, slightly modified from Programming Collective Intelligence:"", 'The following code is a straight-up interpretation of the definition:', 'You can do this with pandas.DataFrame.corr, too:', 'Rather than rely on numpy/scipy, I think my answer should be the easiest to code and understand the steps in calculating the Pearson Correlation Coefficient (PCC) .', 'Pearson coefficient calculation using pandas in python: \nI would suggest trying this approach since your data contains lists. It will be easy to interact with your data and manipulate it from the console since you can visualise your data structure and update it as you wish. You can also export the data set and save it and add new data out of the python console for later analysis. This code is simpler and contains less lines of code. I am assuming you need a few quick lines of code to screen your data for further analysis', 'Hmm, many of these responses have long and hard to read code...', 'This is a implementation of Pearson Correlation function using numpy:', ""Here's a variant on mkh's answer that runs much faster than it, and scipy.stats.pearsonr, using numba."", 'Here is an implementation for pearson correlation based on sparse vector. The vectors here are expressed as a list of tuples expressed as (index, value). The two sparse vectors can be of different length but over all vector size will have to be same. This is useful for text mining applications where the vector size is extremely large due to most features being bag of words and hence calculations are usually performed using sparse vectors.', 'I have a very simple and easy to understand solution for this. For two arrays of equal length, Pearson coefficient can be easily computed as follows:', 'You may wonder how to interpret your probability in the context of looking for a correlation in a particular direction (negative or positive correlation.)  Here is a function I wrote to help with that.  It might even be right!', 'You can take a look at this article. This is a well-documented example for calculating correlation based on historical forex currency pairs data from multiple files using pandas library (for Python), and then generating a heatmap plot using seaborn library.']"
70,Concatenate a NumPy array to another NumPy array,"I have a numpy_array. Something like [ a b c ].
And then I want to concatenate it with another NumPy array (just like we create a list of lists). How do we create a NumPy array containing NumPy arrays?...",https://stackoverflow.com/questions/9775297/concatenate-a-numpy-array-to-another-numpy-array,"['I have a numpy_array. Something like [ a b c ].', 'or this:', ""Well, the error message says it all:  NumPy arrays do not have an append() method.  There's a free function numpy.append() however:"", 'You may use numpy.append()...', 'Sven said it all, just be very cautious because of automatic type adjustments when append is called.', 'I found this link while looking for something slightly different, how to start appending array objects to an empty numpy array, but tried all the solutions on this page to no avail.', 'Actually one can always create an ordinary list of numpy arrays and convert it later.', ""I had the same issue, and I couldn't comment on @Sven Marnach answer (not enough rep, gosh I remember when Stackoverflow first started...) anyway."", ""If I understand your question, here's one way.  Say you have:"", 'Try this code :']"
71,ValueError: setting an array element with a sequence,"This Python code:

import numpy as p

def firstfunction():
    UnFilteredDuringExSummaryOfMeansArray = []
    MeanOutputHeader=['TestID','ConditionName','FilterType','RRMean','HRMean',
                ...",https://stackoverflow.com/questions/4674473/valueerror-setting-an-array-element-with-a-sequence,"['This Python code:', ""From the code you showed us, the only thing we can tell is that you are trying to create an array from a list that isn't shaped like a multi-dimensional array.  For example"", 'The Python ValueError:', 'In my case , I got this Error in Tensorflow , Reason was i was trying to feed a array with different length or sequences :', 'for those who are having trouble with similar problems in Numpy, a very simple solution would be:', 'In my case, the problem was another. I was trying convert lists of lists of int to array. The problem was that there was one list with a different length than others. If you want to prove it, you must do:', 'In my case, the problem was with a scatterplot of a dataframe X[]:', 'When the shape is not regular or the elements have different data types, the dtype argument passed to np.array only can be object.', 'In my case, I had a nested list as the series that I wanted to use as an input.']"
72,Replace all elements of Python NumPy Array that are greater than some value,"I have a 2D NumPy array and would like to replace all values in it greater than or equal to a threshold T with 255.0. To my knowledge, the most fundamental way would be:

shape = arr.shape
result = np....",https://stackoverflow.com/questions/19666626/replace-all-elements-of-python-numpy-array-that-are-greater-than-some-value,"['I have a 2D NumPy array and would like to replace all values in it greater than or equal to a threshold T with 255.0. To my knowledge, the most fundamental way would be:', ""I think both the fastest and most concise way to do this is to use NumPy's built-in Fancy indexing. If you have an ndarray named arr, you can replace all elements >255 with a value x as follows:"", 'Since you actually want a different array which is arr where arr < 255, and 255 otherwise, this can be done simply:', 'I think you can achieve this the quickest by using the where function:', 'You can consider using numpy.putmask:', 'Another way is to use np.place which does in-place replacement and works with multidimentional arrays:', 'You can also use &, | (and/or) for more flexibility:']"
73,What are the differences between Pandas and NumPy+SciPy in Python? [closed],They both seem exceedingly similar and I'm curious as to which package would be more beneficial for financial data analysis.,https://stackoverflow.com/questions/11077023/what-are-the-differences-between-pandas-and-numpyscipy-in-python,"['Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.', 'pandas provides high level data manipulation tools built on top of NumPy. NumPy by itself is a fairly low-level tool, similar to MATLAB. pandas on the other hand provides rich time series functionality, data alignment, NA-friendly statistics, groupby, merge and join methods, and lots of other conveniences. It has become very popular in recent years in financial applications. I will have a chapter dedicated to financial data analysis using pandas in my upcoming book.', 'Numpy is required by pandas (and by virtually all numerical tools for Python).  Scipy is not strictly required for pandas but is listed as an ""optional dependency"".  I wouldn\'t say that pandas is an alternative to Numpy and/or Scipy.  Rather, it\'s an extra tool that provides a more streamlined way of working with numerical and tabular data in Python.  You can use pandas data structures but freely draw on Numpy and Scipy functions to manipulate them.', 'Pandas offer a great way to manipulate tables, as you can make binning easy (binning a dataframe in pandas in Python) and calculate statistics. Other thing that is great in pandas is the Panel class that you can join series of layers with different properties and combine it using groupby function.']"
74,List of lists into numpy array,How do I convert a simple list of lists into a numpy array? The rows are individual sublists and each row contains the elements in the sublist.,https://stackoverflow.com/questions/10346336/list-of-lists-into-numpy-array,"['How do I convert a simple list of lists into a numpy array? The rows are individual sublists and each row contains the elements in the sublist.', 'If your list of lists contains lists with varying number of elements then the answer of Ignacio Vazquez-Abrams will not work. Instead there are at least 3 options:', ""As this is the top search on Google for converting a list of lists into a Numpy array, I'll offer the following despite the question being 4 years old:"", ""It's as simple as:"", ""Again, after searching for the problem of converting nested lists with N levels into an N-dimensional array I found nothing, so here's my way around it:"", ""I had a list of lists of equal length. Even then Ignacio Vazquez-Abrams's answer didn't work out for me. I got a 1-D numpy array whose elements are lists. If you faced the same problem, you can use the below method"", 'Just use pandas']"
75,How to count the number of true elements in a NumPy bool array,"I have a NumPy array 'boolarr' of boolean type. I want to count the number of elements whose values are True. Is there a NumPy or Python routine dedicated for this task? Or, do I need to iterate over ...",https://stackoverflow.com/questions/8364674/how-to-count-the-number-of-true-elements-in-a-numpy-bool-array,"[""I have a NumPy array 'boolarr' of boolean type. I want to count the number of elements whose values are True. Is there a NumPy or Python routine dedicated for this task? Or, do I need to iterate over the elements in my script?"", 'You have multiple options. Two options are the following.', 'That question solved a quite similar question for me and I thought I should share :', 'In terms of comparing two numpy arrays and counting the number of matches (e.g. correct class prediction in machine learning), I found the below example for two dimensions useful:', 'axis = 1 will output number of trues in a row and axis = 0 will count number of trues in columns\nso']"
76,Is it possible to use argsort in descending order?,"Consider the following code:

avgDists = np.array([1, 8, 6, 9, 4])
ids = avgDists.argsort()[:n]
This gives me indices of the n smallest elements. Is it possible to use this same argsort in descending ...",https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order,"['Consider the following code:', 'If you negate an array, the lowest elements become the highest elements and vice-versa.  Therefore, the indices of the n highest elements are:', 'Just like Python, in that [::-1] reverses the array returned by argsort() and [:n] gives that last n elements:', 'Instead of using np.argsort you could use np.argpartition - if you only need the indices of the lowest/highest n elements.', 'You can use the flip commands numpy.flipud() or numpy.fliplr() to get the indexes in descending order after sorting using the argsort command. Thats what I usually do.', 'You could create a copy of the array and then multiply each element with -1.\nAs an effect the before largest elements would become the smallest.\nThe indeces of the n smallest elements in the copy are the n greatest elements in the original.', 'With your example:', 'As @Kanmani hinted, an easier to interpret implementation may use numpy.flip, as in the following:', 'An elegant way could be as follows -', 'Another way is to use only a \'-\' in the argument for argsort as in :  ""df[np.argsort(-df[:, 0])]"", provided df is the dataframe and you want to sort it by the first column (represented by the column number \'0\').  Change the column-name as appropriate.  Of course, the column has to be a numeric one.']"
77,How do I catch a numpy warning like it's an exception (not just for testing)?,I have to make a Lagrange polynomial in Python for a project I'm doing. I'm doing a barycentric style one to avoid using an explicit for-loop as opposed to a Newton's divided difference style one. The ...,https://stackoverflow.com/questions/15933741/how-do-i-catch-a-numpy-warning-like-its-an-exception-not-just-for-testing,"[""I have to make a Lagrange polynomial in Python for a project I'm doing. I'm doing a barycentric style one to avoid using an explicit for-loop as opposed to a Newton's divided difference style one. The problem I have is that I need to catch a division by zero, but Python (or maybe numpy) just makes it a warning instead of a normal exception."", 'It seems that your configuration is using the print option for numpy.seterr:', ""To add a little to @Bakuriu's answer:"", ""To elaborate on @Bakuriu's answer above, I've found that this enables me to catch a runtime warning in a similar fashion to how I would catch an error warning, printing out the warning nicely:"", 'Remove warnings.filterwarnings and add:']"
78,How can I check whether a numpy array is empty or not?,"How can I check whether a numpy array is empty or not?

I used the following code, but this fails if the array contains a zero.

if not self.Definition.all():
Is this the solution?

if self....",https://stackoverflow.com/questions/11295609/how-can-i-check-whether-a-numpy-array-is-empty-or-not,"['How can I check whether a numpy array is empty or not?', 'You can always take a look at the .size attribute. It is defined as an integer, and is zero (0) when there are no elements in the array:', 'http://www.scipy.org/Tentative_NumPy_Tutorial#head-6a1bc005bd80e1b19f812e1e64e0d25d50f99fe2', 'One caveat, though.\nNote that  np.array(None).size returns 1!\nThis is because a.size is equivalent to np.prod(a.shape),\nnp.array(None).shape is (), and an empty product is 1.', ""Why would we want to check if an array is empty?  Arrays don't grow or shrink in the same that lists do.  Starting with a 'empty' array, and growing with np.append is a frequent novice error.""]"
79,Take multiple lists into dataframe,"How do I take multiple lists and put them as different columns in a python dataframe? I tried this solution but had some trouble.

Attempt 1:
Have three lists, and zip them together and use that res =...",https://stackoverflow.com/questions/30522724/take-multiple-lists-into-dataframe,"['How do I take multiple lists and put them as different columns in a python dataframe? I tried this solution but had some trouble.', ""I think you're almost there, try removing the extra square brackets around the lst's (Also you don't need to specify the column names when you're creating a dataframe from a dict like this):"", ""Adding to Aditya Guru's answer here. There is no need of using map. You can do it simply by:"", 'Just adding that using the first approach it can be done as -', 'Adding one more scalable solution.', 'Adding to above answers, we can create on the fly', ""@oopsi used pd.concat() but didn't include the column names.  You could do the following, which, unlike the first solution in the accepted answer, gives you control over the column order (avoids dicts, which are unordered):"", 'There are several ways to create a dataframe from multiple lists.', 'you can simple use this following code']"
80,"Slicing of a NumPy 2d array, or how do I extract an mxm submatrix from an nxn array (n>m)?","I want to slice a NumPy nxn array. I want to extract an arbitrary selection of m rows and columns of that array (i.e. without any pattern in the numbers of rows/columns), making it a new, mxm array. ...",https://stackoverflow.com/questions/4257394/slicing-of-a-numpy-2d-array-or-how-do-i-extract-an-mxm-submatrix-from-an-nxn-ar,"['I want to slice a NumPy nxn array. I want to extract an arbitrary selection of m rows and columns of that array (i.e. without any pattern in the numbers of rows/columns), making it a new, mxm array. For this example let us say the array is 4x4 and I want to extract a 2x2 array from it.', 'As Sven mentioned, x[[[0],[2]],[1,3]] will give back the 0 and 2 rows that match with the 1 and 3 columns while x[[0,2],[1,3]] will return the values x[0,1] and x[2,3] in an array.', ""To answer this question, we have to look at how indexing a multidimensional array works in Numpy.  Let's first say you have the array x from your question.  The buffer assigned to x will contain 16 ascending integers from 0 to 15.  If you access one element, say x[i,j], NumPy has to figure out the memory location of this element relative to the beginning of the buffer.  This is done by calculating in effect i*x.shape[1]+j (and multiplying with the size of an int to get an actual memory offset)."", ""I don't think that x[[1,3]][:,[1,3]] is hardly readable. If you want to be more clear on your intent, you can do:"", 'If you want to skip every other row and every other column, then you can do it with basic slicing:', 'With numpy, you can pass a slice for each component of the index - so, your x[0:2,0:2] example above works.', 'I have a similar question here: Writting in sub-ndarray of a ndarray in the most pythonian way. Python 2\n.', ""I'm not sure how efficient this is but you can use range() to slice in both axis""]"
81,What is the inverse function of zip in python? [duplicate],"Possible Duplicate:
  A Transpose/Unzip Function in Python  
I've used the zip() function from the numpy library to sort tuples and now I have a list containing all the tuples. I had since modified ...",https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python,"['Possible Duplicate:\nA Transpose/Unzip Function in Python', 'should give you the unzipped list.']"
82,How do I build a numpy array from a generator?,"How can I build a numpy array out of a generator object?

Let me illustrate the problem:

>>> import numpy
>>> def gimme():
...   for x in xrange(10):
...     yield x
...
>>>...",https://stackoverflow.com/questions/367565/how-do-i-build-a-numpy-array-from-a-generator,"['How can I build a numpy array out of a generator object?', 'Numpy arrays require their length to be set explicitly at creation time, unlike python lists. This is necessary so that space for each item can be consecutively allocated in memory. Consecutive allocation is the key feature of numpy arrays: this combined with native code implementation let operations on them execute much quicker than regular lists.', 'One google behind this stackoverflow result, I found that there is a numpy.fromiter(data, dtype, count). The default count=-1 takes all elements from the iterable. It requires a dtype to be set explicitly. In my case, this worked:', 'While you can create a 1D array from a generator with numpy.fromiter(), you can create an N-D array from a generator with numpy.stack:', 'Somewhat tangential, but if your generator is a list comprehension, you can use numpy.where to more effectively get your result (I discovered this in my own code after seeing this post)', 'The vstack, hstack, and dstack functions can take as input generators that yield multi-dimensional arrays.']"
83,How do you get the magnitude of a vector in Numpy?,"In keeping with the ""There's only one obvious way to do it"", how do you get the magnitude of a vector (1D array) in Numpy?

def mag(x): 
    return math.sqrt(sum(i**2 for i in x))
The above works, ...",https://stackoverflow.com/questions/9171158/how-do-you-get-the-magnitude-of-a-vector-in-numpy,"['In keeping with the ""There\'s only one obvious way to do it"", how do you get the magnitude of a vector (1D array) in Numpy?', ""The function you're after is numpy.linalg.norm. (I reckon it should be in base numpy as a property of an array -- say x.norm() -- but oh well)."", 'If you are worried at all about speed, you should instead use:', 'Yet another alternative is to use the einsum function in numpy for either arrays:', ""Fastest way I found is via inner1d. Here's how it compares to other numpy methods:"", 'use the function norm in scipy.linalg (or numpy.linalg)', ""You can do this concisely using the toolbelt vg. It's a light layer on top of numpy and it supports single values and stacked vectors.""]"
84,Extracting specific columns in numpy array,"This is an easy question but say I have an MxN matrix. All I want to do is extract specific columns and store them in another numpy array but I get invalid syntax errors.
Here is the code:

...",https://stackoverflow.com/questions/8386675/extracting-specific-columns-in-numpy-array,"['This is an easy question but say I have an MxN matrix. All I want to do is extract specific columns and store them in another numpy array but I get invalid syntax errors.\nHere is the code:', 'I assume you wanted columns 1 and 9?', 'Assuming you want to get columns 1 and 9 with that code snippet, it should be:', 'if you want to extract only some columns:', 'One thing I would like to point out is, if the number of columns you want to extract is 1 the resulting matrix would not be a Mx1 Matrix as you might expect but instead an array containing the elements of the column you extracted.', 'Just:', 'One more thing you should pay attention to when selecting columns from N-D array using a list like this:', 'You can use the following:', 'I think the solution here is not working with an update of the python version anymore, one way to do it with a new python function for it is:', 'you can also use extractedData=data([:,1],[:,9])']"
85,Numpy - add row to array,"How does one add rows to a numpy array?

I have an array A:

A = array([[0, 1, 2], [0, 2, 0]])
I wish to add rows to this array from another array X if the first element of each row in X meets a ...",https://stackoverflow.com/questions/3881453/numpy-add-row-to-array,"['How does one add rows to a numpy array?', 'What is X? If it is a 2D-array, how can you then compare its row to a number: i < 3?', 'well u can do this :', 'As this question is been 7 years before, in the latest version which I am using is numpy version 1.13, and python3, I am doing the same thing with adding a row to a matrix, remember to put a double bracket to the second argument, otherwise, it will raise dimension error.', ""If no calculations are necessary after every row, it's much quicker to add rows in python, then convert to numpy. Here are timing tests using python 3.6 vs. numpy 1.14, adding 100 rows, one at a time:"", 'You can also do this:', ""I use 'np.vstack' which is faster, EX:"", 'If you can do the construction in a single operation, then something like the vstack-with-fancy-indexing answer is a fine approach. But if your condition is more complicated or your rows come in on the fly, you may want to grow the array. In fact the numpythonic way to do something like this - dynamically grow an array - is to dynamically grow a list:', 'You can use numpy.append() to append a row to numpty array and reshape to a matrix later on.', ""I use numpy.insert(arr, i, the_object_to_be_added, axis) in order to insert  object_to_be_added at the i'th row(axis=0) or column(axis=1)""]"
86,How do I get PyLint to recognize numpy members?,"I am running PyLint on a Python project. PyLint makes many complaints about being unable to find numpy members. How can I avoid this while avoiding skipping membership checks.

From the code:

import ...",https://stackoverflow.com/questions/20553551/how-do-i-get-pylint-to-recognize-numpy-members,"['I am running PyLint on a Python project. PyLint makes many complaints about being unable to find numpy members. How can I avoid this while avoiding skipping membership checks.', ""If using Visual Studio Code with Don Jayamanne's excellent Python extension, add a user setting to whitelist numpy:"", 'I had the same issue here, even with the latest versions of all related packages (astroid 1.3.2, logilab_common 0.63.2, pylon 1.4.0).', 'I was getting the same error for a small numpy project I was working on and decided that ignoring the numpy modules would do just fine. I created a .pylintrc file with:', 'In recent versions of pylint you can add --extension-pkg-whitelist=numpy to your pylint command. They had fixed this problem in an earlier version in an unsafe way. Now if you want them to look more carefully at a package outside of the standard library, you must explicitly whitelist it. See here.', 'Since this is the top result in google and it gave me the impression that you have to ignore those warnings in all files:', 'For ignoring all the errors generated by numpy.core‘s attributes, we can now use:', ""If you don't want to add more config, please add this code to your config file, instead of 'whitelist'."", 'There have been many different bugs reported about this over the past few years i.e. https://bitbucket.org/logilab/pylint/issue/58/false-positive-no-member-on-numpy-imports', ""Probably, it's confused with numpy's abstruse method of methods import. Namely, zeros is in fact numpy.core.multiarray.zeros, imported in numpy with statement"", 'I had to add this at the top of any file where I use numpy a lot.', 'In Extension to j_hougs answer, you can now add the modules in question to this line in .pylintrc, which is already prepared empty on generation:', 'This has finally been resolved in Pylint 1.8.2. Works out of the box, no pylintrc tweaks needed!', 'I had the same problem with a different module (kivy.properties) which is a wrapped C module like numpy.', 'This is the pseudo-solution I have come up with for this problem.', 'I had this problem with numpy, scipy, sklearn, nipy, etc., and I solved it by wrapping epylint like so:', 'This seems to work on at least Pylint 1.1.0:', 'This solution worked for me', 'A little bit of copy paste from the previous answer to summarize what is working (at least for me: debian-jessie)', 'I\'ve been working on a patch to pylint to solve the issue with dynamic members in libraries such as numpy. It adds a ""dynamic-modules"" option which forces to check if members exist during runtime by making a real import of the module. See Issue #413 in logilab/pylint. There is also a pull request, see link in one of the comments.', 'A quick answer: update Pylint to 1.7.1 (use conda-forge provided Pylint 1.7.1 if you use conda to manage packages)', ""I'm not sure if this is a solution, but in VSCode once I wrote explicitly in my user settings to enable pylint, all modules were recognized."", 'Lately (since something changed in spyder or pylint or ?), I have been getting E1101 errors (""no member"") from spyder\'s static code analysis on astropy.constants symbols.  No idea why.']"
87,Windows Scipy Install: No Lapack/Blas Resources Found,"I am trying to install python and a series of packages onto a 64bit windows 7 desktop. I have installed Python 3.4, have Microsoft Visual Studio C++ installed, and have successfully installed numpy, ...",https://stackoverflow.com/questions/28190534/windows-scipy-install-no-lapack-blas-resources-found,"['I am trying to install python and a series of packages onto a 64bit windows 7 desktop. I have installed Python 3.4, have Microsoft Visual Studio C++ installed, and have successfully installed numpy, pandas and a few others. I am getting the following error when trying to install scipy;', 'The solution to the absence of BLAS/LAPACK libraries for SciPy installations on Windows 7 64-bit is described here:', 'The following link should solve all problems with Windows and SciPy; just choose the appropriate download. I was able to pip install the package with no problems.  Every other solution I have tried gave me big headaches.', 'It will be successful installed.', 'Sorry to necro, but this is the first google search result. This is the solution that worked for me:', 'This was the order I got everything working. The second point is the most important one. Scipy needs Numpy+MKL, not just vanilla Numpy.', 'If you are working with Windows and Visual Studio 2015', 'Simple and Fast Installation of Scipy in Windows', 'My 5  cents; You can just install the entire (pre-compiled) SciPy from \nhttps://github.com/scipy/scipy/releases', 'For python27\n1、Install numpy + mkl（download link:http://www.lfd.uci.edu/~gohlke/pythonlibs/）\n2、install scipy (the same site)\nOK!', 'Intel now provides a Python distribution for Linux / Windows / OS X for free called ""Intel distribution for Python"".', 'I was also getting same error while installing scikit-fuzzy. I resolved error as follows:', 'Solutions:', 'do this, it solved for me \n        pip install -U scikit-learn', 'This page has overcomplicated solutions to the problem. Most of numpy / scipy users should not need to compile their numpy installations or need to rely on 3rd party ""numpy+mkl"" wheels.', 'Using resources at  http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipy will solve the problem. However, you should be careful about versions compatibility. After trying for several times, finally I decided to uninstall python and then installed a fresh version of python along with numpy and then installed scipy and this resolved my problem.', ""install intel's distribution of python https://software.intel.com/en-us/intel-distribution-for-python""]"
88,"numpy.where() detailed, step-by-step explanation / examples [closed]","I have trouble properly understanding numpy.where() despite reading the doc, this post and this other post.

Can someone provide step-by-step commented examples with 1D and 2D arrays?",https://stackoverflow.com/questions/34667282/numpy-where-detailed-step-by-step-explanation-examples,"['Want to improve this question? Update the question so it focuses on one problem only by editing this post.', 'After fiddling around for a while, I figured things out, and am posting them here hoping it will help others.', ""Here is a little more fun. I've found that very often NumPy does exactly what I wish it would do - sometimes it's faster for me to just try things than it is to read the docs. Actually a mixture of both is best.""]"
89,NumPy or Pandas: Keeping array type as integer while having a NaN value,"Is there a preferred way to keep the data type of a numpy array fixed as int (or int64 or whatever), while still having an element inside listed as numpy.NaN?

In particular, I am converting an in-...",https://stackoverflow.com/questions/11548005/numpy-or-pandas-keeping-array-type-as-integer-while-having-a-nan-value,"['Is there a preferred way to keep the data type of a numpy array fixed as int (or int64 or whatever), while still having an element inside listed as numpy.NaN?', 'This capability has been added to pandas (beginning with version 0.24):\nhttps://pandas.pydata.org/pandas-docs/version/0.24/whatsnew/v0.24.0.html#optional-integer-na-support', ""NaN can't be stored in an integer array. This is a known limitation of pandas at the moment; I have been waiting for progress to be made with NA values in NumPy (similar to NAs in R), but it will be at least 6 months to a year before NumPy gets these features, it seems:"", 'If performance is not the main issue, you can store strings instead.', ""This is not a solution for all cases, but mine (genomic coordinates) I've resorted to using 0 as NaN"", 'Functionality to support NaN in integer series will be available in v0.24 upwards. There\'s information on this in the v0.24 ""What\'s New"" section, and more details under Nullable Integer Data Type.', 'This is now possible, since pandas v 0.24.0', 'Just wanted to add that in case you are trying to convert a float (1.143) vector to integer (1) that has NA converting to the new \'Int64\' dtype will give you an error. In order to solve this you have to round the numbers and then do "".astype(\'Int64\')""', 'If there are blanks in the text data, columns that would normally be integers will be cast to floats as float64 dtype because int64 dtype cannot handle nulls. This can cause inconsistent schema if you are loading multiple files some with blanks (which will end up as float64 and others without which will end up as int64']"
90,“Cloning” row or column vectors,"Sometimes it is useful to ""clone"" a row or column vector to a matrix. By cloning I mean converting a row vector such as

[1,2,3]
Into a matrix

[[1,2,3]
 [1,2,3]
 [1,2,3]
]
or a column vector such ...",https://stackoverflow.com/questions/1550130/cloning-row-or-column-vectors,"['Sometimes it is useful to ""clone"" a row or column vector to a matrix. By cloning I mean converting a row vector such as', ""Here's an elegant, Pythonic way to do it:"", 'Use numpy.tile:', ""First note that with numpy's broadcasting operations it's usually not necessary to duplicate rows and columns.  See this and this for descriptions."", 'Let:', 'I think using the broadcast in numpy is the best, and faster', ""One clean solution is to use NumPy's outer-product function with a vector of ones:"", 'You can use', 'If you have a pandas dataframe and want to preserve the dtypes, even the categoricals, this is a fast way to do it:', 'yields:']"
91,How to do exponential and logarithmic curve fitting in Python? I found only polynomial fitting,"I have a set of data and I want to compare which line describes it best (polynomials of different orders, exponential or logarithmic).

I use Python and Numpy and for polynomial fitting there is a ...",https://stackoverflow.com/questions/3433486/how-to-do-exponential-and-logarithmic-curve-fitting-in-python-i-found-only-poly,"['I have a set of data and I want to compare which line describes it best (polynomials of different orders, exponential or logarithmic).', 'For fitting y = A + B log x, just fit y against (log x).', 'You can also fit a set of a data to whatever function you like using curve_fit from scipy.optimize.  For example if you want to fit an exponential function (from the documentation):', 'I was having some trouble with this so let me be very explicit so noobs like me can understand.', 'Well I guess you can always use:', ""Here's a linearization option on simple data that uses tools from scikit learn."", 'Wolfram has a closed form solution for fitting an exponential.  They also have similar solutions for fitting a logarithmic and power law.', 'We demonstrate features of lmfit while solving both problems.']"
92,Suppress Scientific Notation in Numpy When Creating Array From Nested List,"I have a nested Python list that looks like the following:

my_list = [[3.74, 5162, 13683628846.64, 12783387559.86, 1.81],
 [9.55, 116, 189688622.37, 260332262.0, 1.97],
 [2.2, 768, 6004865.13, ...",https://stackoverflow.com/questions/9777783/suppress-scientific-notation-in-numpy-when-creating-array-from-nested-list,"['I have a nested Python list that looks like the following:', 'I guess what you need is np.set_printoptions(suppress=True), for details see here:\nhttp://pythonquirks.blogspot.fr/2009/10/controlling-printing-in-numpy.html', 'What follows is an explanation for what is going on, scroll to bottom for code demos.', 'for 1D and 2D arrays you can use np.savetxt to print using a specific format string:', 'You could write a function that converts a scientific notation to regular, something like']"
93,Numpy: Get random set of rows from 2D array,"I have a very large 2D array which looks something like this:

a=
[[a1, b1, c1],
 [a2, b2, c2],
 ...,
 [an, bn, cn]]
Using numpy, is there an easy way to get a new 2D array with, e.g., 2 random rows ...",https://stackoverflow.com/questions/14262654/numpy-get-random-set-of-rows-from-2d-array,"['I have a very large 2D array which looks something like this:', 'Putting it together for a general case:', 'This is an old post, but this is what works best for me:', 'Another option is to create a random mask if you just want to down-sample your data by a certain factor. Say I want to down-sample to 25% of my original data set, which is currently held in the array data_arr:', ""This is a similar answer to the one Hezi Rasheff provided, but simplified so newer python users understand what's going on (I noticed many new datascience students fetch random samples in the weirdest ways because they don't know what they are doing in python)."", 'I see permutation has been suggested. In fact it can be made into one line:', 'If you need the same rows but just a random sample then,', 'If you want to generate multiple random subsets of rows, for example if your doing RANSAC.', 'An alternative way of doing it is by using the choice method of the Generator class, https://github.com/numpy/numpy/issues/10835']"
94,How to add a new row to an empty numpy array,"Using standard Python arrays, I can do the following:

arr = []
arr.append([1,2,3])
arr.append([4,5,6])
# arr is now [[1,2,3],[4,5,6]]
However, I cannot do the same thing in numpy. For example:

arr =...",https://stackoverflow.com/questions/22392497/how-to-add-a-new-row-to-an-empty-numpy-array,"['Using standard Python arrays, I can do the following:', 'The way to ""start"" the array that you want is:', 'Here is my solution:', 'In this case you might want to use the functions np.hstack and np.vstack', 'using an custom dtype definition, what worked for me was:', 'In case of adding new rows for array in loop, Assign the array directly for firsttime in loop instead of initialising an empty array.', ""I want to do a for loop, yet with askewchan's method it does not work well, so I have modified it.""]"
95,How do you use the ellipsis slicing syntax in Python?,"This came up in  Hidden features of Python, but I can't see good documentation or examples that explain how the feature works.",https://stackoverflow.com/questions/118370/how-do-you-use-the-ellipsis-slicing-syntax-in-python,"[""This came up in  Hidden features of Python, but I can't see good documentation or examples that explain how the feature works."", ""Ellipsis, or ... is not a hidden feature, it's just a constant. It's quite different to, say, javascript ES6 where it's a part of the language syntax. No builtin class or Python language constuct makes use of it."", 'The ellipsis is used in numpy to slice higher-dimensional data structures.', 'This is another use for Ellipsis, which has nothing to do with slices: I often use it in intra-thread communication with queues, as a mark that signals ""Done""; it\'s there, it\'s an object, it\'s a singleton, and its name means ""lack of"", and it\'s not the overused None (which could be put in a queue as part of normal data flow). YMMV.', 'As stated in other answers, it can be used for creating slices.\nUseful when you do not want to write many full slices notations (:), or when you are just not sure on what is dimensionality of the array being manipulated.']"
96,How do I select elements of an array given condition?,"Suppose I have a numpy array x = [5, 2, 3, 1, 4, 5], y = ['f', 'o', 'o', 'b', 'a', 'r']. I want to select the elements in y corresponding to elements in x that are greater than 1 and less than 5.

I ...",https://stackoverflow.com/questions/3030480/how-do-i-select-elements-of-an-array-given-condition,"[""Suppose I have a numpy array x = [5, 2, 3, 1, 4, 5], y = ['f', 'o', 'o', 'b', 'a', 'r']. I want to select the elements in y corresponding to elements in x that are greater than 1 and less than 5."", 'Your expression works if you add parentheses:', 'IMO OP does not actually want np.bitwise_and() (aka &) but actually wants np.logical_and() because they are comparing logical values such as True and False - see this SO post on logical vs. bitwise to see the difference.', ""Add one detail to @J.F. Sebastian's and @Mark Mikofski's answers:\nIf one wants to get the corresponding indices (rather than the actual values of array), the following code will do:"", 'I like to use np.vectorize for such tasks. Consider the following:', 'Actually I would do it this way:', 'For 2D arrays, you can do this. Create a 2D mask using the condition. Typecast the condition mask to int or float, depending on the array, and multiply it with the original array.']"
97,Find p-value (significance) in scikit-learn LinearRegression,"How can I find the p-value (significance) of each coefficient?

lm = sklearn.linear_model.LinearRegression()
lm.fit(x,y)",https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression,"['How can I find the p-value (significance) of each coefficient?', ""This is kind of overkill but let's give it a go.  First lets use statsmodel to find out what the p-values should be"", ""scikit-learn's LinearRegression doesn't calculate this information but you can easily extend the class to do it:"", 'EDIT: Probably not the right way to do it, see comments', ""The code in elyase's answer https://stackoverflow.com/a/27928411/4240413 does not actually work.  Notice that sse is a scalar, and then it tries to iterate through it.  The following code is a modified version.  Not amazingly clean, but I think it works more or less."", 'An easy way to pull of the p-values is to use statsmodels regression:', ""There could be a mistake in @JARH's answer in the case of a multivariable regression. \n(I do not have enough reputation to comment.)"", 'p_value is among f statistics. if you want to get the value, simply use this few lines of code:', 'You can use scipy for p-value. This code is from scipy documentation.', 'For a one-liner you can use the pingouin.linear_regression function (disclaimer: I am the creator of Pingouin), which works with uni/multi-variate regression using NumPy arrays or Pandas DataFrame, e.g:']"
98,Python memory usage of numpy arrays,"I'm using python to analyse some large files and I'm running into memory issues, so I've been using sys.getsizeof() to try and keep track of the usage, but it's behaviour with numpy arrays is bizarre. ...",https://stackoverflow.com/questions/11784329/python-memory-usage-of-numpy-arrays,"[""I'm using python to analyse some large files and I'm running into memory issues, so I've been using sys.getsizeof() to try and keep track of the usage, but it's behaviour with numpy arrays is bizarre. Here's an example involving a map of albedos that I'm having to open:"", 'You can use array.nbytes for numpy arrays, for example:', 'The field nbytes will give you the size in bytes of all the elements of the array in a numpy.array:', ""In python notebooks I often want to filter out 'dangling' numpy.ndarray's, in particular the ones that are stored in _1, _2, etc that were never really meant to stay alive.""]"
99,Installing SciPy and NumPy using pip,"I'm trying to create required libraries in a package I'm distributing. It requires both the SciPy and NumPy libraries.
While developing, I installed both using

apt-get install scipy
which installed ...",https://stackoverflow.com/questions/11114225/installing-scipy-and-numpy-using-pip,"[""I'm trying to create required libraries in a package I'm distributing. It requires both the SciPy and NumPy libraries.\nWhile developing, I installed both using"", 'I am assuming Linux experience in my answer; I found that there are three prerequisites to getting pip install scipy to proceed nicely.', 'This worked for me on Ubuntu 14.04:', 'you need the libblas and liblapack dev packages if you are using Ubuntu.', 'Since the previous instructions for installing with yum are broken here are the updated instructions for installing on something like fedora. I\'ve tested this on ""Amazon Linux AMI 2016.03""', 'I was working on a project that depended on numpy and scipy. In a clean installation of Fedora 23, using a python virtual environment for Python 3.4 (also worked for Python 2.7), and with the following in my setup.py (in the setup() method)', 'On windows python 3.5, I managed to install scipy by using conda not pip:', ""What operating system is this? The answer might depend on the OS involved. However, it looks like you need to find this BLAS library and install it. It doesn't seem to be in PIP (you'll have to do it by hand thus), but if you install it, it ought let you progress your SciPy install."", ""in my case, upgrading pip did the trick. Also, I've installed scipy with -U parameter (upgrade all packages to the last available version)""]"
100,Numpy matrix to array,"I am using numpy. I have a matrix with 1 column and N rows and I want to get an array from with N elements.

For example, if i have M = matrix([[1], [2], [3], [4]]), I want to get A = array([1,2,3,4])....",https://stackoverflow.com/questions/3337301/numpy-matrix-to-array,"['I am using numpy. I have a matrix with 1 column and N rows and I want to get an array from with N elements.', ""If you'd like something a bit more readable, you can do this:"", 'https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.matrix.A1.html', 'depends what you mean by elegance i suppose but thats what i would do', 'You can try the following variant:', 'If you care for speed; But if you care for memory:', 'Or you could try to avoid some temps with', 'First, Mv = numpy.asarray(M.T), which gives you a 4x1 but 2D array.', 'This will convert the matrix into array', 'ravel() and flatten() functions from numpy are two techniques that I would try here. I will like to add to the posts made by Joe, Siraj, bubble and Kevad.']"
101,"How to split data into 3 sets (train, validation and test)?","I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using train_test_split from sklearn.cross_validation, one can divide the data in two sets (train and test). However, I ...",https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test,"[""I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using train_test_split from sklearn.cross_validation, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data."", 'Numpy solution. We will shuffle the whole dataset first (df.sample(frac=1, random_state=42)) and then split our data set into the following parts:', ""Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets."", 'However, one approach to dividing the dataset into train, test, cv with 0.6, 0.2, 0.2 would be to use the train_test_split method twice.', ""Here is a Python function that splits a Pandas dataframe into train, validation, and test dataframes with stratified sampling. It performs this split by calling scikit-learn's function train_test_split() twice."", 'It is very convenient to use train_test_split without performing reindexing after dividing to several sets and not writing some additional code. Best answer above does not mention that by separating two times using train_test_split not changing partition sizes won`t give initially intended partition:', 'In the case of supervised learning, you may want to split both X and y (where X is your input and y the ground truth output).\nYou just have to pay attention to shuffle X and y the same way before splitting.']"
102,Numpy first occurrence of value greater than existing value,"I have a 1D array in numpy and I want to find the position of the index where a value exceeds the value in numpy array.

E.g.

aa = range(-10,10)
Find position in aa where, the value 5 gets exceeded.",https://stackoverflow.com/questions/16243955/numpy-first-occurrence-of-value-greater-than-existing-value,"['I have a 1D array in numpy and I want to find the position of the index where a value exceeds the value in numpy array.', 'This is a little faster (and looks nicer)', 'given the sorted content of your array, there is an even faster method: searchsorted.', ""I was also interested in this and I've compared all the suggested answers with perfplot. (Disclaimer: I'm the author of perfplot.)"", 'In case of a range or any other linearly increasing array you can simply calculate the index programmatically, no need to actually iterate over the array at all:', ""I'd like to propose"", 'I would go with']"
103,Cartesian product of x and y array points into single array of 2D points,"I have two numpy arrays that define the x and y axes of a grid.  For example:

x = numpy.array([1,2,3])
y = numpy.array([4,5])
I'd like to generate the Cartesian product of these arrays to generate:

...",https://stackoverflow.com/questions/11144513/cartesian-product-of-x-and-y-array-points-into-single-array-of-2d-points,"['I have two numpy arrays that define the x and y axes of a grid.  For example:', 'See Using numpy to build an array of all combinations of two arrays for a general solution for computing the Cartesian product of N arrays.', ""There are many approaches to this problem with different properties. Some are faster than others, and some are more general-purpose. After a lot of testing and tweaking, I've found that the following function, which calculates an n-dimensional cartesian_product, is faster than most others for many inputs. For a pair of approaches that are slightly more complex, but are even a bit faster in many cases, see the answer by Paul Panzer."", 'You can just do normal list comprehension in python', ""I was interested in this as well and did a little performance comparison, perhaps somewhat clearer than in @senderle's answer."", ""Building on @senderle's exemplary ground work I've come up with two versions - one for C and one for Fortran layouts - that are often a bit faster."", 'As of Oct. 2017, numpy now has a generic np.stack function that takes an axis parameter.  Using it, we can have  a ""generalized cartesian product"" using the ""dstack and meshgrid"" technique:', 'I used @kennytm answer for a while, but when trying to do the same in TensorFlow, but I found that TensorFlow has no equivalent of numpy.repeat(). After a little experimentation, I think I found a more general solution for arbitrary vectors of points.', 'The Scikit-learn package has a fast implementation of exactly this:', 'More generally, if you have two 2d numpy arrays a and b, and you want to concatenate every row of a to every row of b (A cartesian product of rows, kind of like a join in a database), you can use this method:', 'The fastest you can get is either by combining a generator expression with the map function:', 'This can also be easily done by using itertools.product method', 'In the specific case that you need to perform simple operations such as addition on each pair, you can introduce an extra dimension and let broadcasting do the job:', ""I'm a bit late to the party, but I encoutered a tricky variant of that problem.\nLet's say I want the cartesian product of several arrays, but that cartesian product ends up being much larger than the computers' memory (however, the computation done with that product are fast, or at least parallelizable).""]"
104,Find indices of elements equal to zero in a NumPy array,NumPy has the efficient function/method nonzero() to identify the indices of non-zero elements in an ndarray object. What is the most efficient way to obtain the indices of the elements that do have a ...,https://stackoverflow.com/questions/4588628/find-indices-of-elements-equal-to-zero-in-a-numpy-array,"['NumPy has the efficient function/method nonzero() to identify the indices of non-zero elements in an ndarray object. What is the most efficient way to obtain the indices of the elements that do have a value of zero?', 'numpy.where() is my favorite.', 'There is np.argwhere,', 'You can search for any scalar condition with:', 'You can also use nonzero() by using it on a boolean mask of the condition, because False is also a kind of zero.', 'You can use numpy.nonzero to find zero.', 'If you are working with a one-dimensional array there is a syntactic sugar:', 'I would do it the following way:']"
105,Using numpy to build an array of all combinations of two arrays,"I'm trying to run over the parameters space of a 6 parameter function to study it's numerical behavior before trying to do anything complex with it so I'm searching for a efficient way to do this.

My ...",https://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays,"[""I'm trying to run over the parameters space of a 6 parameter function to study it's numerical behavior before trying to do anything complex with it so I'm searching for a efficient way to do this."", 'In newer version of numpy (>1.8.x), numpy.meshgrid() provides a much faster implementation:', ""Here's a pure-numpy implementation. It's about 5× faster than using itertools."", ""itertools.combinations is in general the fastest way to get combinations from a Python container (if you do in fact want combinations, i.e., arrangements WITHOUT repetitions and independent of order; that's not what your code appears to be doing, but I can't tell whether that's because your code is buggy or because you're using the wrong terminology)."", 'You can do something like this', 'The following numpy implementation should be approx. 2x the speed of the given answer:', 'It looks like you want a grid to evaluate your function,  in which case you can use numpy.ogrid (open) or numpy.mgrid (fleshed out):', 'you can use np.array(itertools.product(a, b))', ""Here's yet another way, using pure NumPy, no recursion, no list comprehension, and no explicit for loops. It's about 20% slower than the original answer, and it's based on np.meshgrid."", 'For a pure numpy implementation of Cartesian product of 1D arrays (or flat python lists), just use meshgrid(), roll the axes with transpose(), and reshape to the desired ouput:', 'Pandas merge offers a naive, fast solution to the problem:']"
106,From ND to 1D arrays,"Say I have an array a:

a = np.array([[1,2,3], [4,5,6]])

array([[1, 2, 3],
       [4, 5, 6]])
I would like to convert it to a 1D array (i.e. a column vector):

b = np.reshape(a, (1,np.product(a....",https://stackoverflow.com/questions/13730468/from-nd-to-1d-arrays,"['Say I have an array a:', 'Use np.ravel (for a 1D view) or np.ndarray.flatten (for a 1D copy) or np.ndarray.flat (for an 1D iterator):', 'or, simply:', ""I wanted to see a benchmark result of functions mentioned in answers including unutbu's."", 'One of the simplest way is to use flatten(), like this example :', '[1 2 3 4 5 6 7 8]', ""Although this isn't using the np array format, (to lazy to modify my code) this should do what you want...  If, you truly want a column vector you will want to transpose the vector result.  It all depends on how you are planning to use this.""]"
107,How to convert a NumPy array to PIL image applying matplotlib colormap,"I have a simple problem, but I cannot find a good solution to it.

I want to take a NumPy 2D array which represents a grayscale image, and convert it to an RGB PIL image while applying some of the ...",https://stackoverflow.com/questions/10965417/how-to-convert-a-numpy-array-to-pil-image-applying-matplotlib-colormap,"['I have a simple problem, but I cannot find a good solution to it.', 'Quite a busy one-liner, but here it is:', 'Image.fromarray -> returns an image object', ""The method described in the accepted answer didn't work for me even after applying changes mentioned in its comments. But the below simple code worked:""]"
108,"RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility","I have this error for trying to load a saved SVM model. I have tried uninstalling sklearn, NumPy and SciPy, reinstalling the latest versions all-together again (using pip). I am still getting this ...",https://stackoverflow.com/questions/40845304/runtimewarning-numpy-dtype-size-changed-may-indicate-binary-incompatibility,"['I have this error for trying to load a saved SVM model. I have tried uninstalling sklearn, NumPy and SciPy, reinstalling the latest versions all-together again (using pip). I am still getting this error. Why?', 'According to MAINT: silence Cython warnings about changes dtype/ufunc size. - numpy/numpy:', ""It's the issue of new numpy version (1.15.0)"", 'if you are in an anaconda environment use:', ""I've tried the above-mentioned ways, but nothing worked. But the issue was gone after I installed the libraries through apt install,"", 'Just upgrade your numpy module, right now it is 1.15.4. For windows', 'This error occurs because the installed packages were build agains different version of numpy.\nWe need to rebuild scipy and scikit-learn against the local numpy.', 'Meta-information: The recommended way to install sklearn', 'Note that as of cython 0.29 there is a new check_size option that eliminates the warning at the source, so no work-arounds should be needed once that version percolates to the various packages', 'My enviroment is Python 2.7.15', 'When import scipy, error info shows: RuntimeWarning: builtin.type size changed, may indicate binary incompatibility. Expected zd, got zd']"
109,'and' (boolean) vs '&' (bitwise) - Why difference in behavior with lists vs numpy arrays?,"What explains the difference in behavior of boolean and bitwise operations on lists vs NumPy arrays? 

I'm confused about the appropriate use of & vs and in Python, illustrated in the following ...",https://stackoverflow.com/questions/22646463/and-boolean-vs-bitwise-why-difference-in-behavior-with-lists-vs-nump,"['What explains the difference in behavior of boolean and bitwise operations on lists vs NumPy arrays?', 'and tests whether both expressions are logically True while & (when used with True/False values) tests if both are True.', 'First a very important point, from which everything will follow (I hope).', ""The short-circuiting boolean operators (and, or) can't be overriden because there is no satisfying way to do this without introducing new language features or sacrificing short circuiting. As you may or may not know, they evaluate the first operand for its truth value, and depending on that value, either evaluate and return the second argument, or don't evaluate the second argument and return the first:"", 'Example 1:', 'In Python an expression of X and Y returns Y, given that bool(X) == True or any of X or Y evaluate to False, e.g.:', ""For the first example and base on the django's doc\nIt will always return the second list, indeed a non empty list is see as a True value for Python thus python return the 'last' True value so the second list"", ""Operations with a Python list operate on the list. list1 and list2 will check if list1 is empty, and return list1 if it is, and list2 if it isn't. list1 + list2 will append list2 to list1, so you get a new list with len(list1) + len(list2) elements."", 'Good question. Similar to the observation you have about examples 1 and 4 (or should I say 1 & 4 :) ) over logical and bitwise & operators, I experienced on sum operator. The numpy sum and py sum behave differently as well. For example:']"
110,Plotting a 2D heatmap with Matplotlib,"Using Matplotlib, I want to plot a 2D heat map. My data is an n-by-n Numpy array, each with a value between 0 and 1. So for the (i, j) element of this array, I want to plot a square at the (i, j) ...",https://stackoverflow.com/questions/33282368/plotting-a-2d-heatmap-with-matplotlib,"[""Using Matplotlib, I want to plot a 2D heat map. My data is an n-by-n Numpy array, each with a value between 0 and 1. So for the (i, j) element of this array, I want to plot a square at the (i, j) coordinate in my heat map, whose color is proportional to the element's value in the array."", ""The imshow() function with parameters interpolation='nearest' and cmap='hot' should do what you want."", 'Seaborn takes care of a lot of the manual work and automatically plots a gradient at the side of the chart etc.', 'For a 2d numpy array, simply use imshow() may help you:', ""I would use matplotlib's pcolor/pcolormesh function since it allows nonuniform spacing of the data."", ""Here's how to do it from a csv:""]"
111,Fitting empirical distribution to theoretical ones with Scipy (Python)?,"INTRODUCTION: I have a list of more than 30,000 integer values ranging from 0 to 47, inclusive, e.g.[0,0,0,0,..,1,1,1,1,...,2,2,2,2,...,47,47,47,...] sampled from some continuous distribution. The ...",https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python,"[""INTRODUCTION: I have a list of more than 30,000 integer values ranging from 0 to 47, inclusive, e.g.[0,0,0,0,..,1,1,1,1,...,2,2,2,2,...,47,47,47,...] sampled from some continuous distribution. The values in the list are not necessarily in order, but order doesn't matter for this problem."", ""This is an update and modification to Saullo's answer, that uses the full list of the current scipy.stats distributions and returns the distribution with the least SSE between the distribution's histogram and the data's histogram."", 'There are 82 implemented distribution functions in SciPy 0.12.0. You can test how some of them fit to your data using their fit() method. Check the code below for more details:', 'fit() method mentioned by @Saullo Castro provides maximum likelihood estimates (MLE).  The best distribution for your data is the one give you the highest can be determined by several different ways: such as', 'AFAICU, your distribution is discrete (and nothing but discrete). Therefore just counting the frequencies of different values and normalizing them should be enough for your purposes. So, an example to demonstrate this:', 'Try the distfit library.', 'With OpenTURNS, I would use the BIC criteria to select the best distribution that fits such data. This is because this criteria does not give too much advantage to the distributions which have more parameters. Indeed, if a distribution has more parameters, it is easier for the fitted distribution to be closer to the data. Moreover, the Kolmogorov-Smirnov may not make sense in this case, because a small error in the measured values will have a huge impact on the p-value.', 'It sounds like probability density estimation problem to me.', ""Forgive me if I don't understand your need but what about storing your data in a dictionary where keys would be the numbers between 0 and 47 and values the number of occurrences of their related keys in your original list?\nThus your likelihood p(x) will be the sum of all the values for keys greater than x divided by 30000."", 'While many of the above answers are completely valid, no one seems to answer your question completely, specifically the part:']"
112,How can I map True/False to 1/0 in a Pandas DataFrame?,"I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?",https://stackoverflow.com/questions/17383094/how-can-i-map-true-false-to-1-0-in-a-pandas-dataframe,"['I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?', 'A succinct way to convert a single column of boolean values to a column of integers 1 or 0:', 'Just multiply your Dataframe by 1 (int)', 'True is 1 in Python, and likewise False is 0*:', 'You also can do this directly on Frames', 'You can use a transformation for your data frame:', 'Use Series.view for convert boolean to integers:', ""I had to map FAKE/REAL to 0/1 but couldn't find proper answer.""]"
113,"How do I find the length (or dimensions, size) of a numpy matrix in python? [duplicate]","For a numpy matrix in python

from numpy import matrix
A = matrix([[1,2],[3,4]])
How can I find the length of a row (or column) of this matrix? Equivalently, how can I know the number of rows or ...",https://stackoverflow.com/questions/14847457/how-do-i-find-the-length-or-dimensions-size-of-a-numpy-matrix-in-python,"['For a numpy matrix in python', ""shape is a property of both numpy ndarray's and matrices."", 'matrix.size according to the numpy docs returns the Number of elements in the array. Hope that helps.']"
114,Numpy where function multiple conditions,"I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:

 dists[(np.where(dists >= r)) and (np.where(dists <...",https://stackoverflow.com/questions/16343752/numpy-where-function-multiple-conditions,"['I have an array of distances called dists. I want to select dists which are between two values. I wrote the following line of code to do that:', 'The best way in your particular case would just be to change your two criteria to one criterion:', 'The accepted answer explained the problem well enough. However, the the more Numpythonic approach for applying multiple conditions is to use numpy logical functions. In this ase you can use np.logical_and:', 'One interesting thing to point here; the usual way of using OR and AND too will work in this case, but with a small change. Instead of ""and"" and instead of ""or"", rather use Ampersand(&) and Pipe Operator(|) and it will work.', 'I like to use np.vectorize for such tasks. Consider the following:', 'Try:', 'This should work:', 'Try:', 'I have worked out this simple example']"
115,How to normalize a NumPy array to within a certain range?,"After doing some processing on an audio or image array, it needs to be normalized within a range before it can be written back to a file.  This can be done like so:

# Normalize audio channels to ...",https://stackoverflow.com/questions/1735025/how-to-normalize-a-numpy-array-to-within-a-certain-range,"['After doing some processing on an audio or image array, it needs to be normalized within a range before it can be written back to a file.  This can be done like so:', 'Using /= and *= allows you to eliminate an intermediate temporary array, thus saving some memory.  Multiplication is less expensive than division, so', ""If the array contains both positive and negative data, I'd go with:"", 'You can also rescale using sklearn. The advantages are that you can adjust normalize the standard deviation, in addition to mean-centering the data, and that you can do this on either axis, by features, or by records.', 'You can use the ""i"" (as in idiv, imul..) version, and it doesn\'t look half bad:', 'You are trying to min-max scale the values of audio between -1 and +1 and image between 0 and 255.', 'A simple solution is using the scalers offered by the sklearn.preprocessing library.', 'I tried following this, and got the error']"
116,Cython: “fatal error: numpy/arrayobject.h: No such file or directory”,"I'm trying to speed up the answer here using Cython. I try to compile the code (after doing the cygwinccompiler.py hack explained here), but get a fatal error: numpy/arrayobject.h: No such file or ...",https://stackoverflow.com/questions/14657375/cython-fatal-error-numpy-arrayobject-h-no-such-file-or-directory,"[""I'm trying to speed up the answer here using Cython. I try to compile the code (after doing the cygwinccompiler.py hack explained here), but get a fatal error: numpy/arrayobject.h: No such file or directory...compilation terminated error. Can anyone tell me if it's a problem with my code, or some esoteric subtlety with Cython?"", 'In your setup.py, the Extension should have the argument include_dirs=[numpy.get_include()].', ""For a one-file project like yours, another alternative is to use pyximport. You don't need to create a setup.py ... you don't need to even open a command line if you use IPython ... it's all very convenient. In your case, try running these commands in IPython or in a normal Python script:"", ""The error means that a numpy header file isn't being found during compilation."", ""A way simpler way is to add the path to your file distutils.cfg. It's path behalf of Windows 7 is by default C:\\Python27\\Lib\\distutils\\. You just assert the following contents and it should work out:"", ""It should be able to do it within cythonize() function as mentioned here, but it doesn't work beacuse there is a known issue"", 'If you are too lazy to write setup files and figure out the path for include directories, \ntry cyper. It can compile your Cython code and set include_dirs for Numpy automatically.']"
117,numpy max vs amax vs maximum,"numpy has three different functions which seem like they can be used for the same things --- except that numpy.maximum can only be used element-wise, while numpy.max and numpy.amax can be used on ...",https://stackoverflow.com/questions/33569668/numpy-max-vs-amax-vs-maximum,"['numpy has three different functions which seem like they can be used for the same things --- except that numpy.maximum can only be used element-wise, while numpy.max and numpy.amax can be used on particular axes, or all elements.  Why is there more than just numpy.max?  Is there some subtlety to this in performance?', 'np.max is just an alias for np.amax. This function only works on a single input array and finds the value of maximum element in that entire array (returning a scalar). Alternatively, it takes an axis argument and will find the maximum value along an axis of the input array (returning a new array).', ""You've already stated why np.maximum is different - it returns an array that is the element-wise maximum between two arrays."", 'For completeness, in Numpy there are four maximum related functions. They fall into two different categories:', 'np.maximum not only compares elementwise but also compares array elementwise with single value']"
118,Replacing Pandas or Numpy Nan with a None to use with MysqlDB,I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in ...,https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb,"[""I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType."", ""@bogatron has it right, you can use where, it's worth noting that you can do this natively in pandas:"", 'Credit goes to this guy here on this Github issue.', 'You can replace nan with None in your numpy array:', 'After stumbling around, this worked for me:', ""Just an addition to @Andy Hayden's answer:"", ""Another addition: be careful when replacing multiples and converting the type of the column back from object to float. If you want to be certain that your None's won't flip back to np.NaN's apply @andy-hayden's suggestion with using pd.where.\nIllustration of how replace can still go 'wrong':"", 'Quite old, yet I stumbled upon the very same issue.\nTry doing this:']"
119,"Should I use scipy.pi, numpy.pi, or math.pi?","In a project using SciPy and NumPy, should I use scipy.pi, numpy.pi, or math.pi?",https://stackoverflow.com/questions/12645547/should-i-use-scipy-pi-numpy-pi-or-math-pi,"['In a project using SciPy and NumPy, should I use scipy.pi, numpy.pi, or math.pi?', ""So it doesn't matter, they are all the same value."", ""One thing to note is that not all libraries will use the same meaning for pi, of course, so it never hurts to know what you're using. For example, the symbolic math library Sympy's representation of pi is not the same as math and numpy:""]"
120,ImportError: numpy.core.multiarray failed to import,"I'm trying to run this program
import cv2
import time

cv.NamedWindow(""camera"", 1)

capture = cv.CaptureFromCAM(0)

while True:
    img = cv.QueryFrame(capture)
    cv.ShowImage(""camera&...",https://stackoverflow.com/questions/20518632/importerror-numpy-core-multiarray-failed-to-import,"[""I'm trying to run this program"", 'I was getting the same error and was able to solve it by updating my numpy installation to 1.8.0:', 'In the case that', 'If you want a specific version:', 'Try sudo pip install numpy --upgrade --ignore-installed.', 'If you are using python3, the following command fixes the problem in macos.', 'you may need upgrade pip, it works for me', 'done the job for me!', 'I had the same error message, after trying some of the suggested solutions without success, I found that I needed to run:', 'for me this error came up when installing pygrib with conda and importing it.', ""In my case this problem was because I'd two python installations (2.7 and 3.5) and pip was installing numpy in the 3.5 python directory only, irrespective of which pip version I used."", ""I don't really understand this error but I solved this error with below."", 'I was getting the same error and the problem was solved by updating my numpy installation from 1.7.1 to 1.12.1', 'After having a nightmare using the pip install -U numpy several months ago, I gave up.  I went through installing CV2s and opencv without success.', 'It worked for me. So you can try following command', 'In my case installing from apt solved my problem.', ""I Had the same error occurring as I was using the numpy version suggested by the requirements.txt in the repo. When I tried to 'import pandas as pd' this error occurred. Then the solution was to upgrade numpy version to 1.15.2 as the version suggested in the requirements was mismatching with pandas. I uninstalled the existing numpy version with pip and reinstalled the new version."", ""I was able to solve the problem by updating my python to 3.8. I am using Macbook Air with Catalina. The problem started for me after updating TensorFlow. After updating it, the error doesn't disappear after I uninstalled and installed numpy several times."", 'The same error came for me. The problem is that you might have created a file called numpy.py. This file might coincide with numpy library. So, delete that numpy.py file and the problem gets solved.', ""I had the same error after installing python and opencv in my D: drive (C: runs on a SSD). The problem seemed to be that my execution path was inside the numpy folder. You can check if that's the issue with this code :"", 'This helped me', 'For me it was a two part. First:', 'uninstall existing numpy\nand install opencv-python will resolve the issue', ""I had the same issue, and here's how it's solved in my case."", 'Tilde folders', 'I got this same error in a conda environment, only six+ years later.  The other responses were helpful, and eventually I tracked it down to this problem:', 'Encountered this when trying to import Pytorch. Solved it by uninstalling or removing numpy repeatedly until no version was left on my Ubuntu and installing the newest version. In my case pip encountered Access is denied permission errors all the time maybe because of conflict with conda.']"
121,initialize a numpy array,"Is there way to initialize a numpy array of a shape and add to it? I will explain what I need with a list example. If I want to create a list of objects generated in a loop, I can do:

a = []
for i in ...",https://stackoverflow.com/questions/4535374/initialize-a-numpy-array,"['Is there way to initialize a numpy array of a shape and add to it? I will explain what I need with a list example. If I want to create a list of objects generated in a loop, I can do:', 'numpy.zeros', 'The way I usually do that is by creating a regular list, then append my stuff into it, and finally transform the list to a numpy array as follows :', 'Introduced in numpy 1.8:', ""Array analogue for the python's"", 'numpy.fromiter() is what you are looking for:', 'You do want to avoid explicit loops as much as possible when doing array computing, as that reduces the speed gain from that form of computing. There are multiple ways to initialize a numpy array. If you want it filled with zeros, do as katrielalex said:', 'To initialize a numpy array with a specific matrix:', 'For your first array example use,', 'I realize that this is a bit late, but I did not notice any of the other answers mentioning indexing into the empty array:', ""I'd suggest defining shape first. \nThen iterate over it to insert values."", 'Whenever you are in the following situation:', 'Maybe something like this will fit your needs..']"
122,How to flatten only some dimensions of a numpy array,"Is there a quick way to ""sub-flatten"" or flatten only some of the first dimensions in a numpy array?

For example, given a numpy array of dimensions (50,100,25), the resultant dimensions would be (...",https://stackoverflow.com/questions/18757742/how-to-flatten-only-some-dimensions-of-a-numpy-array,"['Is there a quick way to ""sub-flatten"" or flatten only some of the first dimensions in a numpy array?', 'Take a look at numpy.reshape .', 'A slight generalization to Alexander\'s answer - np.reshape can take -1 as an argument, meaning ""total array size divided by product of all other listed dimensions"":', ""A slight generalization to Peter's answer -- you can specify a range over the original array's shape if you want to go beyond three dimensional arrays."", 'An alternative approach is to use numpy.resize() as in:']"
123,Iterating over a numpy array,"Is there a less verbose alternative to this:

for x in xrange(array.shape[0]):
    for y in xrange(array.shape[1]):
        do_stuff(x, y)
I came up with this:

for x, y in itertools.product(map(...",https://stackoverflow.com/questions/6967463/iterating-over-a-numpy-array,"['Is there a less verbose alternative to this:', ""I think you're looking for the ndenumerate."", 'If you only need the indices, you could try numpy.ndindex:', 'see nditer']"
124,python numpy ValueError: operands could not be broadcast together with shapes,"In numpy, I have two ""arrays"", X is (m,n) and y is a vector (n,1)

using 

X*y
I am getting the error

ValueError: operands could not be broadcast together with shapes (97,2) (2,1) 
When  (97,2)x(2,...",https://stackoverflow.com/questions/24560298/python-numpy-valueerror-operands-could-not-be-broadcast-together-with-shapes,"['In numpy, I have two ""arrays"", X is (m,n) and y is a vector (n,1)', 'dot is matrix multiplication, but * does something else.', 'Per numpy docs:', ""It's possible that the error didn't occur in the dot product, but after.\nFor example try this"", 'You are looking for np.matmul(X, y). In Python 3.5+ you can use X @ y.', ""Use np.mat(x) * np.mat(y), that'll work."", 'We might confuse ourselves that a * b is a dot product.']"
125,Multiple linear regression in Python,I can't seem to find any python libraries that do multiple regression. The only things I find only do simple regression. I need to regress my dependent variable (y) against several independent ...,https://stackoverflow.com/questions/11479064/multiple-linear-regression-in-python,"[""I can't seem to find any python libraries that do multiple regression. The only things I find only do simple regression. I need to regress my dependent variable (y) against several independent variables (x1, x2, x3, etc.)."", 'sklearn.linear_model.LinearRegression will do it:', 'Here is a little work around that I created. I checked it with R and it works correct.', 'Just to clarify, the example you gave is multiple linear regression, not multivariate linear regression refer. Difference:', 'You can use numpy.linalg.lstsq:', 'Use scipy.optimize.curve_fit. And not only for linear fit.', 'Once you convert your data to a pandas dataframe (df),', 'I think this may the most easy way to finish this work:', ""Multiple Linear Regression can be handled using the sklearn library as referenced above.  I'm using the Anaconda install of Python 3.6."", 'You can use numpy.linalg.lstsq', 'Finding a linear model such as this one can be handled with OpenTURNS.', 'You can use the function below and pass it a DataFrame:', 'Scikit-learn is a machine learning library for Python which can do this job for you. \nJust import sklearn.linear_model module into your script.', 'Here is an alternative and basic method:']"
126,Normalize data in pandas,"Suppose I have a pandas data frame df: 

I want to calculate the column wise mean of a data frame.

This is easy: 

df.apply(average) 
then the column wise range max(col) - min(col). This is easy ...",https://stackoverflow.com/questions/12525722/normalize-data-in-pandas,"['Suppose I have a pandas data frame df:', ""If you don't mind importing the sklearn library, I would recommend the method talked on this blog."", ""You can use apply for this, and it's a bit neater:"", 'Slightly modified from: Python Pandas Dataframe: Normalize data between 0.01 and 0.99? but from some of the comments thought it was relevant (sorry if considered a repost though...)', 'This is how you do it column-wise:']"
127,Numpy argsort - what is it doing?,"Why is numpy giving this result:

x = numpy.array([1.48,1.41,0.0,0.1])
print x.argsort()

>[2 3 1 0]
when I'd expect it to do this:
  [3 2 0 1]
Clearly my understanding of the function is ...",https://stackoverflow.com/questions/17901218/numpy-argsort-what-is-it-doing,"['Why is numpy giving this result:', 'According to the documentation', '[2, 3, 1, 0] indicates that the smallest element is at index 2, the next smallest at index 3, then index 1, then index 0.', 'As the documentation says, argsort:', ""numpy.argsort(a, axis=-1, kind='quicksort', order=None)"", 'input:\n    import numpy as np\n    x = np.array([1.48,1.41,0.0,0.1])\n    x.argsort().argsort()', 'First, it was ordered the array. Then generate an array with the initial index of the array.', ""np.argsort returns the index of the sorted array given by the 'kind' (which specifies the type of sorting algorithm). However, when a list is used with np.argmax, it returns the index of the largest element in the list. While, np.sort, sorts the given array, list."", ""Just want to directly contrast the OP's original understanding against the actual implementation with code."", 'It returns indices according to the given array indices,[1.48,1.41,0.0,0.1],that means:\n0.0 is the first element, in index [2].\n0.1 is the second element, in index[3].\n1.41 is the third element, in index [1].\n1.48 is the fourth element, in index[0].\nOutput:']"
128,"Confusion between numpy, scipy, matplotlib and pylab","Numpy, scipy, matplotlib, and pylab are common terms among they who use python for scientific computation.

I just learn a bit about pylab, and I got confused.
Whenever I want to import numpy, I can ...",https://stackoverflow.com/questions/12987624/confusion-between-numpy-scipy-matplotlib-and-pylab,"['Numpy, scipy, matplotlib, and pylab are common terms among they who use python for scientific computation.', 'No, pylab is part of matplotlib (in matplotlib.pylab) and tries to give you a MatLab like environment. matplotlib has a number of dependencies, among them numpy which it imports under the common alias np. scipy is not a dependency of matplotlib.', 'Scipy and numpy are scientific projects whose aim is to bring efficient and fast numeric computing to python.', 'Since some people (like me) may still be confused about usage of pylab since examples using pylab are out there on the internet, here is a quote from the official matplotlib FAQ:']"
129,Filtering a list based on a list of booleans,"I have a list of values which I need to filter given the values in a list of booleans:

list_a = [1, 2, 4, 6]
filter = [True, False, True, False]
I generate a new filtered list with the following ...",https://stackoverflow.com/questions/18665873/filtering-a-list-based-on-a-list-of-booleans,"['I have a list of values which I need to filter given the values in a list of booleans:', ""You're looking for itertools.compress:"", 'Like so:', 'With numpy:', 'To do this using numpy, ie, if you have an array, a, instead of list_a:', 'With python 3 you can use list_a[filter] to get True values. To get False values use list_a[~filter]']"
130,Shared-memory objects in multiprocessing,"Suppose I have a large in memory numpy array, I have a function func that takes in this giant array as input (together with some other parameters). func with different parameters can be run in ...",https://stackoverflow.com/questions/10721915/shared-memory-objects-in-multiprocessing,"['Suppose I have a large in memory numpy array, I have a function func that takes in this giant array as input (together with some other parameters). func with different parameters can be run in parallel. For example:', ""If you use an operating system that uses copy-on-write fork() semantics (like any common unix), then as long as you never alter your data structure it will be available to all child processes without taking up additional memory.  You will not have to do anything special (except make absolutely sure you don't alter the object)."", 'I run into the same problem and wrote a little shared-memory utility class to work around it.', 'This is the intended use case for Ray, which is a library for parallel and distributed Python. Under the hood, it serializes objects using the Apache Arrow data layout (which is a zero-copy format) and stores them in a shared-memory object store so they can be accessed by multiple processes without creating copies.', 'Like Robert Nishihara mentioned, Apache Arrow makes this easy, specifically with the Plasma in-memory object store, which is what Ray is built on.']"
131,how does multiplication differ for NumPy Matrix vs Array classes?,"The numpy docs recommend using array instead of matrix for working with matrices. However, unlike octave (which I was using till recently), * doesn't perform matrix multiplication, you need to use the ...",https://stackoverflow.com/questions/3890621/how-does-multiplication-differ-for-numpy-matrix-vs-array-classes,"[""The numpy docs recommend using array instead of matrix for working with matrices. However, unlike octave (which I was using till recently), * doesn't perform matrix multiplication, you need to use the function matrixmultipy(). I feel this makes the code very unreadable."", 'The main reason to avoid using the matrix class is that a) it\'s inherently 2-dimensional, and b) there\'s additional overhead compared to a ""normal"" numpy array. If all you\'re doing is linear algebra, then by all means, feel free to use the matrix class... Personally I find it more trouble than it\'s worth, though.', 'the key things to know for operations on NumPy arrays versus operations on NumPy matrices are:', 'In 3.5, Python finally got a matrix multiplication operator. The syntax is a @ b.', 'There is a situation where the dot operator will give different answers when dealing with arrays as with dealing with matrices.  For example, suppose the following:', 'Reference from http://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html', 'This trick could be what you are looking for. It is a kind of simple operator overload.', 'A pertinent quote from PEP 465 - A dedicated infix operator for matrix multiplication , as mentioned by @petr-viktorin, clarifies the problem the OP was getting at:', 'Function matmul (since numpy 1.10.1) works fine for both types and return result as a numpy matrix class:']"
132,How to save a list as numpy array in python?,Is possible to construct a NumPy array from a python list?,https://stackoverflow.com/questions/5951135/how-to-save-a-list-as-numpy-array-in-python,"['Is possible to construct a NumPy array from a python list?', ""First of all, I'd recommend you to go through NumPy's  Quickstart tutorial, which will probably help with these basic questions."", 'you mean something like this ?', 'Yes it is:', 'You want to save it as a file?', 'You can use numpy.asarray, for example to convert a list into an array:', 'I suppose, you mean converting a list into a numpy array?\nThen,', 'Here is a more complete example:', 'some list comprehension', 'maybe:']"
133,Add single element to array in numpy,"I have a numpy array containing:

[1, 2, 3]
I want to create an array containing:

[1, 2, 3, 1]
That is, I want to add the first element on to the end of the array.

I have tried the obvious:

np....",https://stackoverflow.com/questions/7332841/add-single-element-to-array-in-numpy,"['I have a numpy array containing:', 'append() creates a new array which can be the old array with the appended element.', 'When appending only once or once every now and again, using np.append on your array should be fine. The drawback of this approach is that memory is allocated for a completely new array every time it is called. When growing an array for a significant amount of samples it would be better to either pre-allocate the array (if the total size is known) or to append to a list and convert to an array afterward.', ""a[0] isn't an array, it's the first element of a and therefore has no dimensions."", 'Try this:', 'This command,', 'This might be a bit overkill, but I always use the the np.take function for any wrap-around indexing:', ""Let's say a=[1,2,3] and you want it to be [1,2,3,1]."", 'If you want to add an element use append()']"
134,Why does multiprocessing use only a single core after I import numpy?,"I am not sure whether this counts more as an OS issue, but I thought I would ask here in case anyone has some insight from the Python end of things.

I've been trying to parallelise a CPU-heavy for ...",https://stackoverflow.com/questions/15639779/why-does-multiprocessing-use-only-a-single-core-after-i-import-numpy,"['I am not sure whether this counts more as an OS issue, but I thought I would ask here in case anyone has some insight from the Python end of things.', 'After some more googling I found the answer here.', 'Python 3 now exposes the methods to directly set the affinity', 'This appears to be a common problem with Python on Ubuntu, and is not specific to joblib:']"
135,Find the most frequent number in a numpy vector,"Suppose I have the following list in python: 

a = [1,2,3,1,2,1,1,1,3,2,2,1]
How to find the most frequent number in this list in a neat way?",https://stackoverflow.com/questions/6252280/find-the-most-frequent-number-in-a-numpy-vector,"['Suppose I have the following list in python:', 'If your list contains all non-negative ints, you should take a look at numpy.bincounts:', 'You may use', ""If you're willing to use SciPy:"", ""Best is 'max' with 'set' for small arrays like the problem."", 'Also if you want to get most frequent value(positive or negative) without loading any modules you can use the following code:', ""While most of the answers above are useful, in case you:\n1) need it to support non-positive-integer values (e.g. floats or negative integers ;-)), and\n2) aren't on Python 2.7 (which collections.Counter requires), and\n3) prefer not to add the dependency of scipy (or even numpy) to your code, then a purely python 2.6 solution that is O(nlogn) (i.e., efficient) is just this:"", 'Starting in Python 3.4, the standard library includes the statistics.mode function to return the single most common data point.', 'I like the solution by JoshAdel.', 'Expanding on this method, applied to finding the mode of the data where you may need the index of the actual array to see how far away the value is from the center of the distribution.', 'In Python 3 the following should work:', ""Here is a general solution that may be applied along an axis, regardless of values, using purely numpy.  I've also found that this is much faster than scipy.stats.mode if there are a lot of unique values."", ""I'm recently doing a project and using collections.Counter.(Which tortured me).""]"
136,best way to preserve numpy arrays on disk,"I am looking for a fast way to preserve large numpy arrays. I want to save them to the disk in a binary format, then read them back into memory relatively fastly. cPickle is not fast enough, ...",https://stackoverflow.com/questions/9619199/best-way-to-preserve-numpy-arrays-on-disk,"['I am looking for a fast way to preserve large numpy arrays. I want to save them to the disk in a binary format, then read them back into memory relatively fastly. cPickle is not fast enough, unfortunately.', ""I'm a big fan of hdf5 for storing large numpy arrays. There are two options for dealing with hdf5 in python:"", ""I've compared performance (space and time) for a number of ways to store numpy arrays. Few of them support multiple arrays per file, but perhaps it's useful anyway."", 'There is now a HDF5 based clone of pickle called hickle!', 'savez() save data in a zip file, It may take some time to zip & unzip the file. You can use save() & load() function:', 'Another possibility to store numpy arrays efficiently is Bloscpack:', 'The lookup time is slow because when you use mmap to does not load content of array to memory when you invoke load method. Data is lazy loaded when particular data is needed. \nAnd this happens in lookup in your case. But second lookup won`t be so slow.']"
137,In-place type conversion of a NumPy array,"Given a NumPy array of int32, how do I convert it to float32 in place?  So basically, I would like to do

a = a.astype(numpy.float32)
without copying the array.  It is big.

The reason for doing this ...",https://stackoverflow.com/questions/4389517/in-place-type-conversion-of-a-numpy-array,"['Given a NumPy array of int32, how do I convert it to float32 in place?  So basically, I would like to do', 'You can make a view with a different dtype, and then copy in-place into the view:', ""Update: This function only avoids copy if it can, hence this is not the correct answer for this question. unutbu's answer is the right one."", 'You can change the array type without converting like this:', ""use view() and parameter 'dtype' to change the array in place."", 'Use this:', 'a = np.subtract(a, 0., dtype=np.float32)']"
138,How to check BLAS/LAPACK linkage in NumPy and SciPy?,"I am builing my numpy/scipy environment based on blas and lapack more or less based on this walk through. 

When I am done, how can I check, that my numpy/scipy functions really do use the previously ...",https://stackoverflow.com/questions/9000164/how-to-check-blas-lapack-linkage-in-numpy-and-scipy,"['I am builing my numpy/scipy environment based on blas and lapack more or less based on this walk through.', 'The method numpy.show_config() (or numpy.__config__.show()) outputs information about linkage gathered at build time. My output looks like this. I think it means I am using the BLAS/LAPACK that ships with Mac OS.', 'What you are searching for is this:\nsystem info', 'You can use the link loader dependency tool to look at the C level hook components of your build and see whether they have external dependencies on your blas and lapack of choice. I am not near a linux box right now, but on an OS X machine you can do this inside the site-packages directory which holds the installations:', 'You can display BLAS, LAPACK, MKL linkage using show_config():', 'If you installed anaconda-navigator (at www.anaconda.com/anaconda/install/ for linux, Windows or macOS) - blas, scipy and numpy will all be installed and you can see them by clicking environments tab on left side of navigator home page (look for each directory in alpha order). Installing full anaconda (as opposed to miniconda or individual packages) will take care of installing many of the essential packages needed for data science.']"
139,A column-vector y was passed when a 1d array was expected,"I need to fit RandomForestRegressor from sklearn.ensemble.
forest = ensemble.RandomForestRegressor(**RF_tuned_parameters)
model = forest.fit(train_fold, train_y)
yhat = model.predict(test_fold)

This ...",https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected,"['I need to fit RandomForestRegressor from sklearn.ensemble.', 'Change this line:', ""I also encountered this situation when I was trying to train a KNN classifier. but it seems that the warning was gone after I changed:\nknn.fit(X_train,y_train)\nto\nknn.fit(X_train, np.ravel(y_train,order='C'))"", 'I had the same problem. The problem was that the labels were in a column format while it expected it in a row.\nuse np.ravel()', 'use below code:', 'Another way of doing this is to use ravel', 'With neuraxle, you can easily solve this :', 'Y = y.values[:,0]']"
140,Numpy: Divide each row by a vector element,"Suppose I have a numpy array:

data = np.array([[1,1,1],[2,2,2],[3,3,3]])
and I have a corresponding ""vector:""

vector = np.array([1,2,3])
How do I operate on data along each row to either subtract ...",https://stackoverflow.com/questions/19602187/numpy-divide-each-row-by-a-vector-element,"['Suppose I have a numpy array:', 'Here you go. You just need to use None (or alternatively np.newaxis) combined with broadcasting:', 'As has been mentioned, slicing with None or with np.newaxes is a great way to do this.\nAnother alternative is to use transposes and broadcasting, as in', ""JoshAdel's solution uses np.newaxis to add a dimension. An alternative is to use reshape() to align the dimensions in preparation for broadcasting."", 'Adding to the answer of stackoverflowuser2010, in the general case you can just use', 'Pythonic way to do this is ...']"
141,How to create a density plot in matplotlib?,"In R I can create the desired output by doing: 

data = c(rep(1.5, 7), rep(2.5, 2), rep(3.5, 8),
         rep(4.5, 3), rep(5.5, 1), rep(6.5, 8))
plot(density(data, bw=0.5))
In python (with matplotlib)...",https://stackoverflow.com/questions/4150171/how-to-create-a-density-plot-in-matplotlib,"['In R I can create the desired output by doing:', ""Sven has shown how to use the class gaussian_kde from Scipy, but you will notice that it doesn't look quite like what you generated with R. This is because gaussian_kde tries to infer the bandwidth automatically. You can play with the bandwidth in a way by changing the function covariance_factor of the gaussian_kde class. First, here is what you get without changing that function:"", 'Five years later, when I Google ""how to create a kernel density plot using python"", this thread still shows up at the top!', 'Option 1:', 'Maybe try something like:', 'The density plot can also be created by using matplotlib:\nThe function plt.hist(data) returns the y and x values necessary for the density plot (see the documentation https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html).\nResultingly, the following code creates a density plot by using the matplotlib library:', 'You can do something like:']"
142,Difference between numpy dot() and Python 3.5+ matrix multiplication @,"I recently moved to Python 3.5 and noticed the new matrix multiplication operator (@) sometimes behaves differently from the numpy dot operator. In example, for 3d arrays:

import numpy as np

a = np....",https://stackoverflow.com/questions/34142485/difference-between-numpy-dot-and-python-3-5-matrix-multiplication,"['I recently moved to Python 3.5 and noticed the new matrix multiplication operator (@) sometimes behaves differently from the numpy dot operator. In example, for 3d arrays:', ""The @ operator calls the array's __matmul__ method, not dot. This method is also present in the API as the function np.matmul."", ""The answer by @ajcr explains how the dot and matmul (invoked by the @ symbol) differ. By looking at a simple example, one clearly sees how the two behave differently when operating on 'stacks of matricies' or tensors."", 'Just FYI, @ and its numpy equivalents dot and matmul are all equally fast. (Plot created with perfplot, a project of mine.)', 'In mathematics, I think the dot in numpy makes more sense', 'Here is a comparison with np.einsum to show how the indices are projected', 'My experience with MATMUL and DOT']"
143,Efficient evaluation of a function at every cell of a NumPy array,"Given a NumPy array A, what is the fastest/most efficient way to apply the same function, f, to every cell?
Suppose that we will assign to A(i,j) the f(A(i,j)).
The function, f, doesn't have a binary ...",https://stackoverflow.com/questions/7701429/efficient-evaluation-of-a-function-at-every-cell-of-a-numpy-array,"['Given a NumPy array A, what is the fastest/most efficient way to apply the same function, f, to every cell?', 'You could just vectorize the function and then apply it directly to a Numpy array each time you need it:', 'A similar question is: Mapping a NumPy array in place.\nIf you can find a ufunc for your f(), then you should use the out parameter.', 'If you are working with numbers and f(A(i,j)) = f(A(j,i)), you could use scipy.spatial.distance.cdist defining f as a distance between A(i) and A(j).', 'I believe I have found a better solution. The idea to change the function to python universal function (see documentation), which can exercise parallel computation under the hood.', ""When the 2d-array (or nd-array) is C- or F-contiguous, then this task of mapping a function onto a 2d-array is practically the same as the task of mapping a function onto a 1d-array - we just have to view it that way, e.g. via np.ravel(A,'K')."", 'All above answers compares well, but if you need to use custom function for mapping, and you have numpy.ndarray, and you need to retain the shape of array.']"
144,How to convert an array of strings to an array of floats in numpy?,"How to convert

[""1.1"", ""2.2"", ""3.2""]
to

[1.1, 2.2, 3.2]
in NumPy?",https://stackoverflow.com/questions/3877209/how-to-convert-an-array-of-strings-to-an-array-of-floats-in-numpy,"['How to convert', ""Well, if you're reading the data in as a list, just do np.array(map(float, list_of_strings)) (or equivalently, use a list comprehension). (In Python 3, you'll need to call list on the map return value if you use map, since map returns an iterator now.)"", 'You can use this as well', 'Another option might be numpy.asarray:', 'If you have (or create) a single string, you can use np.fromstring:']"
145,Efficiently sorting a numpy array in descending order?,"I am surprised this specific question hasn't been asked before, but I really didn't find it on SO nor on the documentation of np.sort.

Say I have a random numpy array holding integers, e.g:

> ...",https://stackoverflow.com/questions/26984414/efficiently-sorting-a-numpy-array-in-descending-order,"[""I am surprised this specific question hasn't been asked before, but I really didn't find it on SO nor on the documentation of np.sort."", 'temp[::-1].sort() sorts the array in place, whereas np.sort(temp)[::-1] creates a new array.', 'For short arrays I suggest using np.argsort() by finding the indices of the sorted negatived array, which is slightly faster than reversing the sorted array:', 'Unfortunately when you have a complex array, only np.sort(temp)[::-1] works properly. The two other methods mentioned here are not effective.', 'Testing on a 100×10×10 array 1000 times.', ""Hello I was searching for a solution to reverse sorting a two dimensional numpy array, and I couldn't find anything that worked, but I think I have stumbled on a solution which I am uploading just in case anyone is in the same boat."", 'You could sort the array first (Ascending by default) and then apply np.flip()\n(https://docs.scipy.org/doc/numpy/reference/generated/numpy.flip.html)', 'Here is a quick trick', 'i suggest using this ...']"
146,How does numpy.histogram() work?,"While reading up on numpy, I encountered the function numpy.histogram().

What is it for and how does it work? In the docs they mention bins: What are they?

Some googling led me to the definition of ...",https://stackoverflow.com/questions/9141732/how-does-numpy-histogram-work,"['While reading up on numpy, I encountered the function numpy.histogram().', 'A bin is range that represents the width of a single bar of the histogram along the X-axis. You could also call this the interval. (Wikipedia defines them more formally as ""disjoint categories"".)', 'Below, hist indicates that there are 0 items in bin #0, 2 in bin #1, 4 in bin #3, 1 in bin #4.', 'Another useful thing to do with numpy.histogram is to plot the output as the x and y coordinates on a linegraph. For example:']"
147,Fast check for NaN in NumPy,"I'm looking for the fastest way to check for the occurrence of NaN (np.nan) in a NumPy array X. np.isnan(X) is out of the question, since it builds a boolean array of shape X.shape, which is ...",https://stackoverflow.com/questions/6736590/fast-check-for-nan-in-numpy,"[""I'm looking for the fastest way to check for the occurrence of NaN (np.nan) in a NumPy array X. np.isnan(X) is out of the question, since it builds a boolean array of shape X.shape, which is potentially gigantic."", ""Ray's solution is good. However, on my machine it is about 2.5x faster to use numpy.sum in place of numpy.min:"", 'I think np.isnan(np.min(X)) should do what you want.', 'There are two general approaches here:', ""Even there exist an accepted answer, I'll like to demonstrate the following (with Python 2.7.2 and Numpy 1.6.0 on Vista):"", 'use .any()', ""If you're comfortable with numba it allows to create a fast short-circuit (stops as soon as a NaN is found) function:"", 'Related to this is the question of how to find the first occurrence of NaN. This is the fastest way to handle that that I know of:']"
148,Automatically import modules when entering the python or ipython interpreter,I find myself typing import numpy as np almost every single time I fire up the python interpreter. How do I set up the python or ipython interpreter so that numpy is automatically imported?,https://stackoverflow.com/questions/11124578/automatically-import-modules-when-entering-the-python-or-ipython-interpreter,"['I find myself typing import numpy as np almost every single time I fire up the python interpreter. How do I set up the python or ipython interpreter so that numpy is automatically imported?', 'Use the environment variable PYTHONSTARTUP. From the official documentation:', ""For ipython, there are two ways to achieve this. Both involve ipython's configuration directory which is located in ~/.ipython."", 'I use a ~/.startup.py file like this:', ""While creating a custom startup script like ravenac95 suggests is the best general answer for most cases, it won't work in circumstances where you want to use a from __future__ import X. If you sometimes work in Python 2.x but want to use modern division, there is only one way to do this. Once you create a profile, edit the profile_default (For Ubuntu this is located in ~/.ipython/profile_default) and add something like the following to the bottom:"", 'As a simpler alternative to the accepted answer, on linux:', 'You can create a normal python script as import_numpy.py or anything you like', 'As ravenac95 mentioned in his answer, you can either create a custom profile or modify the default profile. This answer is quick view of Linux commands needed to import numpy as np automatically.', 'My default ipython invocation is']"
149,Finding local maxima/minima with Numpy in a 1D numpy array,"Can you suggest a module function from numpy/scipy that can find local maxima/minima in a 1D numpy array? Obviously the simplest approach ever is to have a look at the nearest neighbours, but I would ...",https://stackoverflow.com/questions/4624970/finding-local-maxima-minima-with-numpy-in-a-1d-numpy-array,"['Can you suggest a module function from numpy/scipy that can find local maxima/minima in a 1D numpy array? Obviously the simplest approach ever is to have a look at the nearest neighbours, but I would like to have an accepted solution that is part of the numpy distro.', 'If you are looking for all entries in the 1d array a smaller than their neighbors, you can try', 'In SciPy >= 0.11', 'For curves with not too much noise, I recommend the following small code snippet:', 'As of SciPy version 1.1, you can also use find_peaks. Below are two examples taken from the documentation itself.', 'Another approach (more words, less code) that may help:', 'Why not use Scipy built-in function signal.find_peaks_cwt to do the job ?', ""Update: \nI wasn't happy with gradient so I found it more reliable to use numpy.diff. Please let me know if it does what you want."", 'While this question is really old. I believe there is a much simpler approach in numpy (a one liner).', 'None of these solutions worked for me since I wanted to find peaks in the center of repeating values as well. for example, in', 'minm and maxm contain indices of minima and maxima, respectively. For a huge data set, it will give lots of maximas/minimas so in that case smooth the curve first and then apply this algorithm.', 'Another solution using essentially a dilate operator:', 'Another one:']"
150,How to get the index of a maximum element in a NumPy array along one axis,"I have a 2 dimensional NumPy array. I know how to get the maximum values over axes:
>>> a = array([[1,2,3],[4,3,1]])
>>> amax(a,axis=0)
array([4, 3, 3])

How can I get the indices of ...",https://stackoverflow.com/questions/5469286/how-to-get-the-index-of-a-maximum-element-in-a-numpy-array-along-one-axis,"['I have a 2 dimensional NumPy array. I know how to get the maximum values over axes:', 'argmax() will only return the first occurrence for each row.\nhttp://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html']"
151,Index all *except* one item in python,"Is there a simple way to index all elements of a list (or array, or whatever) except for a particular index?  E.g., 
mylist[3] will return the item in position 3
milist[~3] will return the whole list ...",https://stackoverflow.com/questions/19286657/index-all-except-one-item-in-python,"['Is there a simple way to index all elements of a list (or array, or whatever) except for a particular index?  E.g.,', 'For a list, you could use a list comp.  For example, to make b a copy of a without the 3rd element:', 'See also', 'The simplest way I found was:', 'If you are using numpy, the closest, I can think of is using a mask', 'Use np.delete ! It does not actually delete anything inplace', ""I'm going to provide a functional (immutable) way of doing it."", ""If you don't know the index beforehand here is a function that will work"", 'Note that if variable is list of lists, some approaches would fail.\nFor example:', 'If you want to cut out the last or the first do this:']"
152,How to write a multidimensional array to a text file?,"In another question, other users offered some help if I could supply the array I was having trouble with. However, I even fail at a basic I/O task, such as writing an array to a file.

Can anyone ...",https://stackoverflow.com/questions/3685265/how-to-write-a-multidimensional-array-to-a-text-file,"['In another question, other users offered some help if I could supply the array I was having trouble with. However, I even fail at a basic I/O task, such as writing an array to a file.', ""If you want to write it to disk so that it will be easy to read back in as a numpy array, look into numpy.save.  Pickling it will work fine, as well, but it's less efficient for large arrays (which yours isn't, so either is perfectly fine)."", ""I am not certain if this meets your requirements, given I think you are interested in making the file readable by people, but if that's not a primary concern, just pickle it."", ""If you don't need a human-readable output, another option you could try is to save the array as a MATLAB .mat file, which is a structured array. I despise MATLAB, but the fact that I can both read and write a .mat in very few lines is convenient."", 'ndarray.tofile() should also work', 'There exist special libraries to do just that. (Plus wrappers for python)', 'You can simply traverse the array in three nested loops and write their values to your file. For reading, you simply use the same exact loop construction. You will get the values in exactly the right order to fill your arrays correctly again.', ""I have a way to do it using a simply filename.write() operation. It works fine for me, but I'm dealing with arrays having ~1500 data elements."", 'Pickle is best for these cases. Suppose you have a ndarray named x_train. You can dump it into a file and revert it back using the following command:']"
153,ImportError in importing from sklearn: cannot import name check_build,"I am getting the following error while trying to import from sklearn:

>>> from sklearn import svm

Traceback (most recent call last):
  File ""<pyshell#17>"", line 1, in <module>
  ...",https://stackoverflow.com/questions/15274696/importerror-in-importing-from-sklearn-cannot-import-name-check-build,"['I am getting the following error while trying to import from sklearn:', 'Worked for me after installing scipy.', 'So, simply try to restart the shell!', 'My solution for Python 3.6.5 64-bit Windows 10:', 'After installing numpy , scipy ,sklearn  still has error', ""Usually when I get these kinds of errors, opening the __init__.py file and poking around helps. Go to the directory C:\\Python27\\lib\\site-packages\\sklearn and ensure that there's a sub-directory called __check_build as a first step. On my machine (with a working sklearn installation, Mac OSX, Python 2.7.3) I have __init__.py, setup.py, their associated .pyc files, and a binary _check_build.so."", ""I had the same issue on Windows. Solved it by installing Numpy+MKL from http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy (there it's recommended to install numpy+mkl before other packages that depend on it) as suggested by this answer."", 'I had problems importing SKLEARN after installing a new 64bit version of Python 3.4 from python.org.', 'If you use Anaconda 2.7 64 bit, try', 'None of the other answers worked for me. After some tinkering I unsinstalled sklearn:', 'For me, \nI was upgrading the existing code into new setup by installing Anaconda from fresh with latest python version(3.7)\nFor this,', 'no need to uninstall & then re-install sklearn', 'i had the same problem reinstalling anaconda solved the issue for me', 'In windows:']"
154,How to fix 'Object arrays cannot be loaded when allow_pickle=False' for imdb.load_data() function?,"I'm trying to implement the binary classification example using the IMDb dataset in Google Colab. I have implemented this model before. But when I tried to do it again after a few days, it returned a ...",https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa,"[""I'm trying to implement the binary classification example using the IMDb dataset in Google Colab. I have implemented this model before. But when I tried to do it again after a few days, it returned a value error: 'Object arrays cannot be loaded when allow_pickle=False' for the load_data() function."", ""Here's a trick to force imdb.load_data to allow pickle by, in your notebook, replacing this line:"", 'This issue is still up on keras git. I hope it gets solved as soon as possible. \nUntil then, try downgrading your numpy version to 1.16.2. It seems to solve the problem.', 'Following this issue on GitHub, the official solution is to edit the imdb.py file.  This fix worked well for me without the need to downgrade numpy.   Find the imdb.py file at tensorflow/python/keras/datasets/imdb.py (full path for me was: C:\\Anaconda\\Lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py - other installs will be different) and change line 85 as per the diff:', 'I just used allow_pickle = True as an argument to np.load() and it worked for me.', 'In my case worked with:', ""I think the answer from cheez (https://stackoverflow.com/users/122933/cheez) is the easiest and most effective one. I'd elaborate a little bit over it so it would not modify a numpy function for the whole session period."", ""You can try changing the flag's value"", 'none of the above listed solutions worked for me: i run anaconda with python 3.7.3.\nWhat worked for me was', 'on jupyter notebook using', 'I landed up here, tried your ways and could not figure out.', 'find the path to imdb.py\nthen just add the flag to np.load(path,...flag...)', 'Yes, installing previous a version of numpy solved the problem.', 'Its work for me', 'What I have found is that TensorFlow 2.0 (I am using 2.0.0-alpha0) is not compatible with the latest version of Numpy i.e. v1.17.0 (and possibly v1.16.5+). As soon as TF2 is imported, it throws a huge list of FutureWarning, that looks something like this:', 'Use this', 'Tensorflow has a fix in tf-nightly version.', ""The answer of @cheez sometime doesn't work and recursively call the function again and again. To solve this problem you should copy the function deeply. You can do this by using the function partial, so the final code is:"", ""I don't usually post to these things but this was super annoying. The confusion comes from the fact that some of the Keras imdb.py files have already updated:"", 'The easiest way is to change imdb.py setting allow_pickle=True to np.load at the line where imdb.py throws error.']"
155,Compute a confidence interval from sample data,"I have sample data which I would like to compute a confidence interval for, assuming a normal distribution.

I have found and installed the numpy and scipy packages and have gotten numpy to return a ...",https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data,"['I have sample data which I would like to compute a confidence interval for, assuming a normal distribution.', 'you can calculate like this way.', ""Here a shortened version of shasan's code, calculating the 95% confidence interval of the mean of array a:"", 'Starting Python 3.8, the standard library provides the NormalDist object as part of the statistics module:', 'Start with looking up the z-value for your desired confidence interval from a look-up table.  The confidence interval is then mean +/- z*sigma, where sigma is the estimated standard deviation of your sample mean, given by sigma = s / sqrt(n), where s is the standard deviation computed from your sample data and n is your sample size.']"
156,"copy 2D array into 3rd dimension, N times (Python)","I'd like to copy a numpy 2D array into a third dimension. For example, given the (2D) numpy array:

import numpy as np
arr = np.array([[1,2],[1,2]])
# arr.shape = (2, 2)
convert it into a 3D matrix ...",https://stackoverflow.com/questions/32171917/copy-2d-array-into-3rd-dimension-n-times-python,"[""I'd like to copy a numpy 2D array into a third dimension. For example, given the (2D) numpy array:"", 'Probably the cleanest way is to use np.repeat:', 'Another way is to use numpy.dstack.  Supposing that you want to repeat the matrix a num_repeats times:', 'Introduced in NumPy 1.10.0, we can leverage numpy.broadcast_to to simply generate a 3D view into the 2D input array. The benefit would be no extra memory overhead and virtually free runtime. This would be essential in cases where the arrays are big and we are okay to work with views. Also, this would work with generic n-dim cases.', 'Edit @Mr.F, to preserve dimension order:', 'This can now also be achived using np.tile as follows:', ""Here's a broadcasting example that does exactly what was requested.""]"
157,NumPy: function for simultaneous max() and min(),"numpy.amax() will find the max value in an array, and numpy.amin() does the same for the min value.  If I want to find both max and min, I have to call both functions, which requires passing over the (...",https://stackoverflow.com/questions/12200580/numpy-function-for-simultaneous-max-and-min,"['numpy.amax() will find the max value in an array, and numpy.amin() does the same for the min value.  If I want to find both max and min, I have to call both functions, which requires passing over the (very big) array twice, which seems slow.', 'Is there a function in the numpy API that finds both max and min with only a single pass through the data?', ""I don't think that passing over the array twice is a problem.  Consider the following pseudo-code:"", ""There is a function for finding (max-min) called numpy.ptp if that's useful for you:"", 'You could use Numba, which is a NumPy-aware dynamic Python compiler using LLVM. The resulting implementation is pretty simple and clear:', 'Just to get some ideas on the numbers one could expect, given the following approaches:', 'In general you can reduce the amount of comparisons for a minmax algorithm by processing two elements at a time and only comparing the smaller to the temporary minimum and the bigger one to the temporary maximum. On average one needs only 3/4 of the comparisons than a naive approach.', 'Nobody mentioned numpy.percentile, so I thought I would. If you ask for [0, 100] percentiles, it will give you an array of two elements, the min (0th percentile) and the max (100th percentile).', 'This is an old thread, but anyway, if anyone ever looks at this again...', 'At first glance, numpy.histogram appears to do the trick:', ""It was worth the effort for me anyways, so I'll propose the most difficult and least elegant solution here for whoever may be interested.  My solution is to implement a multi-threaded min-max in one pass algorithm in C++, and use this to create an Python extension module.  This effort requires a bit of overhead for learning how to use the Python and NumPy C/C++ APIs, and here I will show the code and give some small explanations and references for whoever wishes to go down this path."", ""Inspired by the previous answer I've written numba implementation returning minmax for axis=0 from 2-D array. It's ~5x faster than calling numpy min/max.\nMaybe someone will find it useful."", ""The shortest way I've come up with is this:""]"
158,How to calculate moving average using NumPy?,"There seems to be no function that simply calculates the moving average on numpy/scipy, leading to convoluted solutions.

My question is two-fold:
What's the easiest way to (correctly) implement a ...",https://stackoverflow.com/questions/14313510/how-to-calculate-moving-average-using-numpy,"['There seems to be no function that simply calculates the moving average on numpy/scipy, leading to convoluted solutions.', 'If you just want a straightforward non-weighted moving average, you can easily implement it with np.cumsum, which may be is faster than FFT based methods:', ""NumPy's lack of a particular domain-specific function is perhaps due to the Core Team's discipline and fidelity to NumPy's prime directive: provide an N-dimensional array type, as well as functions for creating, and indexing those arrays. Like many foundational objectives, this one is not small, and NumPy does it brilliantly."", 'A simple way to achieve this is by using np.convolve.\nThe idea behind this is to leverage the way the discrete convolution is computed and use it to return a rolling mean. This can be done by convolving with a sequence of np.ones of a length equal to the sliding window length we want.', ""Here are a variety of ways to do this, along with some benchmarks. The best methods are versions using optimized code from other libraries. The bottleneck.move_mean method is probably best all around. The scipy.convolve approach is also very fast, extensible, and syntactically and conceptually simple, but doesn't scale well for very large window values. The numpy.cumsum method is good if you need a pure numpy approach."", 'This answer using Pandas is adapted from above, as rolling_mean is not part of Pandas anymore', 'I feel this can be easily solved using bottleneck', 'In case you want to take care the edge conditions carefully (compute mean only from available elements at edges), the following function will do the trick.', ""Try this piece of code. I think it's simpler and does the job.\nlookback is the window of the moving average."", 'moving average', 'I actually wanted a slightly different behavior than the accepted answer. I was building a moving average feature extractor for an sklearn pipeline, so I required that the output of the moving average have the same dimension as the input. What I want is for the moving average to assume the series stays constant, ie a moving average of [1,2,3,4,5] with window 2 would give [1.5,2.5,3.5,4.5,5.0].', 'talib contains a simple moving average tool, as well as other similar averaging tools (i.e. exponential moving average). Below compares the method to some of the other solutions.', 'Here is a fast implementation using numba (mind the types). Note it does contain nans where shifted.', ""I use either the accepted answer's solution, slightly modified to have same length for output as input, or pandas' version as mentioned in a comment of another answer. I summarize both here with a reproducible example for future reference:"", 'By comparing the solution below with the one that uses cumsum of numpy, This one takes almost half the time. This is because it does not need to go through the entire array to do the cumsum and then do all the subtraction. Moreover, the cumsum can be ""dangerous"" if the array is huge and the number are huge (possible overflow). Of course, also here the danger exists but at least are summed together only the essential numbers.']"
159,What is the difference between i = i + 1 and i += 1 in a 'for' loop? [duplicate],"I found out a curious thing today and was wondering if somebody could shed some light into what the difference is here?

import numpy as np

A = np.arange(12).reshape(4,3)
for a in A:
    a = a + 1

B ...",https://stackoverflow.com/questions/41446833/what-is-the-difference-between-i-i-1-and-i-1-in-a-for-loop,"['I found out a curious thing today and was wondering if somebody could shed some light into what the difference is here?', 'The difference is that one modifies the data-structure itself (in-place operation) b += 1 while the other just reassigns the variable a = a + 1.', 'In the first example, you are reassigning the variable a, while in the second one you are modifying the data in-place, using the += operator.', 'As already pointed out, b += 1 updates b in-place, while a = a + 1 computes a + 1 and then assigns the name a to the result (now a does not refer to a row of A anymore).', ""The short form(a += 1) has the option to modify a in-place , instead of creating a new object representing the sum and rebinding it back to the same name(a = a + 1).So,The short form(a += 1) is much efficient as it doesn't necessarily need to make a copy of a unlike a = a + 1."", 'First off: The variables a and b in the loops refer to numpy.ndarray objects.', 'A key issue here is that this loop iterates over the rows (1st dimension) of B:']"
160,how to convert an RGB image to numpy array?,"I have an RGB image. I want to convert it to numpy array. I did the following

im = cv.LoadImage(""abc.tiff"")
a = numpy.asarray(im)
It creates an array with no shape. I assume it is a iplimage object.",https://stackoverflow.com/questions/7762948/how-to-convert-an-rgb-image-to-numpy-array,"['I have an RGB image. I want to convert it to numpy array. I did the following', ""You can use newer OpenCV python interface (if I'm not mistaken it is available since OpenCV 2.2). It natively uses numpy arrays:"", 'PIL (Python Imaging Library) and Numpy work well together.', 'You can also use matplotlib for this.', 'As of today, your best bet is to use:', ""Late answer, but I've come to prefer the imageio module to the other alternatives"", 'You need to use cv.LoadImageM instead of cv.LoadImage:', 'When using the answer from David Poole I get a SystemError with gray scale PNGs and maybe other files. My solution is:', 'OpenCV image format supports the numpy array interface. A helper function can be made to support either grayscale or color images. This means the BGR -> RGB conversion can be conveniently done with a numpy slice, not a full copy of image data.', 'I also adopted imageio, but I found the following machinery useful for pre- and post-processing:', 'You can get numpy array of rgb image easily by using numpy and Image from PIL', 'load the image by using following syntax:-', 'Using Keras:']"
161,Numpy how to iterate over columns of array?,"Suppose I have and m x n array.  I want to pass each column of this array to a function to perform some operation on the entire column.  How do I iterate over the columns of the array?

For example, I ...",https://stackoverflow.com/questions/10148818/numpy-how-to-iterate-over-columns-of-array,"['Suppose I have and m x n array.  I want to pass each column of this array to a function to perform some operation on the entire column.  How do I iterate over the columns of the array?', 'Just iterate over the transposed of your array:', 'This should give you a start', 'For a three dimensional array you could try:', 'You can also use unzip to iterate through the columns', ""For example you want to find a mean of each column in matrix. Let's create the following matrix"", 'Alternatively, you can use enumerate. It gives you the column number and the column values as well.']"
162,"What is dtype('O'), in pandas?","I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get;

dtype('O')...",https://stackoverflow.com/questions/37561991/what-is-dtypeo-in-pandas,"[""I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get;"", 'It means:', 'What is dtype?', 'It means ""a python object"", i.e. not one of the builtin scalar types supported by numpy.', ""'O' stands for object.""]"
163,Use numpy array in shared memory for multiprocessing,"I would like to use a numpy array in shared memory for use with the multiprocessing module.  The difficulty is using it like a numpy array, and not just as a ctypes array.

from multiprocessing import ...",https://stackoverflow.com/questions/7894791/use-numpy-array-in-shared-memory-for-multiprocessing,"['I would like to use a numpy array in shared memory for use with the multiprocessing module.  The difficulty is using it like a numpy array, and not just as a ctypes array.', ""To add to @unutbu's (not available anymore) and @Henry Gomersall's answers. You could use shared_arr.get_lock() to synchronize access when needed:"", 'The Array object has a get_obj() method associated with it, which returns the ctypes array which presents a buffer interface. I think the following should work...', 'While the answers already given are good, there is a much easier solution to this problem provided two conditions are met:', ""I've written a small python module that uses POSIX shared memory to share numpy arrays between python interpreters. Maybe you will find it handy."", 'You can use the sharedmem module: https://bitbucket.org/cleemesser/numpy-sharedmem']"
164,binning data in python with scipy/numpy,"is there a more efficient way to take an average of an array in prespecified bins? for example, i have an array of numbers and an array corresponding to bin start and end positions in that array, and ...",https://stackoverflow.com/questions/6163334/binning-data-in-python-with-scipy-numpy,"['is there a more efficient way to take an average of an array in prespecified bins? for example, i have an array of numbers and an array corresponding to bin start and end positions in that array, and I want to just take the mean in those bins? I have code that does it below but i am wondering how it can be cut down and improved. thanks.', ""It's probably faster and easier to use numpy.digitize():"", 'The Scipy (>=0.11) function scipy.stats.binned_statistic specifically addresses the above question.', 'Not sure why this thread got necroed; but here is a 2014 approved answer, which should be far faster:', 'The numpy_indexed package (disclaimer: I am its author) contains functionality to efficiently perform operations of this type:', 'I would add, and also to answer the question find mean bin values using histogram2d python that the scipy also have a function specially designed to compute a bidimensional binned statistic for one or more sets of data', 'Another alternative is to use the ufunc.at. This method applies in-place a desired operation at specified indices.\nWe can get the bin position for each datapoint using the searchsorted method. \nThen we can use at to increment by 1 the position of histogram at the index given by bin_indexes, every time we encounter an index at bin_indexes.']"
165,Principal component analysis in Python,"I'd like to use principal component analysis (PCA) for dimensionality reduction.  Does numpy or scipy already have it, or do I have to roll my own using numpy.linalg.eigh?

I don't just want to use ...",https://stackoverflow.com/questions/1730600/principal-component-analysis-in-python,"[""I'd like to use principal component analysis (PCA) for dimensionality reduction.  Does numpy or scipy already have it, or do I have to roll my own using numpy.linalg.eigh?"", 'You might have a look at MDP.', ""Months later, here's a small class PCA, and a picture:"", ""PCA using numpy.linalg.svd is super easy. Here's a simple demo:"", 'You can use sklearn:', 'matplotlib.mlab has a PCA implementation.', 'SVD should work fine with 460 dimensions. It takes about 7 seconds on my Atom netbook. The eig() method takes more time (as it should, it uses more floating point operations) and will almost always be less accurate.', 'You can quite easily ""roll"" your own using scipy.linalg (assuming a pre-centered dataset data):', 'I just finish reading the book Machine Learning: An Algorithmic Perspective. All code examples in the book was written by Python(and almost with Numpy). The code snippet of chatper10.2 Principal Components Analysis maybe worth a reading. It use numpy.linalg.eig.\nBy the way, I think SVD can handle 460 * 460 dimensions very well. I have calculate a 6500*6500 SVD with numpy/scipy.linalg.svd on a very old PC:Pentium III 733mHz. To be honest, the script needs a lot of memory(about 1.xG) and a lot of time(about 30 minutes) to get the SVD result.\nBut I think 460*460 on a modern PC will not be a big problem unless u need do SVD a huge number of times.', 'You do not need full Singular Value Decomposition (SVD) at it computes all eigenvalues and eigenvectors and can be prohibitive for large matrices.\nscipy and its sparse module provide generic linear algrebra functions working on both sparse and dense matrices, among which there is the eig* family of functions :', 'Here is another implementation of a PCA module for python using numpy, scipy and C-extensions. The module carries out PCA using either a SVD or the NIPALS (Nonlinear Iterative Partial Least Squares) algorithm which is implemented in C.', ""If you're working with 3D vectors, you can apply SVD concisely using the toolbelt vg. It's a light layer on top of numpy.""]"
166,List to array conversion to use ravel() function,I have a list in python and I want to convert it to an array to be able to use ravel() function.,https://stackoverflow.com/questions/15868512/list-to-array-conversion-to-use-ravel-function,"['I have a list in python and I want to convert it to an array to be able to use ravel() function.', 'Use numpy.asarray:', 'I wanted a way to do this without using an extra module. First turn list to string, then append to an array:', ""If all you want is calling ravel on your (nested, I s'pose?) list, you can do that directly, numpy will do the casting for you:"", 'Use the following code:', 'if variable b has a list then you can simply do the below:']"
167,How to get element-wise matrix multiplication (Hadamard product) in numpy?,"I have two matrices 

a = np.matrix([[1,2], [3,4]])
b = np.matrix([[5,6], [7,8]])
and I want to get the element-wise product, [[1*5,2*6], [3*7,4*8]], equaling 

[[5,12], [21,32]]

I have tried

print(...",https://stackoverflow.com/questions/40034993/how-to-get-element-wise-matrix-multiplication-hadamard-product-in-numpy,"['I have two matrices', 'For elementwise multiplication of matrix objects, you can use numpy.multiply:', 'just do this:', 'Both np.multiply and * would yield element wise multiplication known as the Hadamard Product', 'Try this:']"
168,Convert numpy array to tuple,"Note: This is asking for the reverse of the usual tuple-to-array conversion.

I have to pass an argument to a (wrapped c++) function as a nested tuple.  For example, the following works

X = ...",https://stackoverflow.com/questions/10016352/convert-numpy-array-to-tuple,"['Note: This is asking for the reverse of the usual tuple-to-array conversion.', ""Here's a function that'll do it:"", 'I was not satisfied, so I finally used this:', 'Another option', 'If you like long cuts, here is another way\ntuple(tuple(a_m.tolist()) for a_m in a )']"
169,ValueError when checking if variable is None or numpy.array,"I'd like to check if variable is None or numpy.array. I've implemented check_a function to do this.

def check_a(a):
    if not a:
        print ""please initialize a""

a = None
check_a(a)
a = np.array(...",https://stackoverflow.com/questions/36783921/valueerror-when-checking-if-variable-is-none-or-numpy-array,"[""I'd like to check if variable is None or numpy.array. I've implemented check_a function to do this."", ""Using not a to test whether a is None assumes that the other possible values of a have a truth value of True. However, most NumPy arrays don't have a truth value at all, and not cannot be applied to them."", 'If you are trying to do something very similar: a is not None, the same issue comes up. That is, Numpy complains that one must use a.any or a.all.', 'You can see if object has shape or not']"
170,Detect if a NumPy array contains at least one non-numeric value?,I need to write a function which will detect if the input contains at least one value which is non-numeric. If a non-numeric value is found I will raise an error (because the calculation should only ...,https://stackoverflow.com/questions/911871/detect-if-a-numpy-array-contains-at-least-one-non-numeric-value,"['I need to write a function which will detect if the input contains at least one value which is non-numeric. If a non-numeric value is found I will raise an error (because the calculation should only return a numeric value). The number of dimensions of the input array is not known in advance - the function should give the correct value regardless of ndim. As an extra complication the input could be a single float or numpy.float64 or even something oddball like a zero-dimensional array.', 'This should be faster than iterating and will work regardless of shape.', 'If infinity is a possible value, I would use numpy.isfinite', 'Pfft! Microseconds!\nNever solve a problem in microseconds that can be solved in nanoseconds.', 'With numpy 1.3 or svn you can do this', '(np.where(np.isnan(A)))[0].shape[0] will be greater than 0 if A contains at least one element of nan, A could be an n x m matrix.']"
171,surface plots in matplotlib,"I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points.

The plot_surface function in the mplot3d package requires as arguments X,Y ...",https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib,"['I have a list of 3-tuples representing a set of points in 3D space. I want to plot a surface that covers all these points.', ""For surfaces it's a bit different than a list of 3-tuples, you should pass in a grid for the domain in 2d arrays."", 'You can read data direct from some file and plot', ""I just came across this same problem.  I have evenly spaced data that is in 3 1-D arrays instead of the 2-D arrays that matplotlib's plot_surface wants.  My data happened to be in a pandas.DataFrame so here is the matplotlib.plot_surface example with the modifications to plot 3 1-D arrays."", 'Just to chime in, Emanuel had the answer that I (and probably many others) are looking for. If you have 3d scattered data in 3 separate arrays, pandas is an incredible help and works much better than the other options. To elaborate, suppose your x,y,z are some arbitrary variables. In my case these were c,gamma, and errors because I was testing a support vector machine. There are many potential choices to plot the data:', 'check the official example.\nX,Y and Z are indeed 2d arrays, numpy.meshgrid() is a simple way to get 2d x,y mesh out of 1d x and y values.', ""Just to add some further thoughts which may help others with irregular domain type problems. For a situation where the user has three vectors/lists, x,y,z representing a 2D solution where z is to be plotted on a rectangular grid as a surface, the 'plot_trisurf()' comments by ArtifixR are applicable. A similar example but with non rectangular domain is:"", 'In Matlab I did something similar using the delaunay function on the x, y coords only (not the z), then plotting with trimesh or trisurf, using z as the height.', 'It is not possible to directly make a 3d surface using your data. I would recommend you to build an interpolation model using some tools like pykridge. The process will include three steps:']"
172,How to convert list of numpy arrays into single numpy array?,"Suppose I have ;

LIST = [[array([1, 2, 3, 4, 5]), array([1, 2, 3, 4, 5],[1,2,3,4,5])] # inner lists are numpy arrays
I try to convert;

array([[1, 2, 3, 4, 5],
       [1, 2, 3, 4, 5],
       [1, 2, ...",https://stackoverflow.com/questions/27516849/how-to-convert-list-of-numpy-arrays-into-single-numpy-array,"['Suppose I have ;', 'In general you can concatenate a whole sequence of arrays along any axis:', 'Starting in NumPy version 1.10, we have the method stack. It can stack arrays of any dimension (all equal):', 'I checked some of the methods for speed performance and find that there is no difference!\nThe only difference is that using some methods you must carefully check dimension.']"
173,python numpy machine epsilon,"I am trying to understand what is machine epsilon. According to the Wikipedia, it can be calculated as follows:

def machineEpsilon(func=float):
    machine_epsilon = func(1)
    while func(1)+func(...",https://stackoverflow.com/questions/19141432/python-numpy-machine-epsilon,"['I am trying to understand what is machine epsilon. According to the Wikipedia, it can be calculated as follows:', 'An easier way to get the machine epsilon for a given float type is to use np.finfo():', 'Another easy way to get epsilon is:', 'It will already work, as David pointed out!']"
174,Histogram Matplotlib,"So I have a little problem. I have a data set in scipy that is already in the histogram format, so I have the center of the bins and the number of events per bin. How can I now plot is as a histogram. ...",https://stackoverflow.com/questions/5328556/histogram-matplotlib,"['So I have a little problem. I have a data set in scipy that is already in the histogram format, so I have the center of the bins and the number of events per bin. How can I now plot is as a histogram. I tried just doing', '', ""If you don't want bars you can plot it like this:"", 'I know this does not answer your question, but I always end up on this page, when I search for the matplotlib solution to histograms, because the simple histogram_demo was removed from the matplotlib example gallery page.', ""If you're willing to use pandas:"", 'I think this might be useful for someone.']"
175,How can I use numpy.correlate to do autocorrelation?,"I need to do auto-correlation of a set of numbers, which as I understand it is just the correlation of the set with itself. 

I've tried it using numpy's correlate function, but I don't believe the ...",https://stackoverflow.com/questions/643699/how-can-i-use-numpy-correlate-to-do-autocorrelation,"['I need to do auto-correlation of a set of numbers, which as I understand it is just the correlation of the set with itself.', ""To answer your first question, numpy.correlate(a, v, mode) is performing the convolution of a with the reverse of v and giving the results clipped by the specified mode. The definition of convolution, C(t)=∑ -∞ < i < ∞  aivt+i where -∞ < t < ∞, allows for results from -∞ to ∞, but you obviously can't store an infinitely long array. So it has to be clipped, and that is where the mode comes in. There are 3 different modes: full, same, & valid:"", 'Auto-correlation comes in two versions: statistical and convolution. They both do the same, except for a little detail: The statistical version is normalized to be on the interval [-1,1]. Here is an example of how you do the statistical one:', 'Use the numpy.corrcoef function instead of numpy.correlate to calculate the statistical correlation for a lag of t:', 'I think there are 2 things that add confusion to this topic:', ""As I just ran into the same problem, I would like to share a few lines of code with you. In fact there are several rather similar posts about autocorrelation in stackoverflow by now. If you define the autocorrelation as a(x, L) = sum(k=0,N-L-1)((xk-xbar)*(x(k+L)-xbar))/sum(k=0,N-1)((xk-xbar)**2) [this is the definition given in IDL's a_correlate function and it agrees with what I see in answer 2 of question #12269834], then the following seems to give the correct results:"", 'Your question 1 has been already extensively discussed in several excellent answers here.', 'An alternative to numpy.correlate is available in statsmodels.tsa.stattools.acf(). This yields a continuously decreasing autocorrelation function like the one described by OP. Implementing it is fairly simple:', ""I'm a computational biologist, and when I had to compute the auto/cross-correlations between couples of time series of stochastic processes I realized that np.correlate was not doing the job I needed."", 'I use talib.CORREL for autocorrelation like this, I suspect you could do the same with other packages:', 'Using Fourier transformation and the convolution theorem', ""I think the real answer to the OP's question is succinctly contained in this excerpt from the Numpy.correlate documentation:"", 'A simple solution without pandas:', 'Plot the statistical autocorrelation given a pandas datatime Series of returns:']"
176,Best way to assert for numpy.array equality?,"I want to make some unit-tests for my app, and I need to compare two arrays. Since array.__eq__ returns a new array (so TestCase.assertEqual fails), what is the best way to assert for equality?

...",https://stackoverflow.com/questions/3302949/best-way-to-assert-for-numpy-array-equality,"['I want to make some unit-tests for my app, and I need to compare two arrays. Since array.__eq__ returns a new array (so TestCase.assertEqual fails), what is the best way to assert for equality?', 'check out the assert functions in numpy.testing, e.g.', 'I think (arr1 == arr2).all() looks pretty nice. But you could use:', 'I find that using\nself.assertEqual(arr1.tolist(), arr2.tolist())\nis the easiest way of comparing arrays with unittest.', 'Since Python 3.2 you can use assertSequenceEqual(array1.tolist(), array2.tolist()).', 'In my tests I use this:', 'np.linalg.norm(arr1 - arr2) < 1e-6']"
177,Numpy array assignment with copy,"For example, if we have a numpy array A, and we want a numpy array B with the same elements.

What is the difference between the following (see below) methods? When is additional memory allocated, and ...",https://stackoverflow.com/questions/19676538/numpy-array-assignment-with-copy,"['For example, if we have a numpy array A, and we want a numpy array B with the same elements.', 'All three versions do different things:', 'the last two need additional memory.', 'This is the only working answer for me:']"
178,Benchmarking (python vs. c++ using BLAS) and (numpy),"I would like to write a program that makes extensive use of BLAS and LAPACK linear algebra functionalities. Since performance is an issue I did some benchmarking and would like know, if the approach I ...",https://stackoverflow.com/questions/7596612/benchmarking-python-vs-c-using-blas-and-numpy,"['I would like to write a program that makes extensive use of BLAS and LAPACK linear algebra functionalities. Since performance is an issue I did some benchmarking and would like know, if the approach I took is legitimate.', ""I've run your benchmark. There is no difference between C++ and numpy on my machine:"", 'I re-run the the benchmark on our new HPC. \nBoth the hardware as well as the software stack changed from the setup in the original answer.', ""Here's another benchmark (on Linux, just type make): http://dl.dropbox.com/u/5453551/blas_call_benchmark.zip"", ""Given the rigor you've shown with your analysis, I'm surprised by the results thus far.  I put this as an 'answer' but only because it's too long for a comment and does provide a possibility (though I expect you've considered it).""]"
179,What is the difference between contiguous and non-contiguous arrays?,"In the numpy manual about the reshape() function, it says 

>>> a = np.zeros((10, 2))
# A transpose make the array non-contiguous
>>> b = a.T
# Taking a view makes it possible to ...",https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays,"['In the numpy manual about the reshape() function, it says', 'A contiguous array is just an array stored in an unbroken block of memory: to access the next value in the array, we just move to the next memory address.', 'Maybe this example with 12 different array values will help:']"
180,How to save and load numpy.array() data properly?,"I wonder, how to save and load numpy.array data properly. Currently I'm using the numpy.savetxt() method. For example, if I got an array markers, which looks like this:
I try to save it by the use of:...",https://stackoverflow.com/questions/28439701/how-to-save-and-load-numpy-array-data-properly,"[""I wonder, how to save and load numpy.array data properly. Currently I'm using the numpy.savetxt() method. For example, if I got an array markers, which looks like this:"", 'The most reliable way I have found to do this is to use np.savetxt with np.loadtxt and not np.fromfile which is better suited to binary files written with tofile. The np.fromfile and np.tofile methods write and read binary files whereas np.savetxt writes a text file.\nSo, for example:', 'np.fromfile() has a sep= keyword argument:', 'For a short answer you should use np.save and np.load. The advantages of these is that they are made by developers of the numpy library and they already work (plus are likely already optimized nicely) e.g.']"
181,How to return 0 with divide by zero,"I'm trying to perform an element wise divide in python, but if a zero is encountered, I need the quotient to just be zero.

For example:

array1 = np.array([0, 1, 2])
array2 = np.array([0, 1, 1])

...",https://stackoverflow.com/questions/26248654/how-to-return-0-with-divide-by-zero,"[""I'm trying to perform an element wise divide in python, but if a zero is encountered, I need the quotient to just be zero."", 'In numpy v1.7+, you can take advantage of the ""where"" option for ufuncs.  You can do things in one line and you don\'t have to deal with the errstate context manager.', ""Building on @Franck Dernoncourt's answer, fixing -1 / 0:"", 'Building on the other answers, and improving on:', 'One-liner (throws warning)', 'Try doing it in two steps. Division first, then replace.', 'You can also replace based on inf, only if the array dtypes are floats, as per this answer:', 'One answer I found searching a related question was to manipulate the output based upon whether the denominator was zero or not.', 'An other solution worth mentioning :']"
182,"Rank items in an array using Python/NumPy, without sorting array twice","I have an array of numbers and I'd like to create another array that represents the rank of each item in the first array.  I'm using Python and NumPy.

For example:

array = [4,2,7,1]
ranks = [2,1,3,0]...",https://stackoverflow.com/questions/5284646/rank-items-in-an-array-using-python-numpy-without-sorting-array-twice,"[""I have an array of numbers and I'd like to create another array that represents the rank of each item in the first array.  I'm using Python and NumPy."", 'Use slicing on the left-hand side in the last step:', 'Use argsort twice, first to obtain the order of the array, then to obtain ranking:', ""This question is a few years old, and the accepted answer is great, but I think the following is still worth mentioning.  If you don't mind the dependency on scipy, you can use scipy.stats.rankdata:"", 'I tried to extend both solution for arrays A of more than one dimension, supposing you process your array row-by-row (axis=1).', ""For a vectorized version of an averaged rank, see below. I love np.unique, it really widens the scope of what code can and cannot be efficiently vectorized. Aside from avoiding python for-loops, this approach also avoids the implicit double loop over 'a'."", 'Apart from the elegance and shortness of solutions, there is also the question of performance. Here is a little benchmark:', 'Use argsort() twice will do it:', 'I tried the above methods, but failed because I had many zeores. Yes, even with floats duplicate items may be important.', 'I liked the method by k.rooijers, but as rcoup wrote, repeated numbers are ranked according to array position. This was no good for me, so I modified the version to postprocess the ranks and merge any repeated numbers into a combined average rank:', 'argsort and slice are symmetry operations.', 'More general version of one of the answers:']"
183,Installing Numpy on 64bit Windows 7 with Python 2.7.3 [closed],"It looks like the only 64 bit windows installer for Numpy is for Numpy version 1.3.0 which only works with Python 2.6

http://sourceforge.net/projects/numpy/files/NumPy/

It strikes me as strange that ...",https://stackoverflow.com/questions/11200137/installing-numpy-on-64bit-windows-7-with-python-2-7-3,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'Try the (unofficial) binaries in this site:', 'Assuming you have python 2.7 64bit on your computer and have downloaded numpy from here, follow the steps below (changing numpy‑1.9.2+mkl‑cp27‑none‑win_amd64.whl as appropriate).', 'Download numpy-1.9.2+mkl-cp27-none-win32.whl from http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy .', ""The (unofficial) binaries (http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy)  worked for me.\nI've tried Mingw, Cygwin, all failed due to varies reasons. I am on Windows 7 Enterprise, 64bit."", 'You may also try this, anaconda\nhttp://continuum.io/downloads', 'It is not improbable, that programmers looking for python on windows, also use the Python Tools for Visual Studio. In this case it is easy to install additional packages, by taking advantage of the included ""Python Environment"" Window. ""Overview"" is selected within the window as default. You can select ""Pip"" there.']"
184,'DataFrame' object has no attribute 'sort',"I face some problem here, in my python package I have install numpy, but I still have this error 'DataFrame' object has no attribute 'sort'

Anyone can give me some idea..

This is my code :

final....",https://stackoverflow.com/questions/44123874/dataframe-object-has-no-attribute-sort,"[""I face some problem here, in my python package I have install numpy, but I still have this error 'DataFrame' object has no attribute 'sort'"", 'sort() was deprecated for DataFrames in favor of either:', 'sort has been replaced in v0.20 by DataFrame.sort_values and DataFrame.sort_index. Aside from this, we also have argsort.']"
185,Binning column with python pandas,"I have a Data Frame column with numeric values:

df['percentage'].head()
46.5
44.2
100.0
42.12
I want to see the column as bin counts:

bins = [0, 1, 5, 10, 25, 50, 100]
How can I get the result as ...",https://stackoverflow.com/questions/45273731/binning-column-with-python-pandas,"['I have a Data Frame column with numeric values:', 'You can use pandas.cut:', 'On big datasets (500k >) pd.cut can be quite slow for binning data.']"
186,Numpy: find first index of value fast,"How can I find the index of the first occurrence of a number in a Numpy array?
Speed is important to me. I am not interested in the following answers because they scan the whole array and don't stop ...",https://stackoverflow.com/questions/7632963/numpy-find-first-index-of-value-fast,"[""How can I find the index of the first occurrence of a number in a Numpy array?\nSpeed is important to me. I am not interested in the following answers because they scan the whole array and don't stop when they find the first occurrence:"", 'There is a feature request for this scheduled for Numpy 2.0.0: https://github.com/numpy/numpy/issues/2269', 'Although it is way too late for you, but for future reference:\nUsing numba (1) is the easiest way until numpy implements it. If you use anaconda python distribution it should already be installed.\nThe code will be compiled so it will be fast.', ""I've made a benchmark for several methods:"", 'You can convert a boolean array to a Python string using array.tostring() and then using the find() method:', 'In case of sorted arrays np.searchsorted works.', 'I think you have hit a problem where a different method and some a priori knowledge of the array would really help.  The kind of thing where you have a X probability of finding your answer in the first Y percent of the data.  The splitting up the problem with the hope of getting lucky then doing this in python with a nested list comprehension or something.', '@tal already presented a numba function to find the first index but that only works for 1D arrays. With np.ndenumerate you can also find the first index in an arbitarly dimensional array:', ""If your list is sorted, you can achieve very quick search of index with the 'bisect' package.\nIt's O(log(n)) instead of O(n)."", 'As far as I know only np.any and np.all on boolean arrays are short-circuited.', ""I needed this for my job so I taught myself Python and Numpy's C interface and wrote my own.  http://pastebin.com/GtcXuLyd It's only for 1-D arrays, but works for most data types (int, float, or strings) and testing has shown it is again about 20 times faster than the expected approach in pure Python-numpy."", 'This problem can be effectively solved in pure numpy by processing the array in chunks:', 'If you are looking for the first non-zero element you can use a following hack:', 'As a longtime matlab user I a have been searching for an efficient solution to this problem for quite a while. Finally, motivated by discussions a propositions in this thread I have tried to come up with a solution that is implementing an API similar to what was suggested here, supporting for the moment only 1D arrays.', ""Just a note that if you are doing a sequence of searches, the performance gain from doing something clever like converting to string, might be lost in the outer loop if the search dimension isn't big enough. See how the performance of iterating find1 that uses the string conversion trick proposed above and find2 that uses argmax along the inner axis (plus an adjustment to ensure a non-match returns as -1)"", 'how about this', ""You can covert your array into a list and use it's index() method:""]"
187,Is there a numpy builtin to reject outliers from a list,"Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in ...",https://stackoverflow.com/questions/11686720/is-there-a-numpy-builtin-to-reject-outliers-from-a-list,"['Is there a numpy builtin to do something like the following? That is, take a list d and return a list filtered_d with any outlying elements removed based on some assumed distribution of the points in d.', 'This method is almost identical to yours, just more numpyst (also working on numpy arrays only):', 'Something important when dealing with outliers is that one should try to use estimators as robust as possible. The mean of a distribution will be biased by outliers but e.g. the median will be much less.', ""Benjamin Bannier's answer yields a pass-through when the median of distances from the median is 0, so I found this modified version a bit more helpful for cases as given in the example below."", ""Building on Benjamin's, using pandas.Series, and replacing MAD with IQR:"", 'An alternative is to make a robust estimation of the standard deviation (assuming Gaussian statistics). Looking up online calculators, I see that the 90% percentile corresponds to 1.2815σ and the 95% is 1.645σ (http://vassarstats.net/tabs.html?#z)', 'I would like to provide two methods in this answer, solution based on ""z score"" and solution based on ""IQR"".', 'Consider that all the above methods fail when your standard deviation gets very large due to huge outliers.', ""I wanted to do something similar, except setting the number to NaN rather than removing it from the data, since if you remove it you change the length which can mess up plotting (i.e. if you're only removing outliers from one column in a table, but you need it to remain the same as the other columns so you can plot them against each other)."", 'if you want to get the index position of the outliers idx_list will return it.', 'For a set of images (each image has 3 dimensions), where I wanted to reject outliers for each pixel I used:']"
188,Numpy isnan() fails on an array of floats (from pandas dataframe apply),"I have an array of floats (some normal numbers, some nans) that is coming out of an apply on a pandas dataframe.

For some reason, numpy.isnan is failing on this array, however as shown below, each ...",https://stackoverflow.com/questions/36000993/numpy-isnan-fails-on-an-array-of-floats-from-pandas-dataframe-apply,"['I have an array of floats (some normal numbers, some nans) that is coming out of an apply on a pandas dataframe.', 'np.isnan can be applied to NumPy arrays of native dtype (such as np.float64):', 'A great substitute for np.isnan() and pd.isnull() is', 'On top of @unutbu answer, you could coerce pandas numpy object array to native (float64) type, something along the line', 'Make sure you import csv file using Pandas']"
189,Efficiently checking if arbitrary object is NaN in Python / numpy / pandas?,"My numpy arrays use np.nan to designate missing values. As I iterate over the data set, I need to detect such missing values and handle them in special ways.

Naively I used numpy.isnan(val), which ...",https://stackoverflow.com/questions/18689512/efficiently-checking-if-arbitrary-object-is-nan-in-python-numpy-pandas,"['My numpy arrays use np.nan to designate missing values. As I iterate over the data set, I need to detect such missing values and handle them in special ways.', 'pandas.isnull() (also pd.isna(), in newer versions) checks for missing values in both numeric and string/object arrays. From the documentation, it checks for:', 'Is your type really arbitrary?  If you know it is just going to be a int float or string you could just do']"
190,What is the equivalent of MATLAB's repmat in NumPy,"I would like to execute the equivalent of the following MATLAB code using NumPy: repmat([1; 1], [1 1 1]).  How would I accomplish this?",https://stackoverflow.com/questions/1721802/what-is-the-equivalent-of-matlabs-repmat-in-numpy,"['I would like to execute the equivalent of the following MATLAB code using NumPy: repmat([1; 1], [1 1 1]).  How would I accomplish this?', ""Here is a much better (official) NumPy for Matlab Users link - I'm afraid the mathesaurus one is quite out of date."", ""Note that some of the reasons you'd need to use MATLAB's repmat are taken care of by NumPy's broadcasting mechanism, which allows you to do various types of math with arrays of similar shape. So if you had, say, a 1600x1400x3 array representing a 3-color image, you could (elementwise) multiply it by [1.0 0.25 0.25] to reduce the amount of green and blue at each pixel. See the above link for more information."", 'See NumPy for Matlab users.', 'This is how I understood it out of a bit of fiddling around. Happy to be corrected and hope this helps.', 'Know both tile and repeat.', 'numpy.matlib has a repmat function with a similar interface as the matlab function']"
191,Why does it take ages to install Pandas on Alpine Linux,I've noticed that installing Pandas and Numpy (it's dependency) in a Docker container using the base OS Alpine vs. CentOS or Debian takes much longer. I created a little test below to demonstrate the ...,https://stackoverflow.com/questions/49037742/why-does-it-take-ages-to-install-pandas-on-alpine-linux,"[""I've noticed that installing Pandas and Numpy (it's dependency) in a Docker container using the base OS Alpine vs. CentOS or Debian takes much longer. I created a little test below to demonstrate the time difference. Aside from the few seconds Alpine takes to update and download the build dependencies to install Pandas and Numpy, why does the setup.py take around 70x more time than on Debian install?"", 'Debian based images use only python pip to install packages with .whl format:', ""ANSWER: AS OF 3/9/2020, FOR PYTHON 3, IT STILL DOESN'T!"", 'ATTENTION\nLook at the @jtlz2 answer with the latest update', 'Just going to bring some of these answers together in one answer and add a detail I think was missed. The reason certain python libraries, particularly optimized math and data libraries, take so long to build on alpine is because the pip wheels for these libraries include binaries precompiled from c/c++ and linked against glibc, a common set of c standard libraries. Debian, Fedora, CentOS all (typically) use glibc, but alpine, in order to stay lightweight, uses musl-libc instead. c/c++ binaries build on a glibc system will not work on a system without glibc and the same goes for musl.', 'Real honest advice here, switch to Debian based image and then all your problems will be gone.', 'This worked for me:', ""pandas is considered a community supported package, so the answers pointing to edge/testing are not going to work as Alpine does not officially support pandas as a core package (it still works, it's just not supported by the core Alpine developers)."", 'alpine takes lot of time to install pandas and the image size is also huge. I tried the python:3.8-slim-buster version of python base image. Image build was very fast and size of image was less than half in comparison to alpine python docker image']"
192,"How to split/partition a dataset into training and test datasets for, e.g., cross validation?",What is a good way to split a NumPy array randomly into training and testing/validation dataset? Something similar to the cvpartition or crossvalind functions in Matlab.,https://stackoverflow.com/questions/3674409/how-to-split-partition-a-dataset-into-training-and-test-datasets-for-e-g-cros,"['What is a good way to split a NumPy array randomly into training and testing/validation dataset? Something similar to the cvpartition or crossvalind functions in Matlab.', 'If you want to split the data set once in two halves, you can use numpy.random.shuffle, or numpy.random.permutation if you need to keep track of the indices:', ""There is another option that just entails using scikit-learn. As scikit's wiki describes, you can just use the following instructions:"", 'Just a note. In case you want train, test, AND validation sets, you can do this:', 'As sklearn.cross_validation module was deprecated, you can use:', 'You may also consider stratified division into training and testing set. Startified division also generates training and testing set randomly but in such a way that original class proportions are preserved. This makes training and testing sets better reflect the properties of the original dataset.', ""I wrote a function for my own project to do this (it doesn't use numpy, though):"", 'Here is a code to split the data into n=5 folds in a stratified manner', 'Thanks pberkes for your answer. I just modified it to avoid (1) replacement while sampling (2) duplicated instances occurred in both training and testing:', 'After doing some reading and taking into account the (many..) different ways of splitting the data to train and test, I decided to timeit!', 'Likely you will not only need to split into train and test, but also cross validation to make sure your model generalizes. \nHere I am assuming 70% training data, 20% validation and 10% holdout/test data.', 'Split into train test and valid', ""I'm aware that my solution is not the best, but it comes in handy when you want to split data in a simplistic way, especially when teaching data science to newbies!""]"
193,Input and output numpy arrays to h5py,"I have a Python code whose output is a  sized matrix, whose entries are all of the type float. If I save it with the extension .dat the file size is of the order of 500 MB. I read that using h5py ...",https://stackoverflow.com/questions/20928136/input-and-output-numpy-arrays-to-h5py,"[""I have a Python code whose output is a  sized matrix, whose entries are all of the type float. If I save it with the extension .dat the file size is of the order of 500 MB. I read that using h5py reduces the file size considerably. So, let's say I have the 2D numpy array named A. How do I save it to an h5py file?\nAlso, how do I read the same file and put it as a numpy array in a different code, as I need to do manipulations with the array?"", 'h5py provides a model of datasets and groups. The former is basically arrays and the latter you can think of as directories. Each is named. You should look at the documentation for the API and examples:', 'A cleaner way to handle file open/close and avoid memory leaks:']"
194,How to identify numpy types in python?,"How can one reliably determine if an object has a numpy type?

I realize that this question goes against the philosophy of duck typing, but idea is to make sure a function (which uses scipy and numpy) ...",https://stackoverflow.com/questions/12569452/how-to-identify-numpy-types-in-python,"['How can one reliably determine if an object has a numpy type?', 'Use the builtin type function to get the type, then you can use the __module__ property to find out where it was defined:', ""The solution I've come up with is:"", ""Old question but I came up with a definitive answer with an example. Can't hurt to keep questions fresh as I had this same problem and didn't find a clear answer. The key is to make sure you have numpy imported, and then run the isinstance bool.  While this may seem simple, if you are doing some computations across different data types, this small check can serve as a quick test before your start some numpy vectorized operation."", ""That actually depends on what you're looking for."", 'To get the type, use the builtin type function. With the in operator, you can test if the type is a numpy type by checking if it contains the string numpy;', ""Note that the type(numpy.ndarray) is a type itself and watch out for boolean and scalar types.  Don't be too discouraged if it's not intuitive or easy, it's a pain at first.""]"
195,How to calculate cumulative normal distribution?,I am looking for a function in Numpy or Scipy (or any rigorous Python library) that will give me the cumulative normal distribution function in Python.,https://stackoverflow.com/questions/809362/how-to-calculate-cumulative-normal-distribution,"['I am looking for a function in Numpy or Scipy (or any rigorous Python library) that will give me the cumulative normal distribution function in Python.', ""Here's an example:"", 'It may be too late to answer the question but since Google still leads people here, I decide to write my solution here.', 'Starting Python 3.8, the standard library provides the NormalDist object as part of the statistics module.', 'Adapted from here http://mail.python.org/pipermail/python-list/2000-June/039873.html', ""To build upon Unknown's example, the Python equivalent of the function normdist() implemented in a lot of libraries would be:"", ""Alex's answer shows you a solution for standard normal distribution (mean = 0, standard deviation = 1). If you have normal distribution with mean and std (which is sqr(var)) and you want to calculate:"", 'Taken from above:', 'Simple like this:', ""As Google gives this answer for the search netlogo pdf, here's the netlogo version of the above python code""]"
196,"Strings in a DataFrame, but dtype is object","Why does Pandas tell me that I have objects, although every item in the selected column is a string — even after explicit conversion.

This is my DataFrame:

<class 'pandas.core.frame.DataFrame'>...",https://stackoverflow.com/questions/21018654/strings-in-a-dataframe-but-dtype-is-object,"['Why does Pandas tell me that I have objects, although every item in the selected column is a string — even after explicit conversion.', 'The dtype object comes from NumPy, it describes the type of element in a ndarray. Every element in a ndarray must has the same size in byte. For int64 and float64, they are 8 bytes. But for strings, the length of the string is not fixed. So instead of save the bytes of strings in the ndarray directly, Pandas use object ndarray, which save pointers to objects, because of this the dtype of this kind ndarray is object.', ""@HYRY's answer is great. I just want to provide a little more context.."", 'The accepted answer is good. Just wanted to provide an answer which referenced the documentation. The documentation says:', 'As of version 1.0.0 (January 2020), pandas has introduced as an experimental feature providing first-class support for string types through pandas.StringDtype.']"
197,Numpy index slice without losing dimension information,"I'm using numpy and want to index a row without losing the dimension information.

import numpy as np
X = np.zeros((100,10))
X.shape        # >> (100, 10)
xslice = X[10,:]
xslice.shape   # >&...",https://stackoverflow.com/questions/3551242/numpy-index-slice-without-losing-dimension-information,"[""I'm using numpy and want to index a row without losing the dimension information."", ""It's probably easiest to do x[None, 10, :] or equivalently (but more readable) x[np.newaxis, 10, :]."", 'Another solution is to do', 'I found a few reasonable solutions.', ""Here's an alternative I like better. Instead of indexing with a single number, index with a range. That is, use X[10:11,:]. (Note that 10:11 does not include 11)."", 'To add to the solution involving indexing by lists or arrays by gnebehay, it is also possible to use tuples:', ""This is especially annoying if you're indexing by an array that might be length 1 at runtime.  For that case, there's np.ix_:""]"
198,Counting unique values in a column in pandas dataframe like in Qlik?,"If I have a table like this:  

df = pd.DataFrame({
         'hID': [101, 102, 103, 101, 102, 104, 105, 101],
         'dID': [10, 11, 12, 10, 11, 10, 12, 10],
         'uID': ['James', 'Henry', 'Abe',...",https://stackoverflow.com/questions/45759966/counting-unique-values-in-a-column-in-pandas-dataframe-like-in-qlik,"['If I have a table like this:', 'Count distinct values, use nunique:', 'If I assume data is the name of your dataframe, you can do :', 'Or get the number of unique values for each column:', 'You can use nunique in pandas:', 'To count unique values in column, say hID of dataframe df, use:', 'you can use unique property by using len function']"
199,Argmax of numpy array returning non-flat indices,"I'm trying to get the indices of the maximum element in a Numpy array.
This can be done using numpy.argmax. My problem is, that I would like to find the biggest element in the whole array and get the ...",https://stackoverflow.com/questions/9482550/argmax-of-numpy-array-returning-non-flat-indices,"[""I'm trying to get the indices of the maximum element in a Numpy array.\nThis can be done using numpy.argmax. My problem is, that I would like to find the biggest element in the whole array and get the indices of that."", 'You could use numpy.unravel_index() on the result of numpy.argmax():', 'returns coordinates of the maximum element(s), but has to parse the array twice.', ""To get the non-flat index of all occurrences of the maximum value, you can modify eumiro's answer slightly by using argwhere instead of where:""]"
200,"FutureWarning: elementwise comparison failed; returning scalar, but in the future will perform elementwise comparison","I am using Pandas 0.19.1 on Python 3. I am getting a warning on these lines of code. I'm trying to get a list that contains all the row numbers where string Peter is present at column Unnamed: 5.

df =...",https://stackoverflow.com/questions/40659212/futurewarning-elementwise-comparison-failed-returning-scalar-but-in-the-futur,"[""I am using Pandas 0.19.1 on Python 3. I am getting a warning on these lines of code. I'm trying to get a list that contains all the row numbers where string Peter is present at column Unnamed: 5."", ""This FutureWarning isn't from Pandas, it is from numpy and the bug also affects matplotlib and others, here's how to reproduce the warning nearer to the source of the trouble:"", ""I get the same error when I try to set the index_col reading a file into a Panda's data-frame:"", 'My experience to the same warning message was caused by TypeError.', ""Can't beat Eric Leschinski's awesomely detailed answer, but here's a quick workaround to the original question that I don't think has been mentioned yet - put the string in a list and use .isin instead of =="", 'A quick workaround for this is to use numpy.core.defchararray. I also faced the same warning message and was able to resolve it using above module.', ""Eric's answer helpfully explains that the trouble comes from comparing a Pandas Series (containing a NumPy array) to a Python string.  Unfortunately, his two workarounds both just suppress the warning."", ""If your arrays aren't too big or you don't have too many of them, you might be able to get away with forcing the left hand side of == to be a string:"", 'I got this warning because I thought my column contained null strings, but on checking, it contained np.nan!', ""I've compared a few of the methods possible for doing this, including pandas, several numpy methods, and a list comprehension method."", 'I had this code which was causing the error:', 'In my case, the warning occurred because of just the regular type of boolean indexing -- because the series had only np.nan. Demonstration (pandas 1.0.3):']"
201,Numpy Resize/Rescale Image,"I would like to take an image and change the scale of the image, while it is a numpy array.

For example I have this image of a coca-cola bottle:
bottle-1

Which translates to a numpy array of shape (...",https://stackoverflow.com/questions/48121916/numpy-resize-rescale-image,"['I would like to take an image and change the scale of the image, while it is a numpy array.', 'Yeah, you can install opencv (this is a library used for image processing, and computer vision), and use the cv2.resize function. And for instance use:', 'While it might be possible to use numpy alone to do this, the operation is not built-in. That said, you can use scikit-image (which is built on numpy) to do this kind of image manipulation.', ""For people coming here from Google looking for a fast way to downsample images in numpy arrays for use in Machine Learning applications, here's a super fast method (adapted from here ). This method only works when the input dimensions are a multiple of the output dimensions."", ""If anyone came here looking for a simple method to scale/resize an image in Python, without using additional libraries, here's a very simple image resize function:"", ""SciPy's imresize() method was another resize method, but it will be removed starting with SciPy v 1.3.0 . SciPy refers to PIL image resize method: Image.resize(size, resample=0)"", 'Are there any libraries to do this in numpy/SciPy']"
202,Problems with pip install numpy - RuntimeError: Broken toolchain: cannot link a simple C program,"I'm trying to install numpy (and scipy and matplotlib) into a virturalenv.

I keep getting these errors though:

RuntimeError: Broken toolchain: cannot link a simple C program

------------------------...",https://stackoverflow.com/questions/22388519/problems-with-pip-install-numpy-runtimeerror-broken-toolchain-cannot-link-a,"[""I'm trying to install numpy (and scipy and matplotlib) into a virturalenv."", ""While it's ugly, it appears to work"", 'For Docker (Alpine) and Python 3.x this worked for me:', 'The problem is that you are unable to compile.', ""If you don't want to use sudo (so permissions and things like that are preserved when using venv), you can add the ARCHFLAGS declaration to your .bash_profile, and run as normal. This worked for me with Mavericks and Xcode 5.1 using with venv:"", 'I simply had to open XCode and accept the agreement and let it install the tools.  I then went back to PyCharm and installed numpy again with no issues.', ""If you are running a linux distribution, you may need a C compiler, especially if you see telltale log lines like sh: gcc: command not found.  You can follow the instructions here, which I've summarized below:"", 'For fedora users that are having a similar problem try installing these packeges:', 'In some cases after OS X upgrades XCode, XCode will require the user (with administrative privileges) to accept a new license. Until the license is accepted, clang and gcc will issue an error when attempting to compile and link code. Or at least python packages.', ""This means it can't find your C compiler.\nTry installing the gcc compiler if linking other compiler fails."", ""In my case this happened during a docker build. The problem was that the base image wasn't fixed to a specific python version and numpy couldn't compile with the new one."", 'The above worked for me only after installing python3-dev.', 'On Fedora 22 this was resolved by updating pip itself:\nsudo pip install --upgrade pip']"
203,figure of imshow() is too small,"I'm trying to visualize a numpy array using imshow() since it's similar to imagesc() in Matlab.

imshow(random.rand(8, 90), interpolation='nearest')
The resulting figure is very small at the center ...",https://stackoverflow.com/questions/10540929/figure-of-imshow-is-too-small,"[""I'm trying to visualize a numpy array using imshow() since it's similar to imagesc() in Matlab."", ""If you don't give an aspect argument to imshow, it will use the value for image.aspect in your matplotlibrc. The default for this value in a new matplotlibrc is equal.\nSo imshow will plot your array with equal aspect ratio."", ""That's strange, it definitely works for me:"", ""I'm new to python too. Here is something that looks like will do what you want to"", 'This works with matplotlip 3.2.1:']"
204,Unable to allocate array with shape and data type,"I'm facing an issue with allocating huge arrays in numpy on Ubuntu 18 while not facing the same issue on MacOS.

I am trying to allocate memory for a numpy array with shape (156816, 36, 53806)
with 

...",https://stackoverflow.com/questions/57507832/unable-to-allocate-array-with-shape-and-data-type,"[""I'm facing an issue with allocating huge arrays in numpy on Ubuntu 18 while not facing the same issue on MacOS."", ""This is likely due to your system's overcommit handling mode."", ""I had this same problem on Window's and came across this solution. So if someone comes across this problem in Windows the solution for me was to increase the pagefile size, as it was a Memory overcommitment problem for me too."", 'I came across this problem on Windows too. The solution for me was to switch from a 32-bit to a 64-bit version of Python. Indeed, a 32-bit software, like a 32-bit CPU, can adress a maximum of 4\xa0GB of RAM (2^32). So if you have more than 4 GB of RAM, a 32-bit version cannot take advantage of it.', 'In my case, adding a dtype attribute changed dtype of the array to a smaller type(from float64 to uint8), decreasing array size enough to not throw MemoryError in Windows(64 bit).', 'Sometimes, this error pops up because of the kernel has reached its limit. Try to restart the kernel redo the necessary steps.', 'change the data type to another one which uses less memory works. For me, I change the data type to numpy.uint8:']"
205,How do you do natural logs (e.g. “ln()”) with numpy in Python?,"Using numpy, how can I do the following:

ln(x)
Is it equivalent to:

np.log(x)
I apologise for such a seemingly trivial question, but my understanding of the difference between log and ln is that ...",https://stackoverflow.com/questions/10593100/how-do-you-do-natural-logs-e-g-ln-with-numpy-in-python,"['Using numpy, how can I do the following:', 'np.log is ln, whereas np.log10 is your standard base 10 log.', 'Correct, np.log(x) is the Natural Log (base e log) of x.', 'I usually do like this:', 'You could simple just do the reverse by making the base of log to e.']"
206,What is the difference between NaN and None?,I am reading two columns of a csv file using pandas readcsv() and then assigning the values to a dictionary. The columns contain strings of numbers and letters. Occasionally there are cases where a ...,https://stackoverflow.com/questions/17534106/what-is-the-difference-between-nan-and-none,"['I am reading two columns of a csv file using pandas readcsv() and then assigning the values to a dictionary. The columns contain strings of numbers and letters. Occasionally there are cases where a cell is empty. In my opinion, the value read to that dictionary entry should be None but instead nan is assigned. Surely None is more descriptive of an empty cell as it has a null value, whereas nan just says that the value read is not a number.', 'NaN is used as a placeholder for missing data consistently in pandas, consistency is good. I usually read/translate NaN as ""missing"". Also see the \'working with missing data\' section in the docs.', ""NaN can be used as a numerical value on mathematical operations, while None cannot (or at least shouldn't)."", 'The function isnan() checks to see if something is ""Not A Number"" and will return whether or not a variable is a number, for example isnan(2) would return false', 'Below are the differences:', 'NaN stants for NOT a number.\nNone might stand for any.']"
207,Is there an analysis speed or memory usage advantage to using HDF5 for large array storage (instead of flat binary files)?,"I am processing large 3D arrays, which I often need to slice in various ways to do a variety of data analysis. A typical ""cube"" can be ~100GB (and will likely get larger in the future)

It seems that ...",https://stackoverflow.com/questions/27710245/is-there-an-analysis-speed-or-memory-usage-advantage-to-using-hdf5-for-large-arr,"['I am processing large 3D arrays, which I often need to slice in various ways to do a variety of data analysis. A typical ""cube"" can be ~100GB (and will likely get larger in the future)', ""Some of the main advantages of HDF5 are its hierarchical structure (similar to folders/files), optional arbitrary metadata stored with each item, and its flexibility (e.g. compression).  This organizational structure and metadata storage may sound trivial, but it's very useful in practice.""]"
208,data type not understood,"I'm trying to use a matrix to compute stuff. The code is this

import numpy as np
# some code
mmatrix = np.zeros(nrows, ncols)
print mmatrix[0, 0]
but I get 'data type not understood', and it works ...",https://stackoverflow.com/questions/5446522/data-type-not-understood,"[""I'm trying to use a matrix to compute stuff. The code is this"", 'Try:']"
209,python how to pad numpy array with zeros,"I want to know how I can pad a 2D numpy array with zeros using python 2.6.6 with numpy version 1.5.0. Sorry! But these are my limitations. Therefore I cannot use np.pad. For example, I want to pad a ...",https://stackoverflow.com/questions/35751306/python-how-to-pad-numpy-array-with-zeros,"['I want to know how I can pad a 2D numpy array with zeros using python 2.6.6 with numpy version 1.5.0. Sorry! But these are my limitations. Therefore I cannot use np.pad. For example, I want to pad a with zeros such that its shape matches b. The reason why I want to do this is so I can do:', 'Very simple, you create an array containing zeros using the reference shape:', 'NumPy 1.7.0 (when numpy.pad was added) is pretty old now (it was released in 2013) so even though the question asked for a way without using that function I thought it could be useful to know how that could be achieved using numpy.pad.', 'I understand that your main problem is that you need to calculate d=b-a but your arrays have different sizes. There is no need for an intermediate padded c', 'In case you need to add a fence of 1s to an array:', ""I know I'm a bit late to this, but in case you wanted to perform relative padding (aka edge padding), here's how you can implement it. Note that the very first instance of assignment results in zero-padding, so you can use this for both zero-padding and relative padding (this is where you copy the edge values of the original array into the padded array).""]"
210,convert nan value to zero,"I have a 2D numpy array. Some of the values in this array are NaN. I want to perform certain operations using this array. For example consider the array:

[[   0.   43.   67.    0.   38.]
 [ 100.   86....",https://stackoverflow.com/questions/5124376/convert-nan-value-to-zero,"['I have a 2D numpy array. Some of the values in this array are NaN. I want to perform certain operations using this array. For example consider the array:', 'This should work:', 'Where A is your 2D array:', 'How about nan_to_num()?', 'You could use np.where to find where you have NaN:', ""A code example for drake's answer to use nan_to_num:"", 'You can use numpy.nan_to_num  :', 'nan is never equal to nan', 'You can use lambda function, an example for 1D array:', ""For your purposes, if all the items are stored as str and you just use sorted as you are using and then check for the first element and replace it with '0'""]"
211,How do I compute derivative using Numpy?,"How do I calculate the derivative of a function, for example 
  y = x2+1 
using numpy?

Let's say, I want the value of derivative at x = 5...",https://stackoverflow.com/questions/9876290/how-do-i-compute-derivative-using-numpy,"['How do I calculate the derivative of a function, for example', 'You have four options', ""The most straight-forward way I can think of is using numpy's gradient function:"", 'NumPy does not provide general functionality to compute derivatives.  It can handles the simple special case of polynomials however:', 'Assuming you want to use numpy, you can numerically compute the derivative of a function at any point using the  Rigorous definition:', ""I'll throw another method on the pile..."", 'To calculate gradients, the machine learning community uses Autograd:', 'Depending on the level of precision you require you can work it out yourself, using the simple proof of differentiation:', 'You can use scipy, which is pretty straight forward:']"
212,Using a pre-trained word embedding (word2vec or Glove) in TensorFlow,I've recently reviewed an interesting implementation for convolutional text classification. However all TensorFlow code I've reviewed uses a random (not pre-trained) embedding vectors like the ...,https://stackoverflow.com/questions/35687678/using-a-pre-trained-word-embedding-word2vec-or-glove-in-tensorflow,"[""I've recently reviewed an interesting implementation for convolutional text classification. However all TensorFlow code I've reviewed uses a random (not pre-trained) embedding vectors like the following:"", ""There are a few ways that you can use a pre-trained embedding in TensorFlow. Let's say that you have the embedding in a NumPy array called embedding, with vocab_size rows and embedding_dim columns and you want to create a tensor W that can be used in a call to tf.nn.embedding_lookup()."", 'I use this method to load and share embedding.', 'The answer of @mrry is not right because it provoques the overwriting of the embeddings weights each the network is run, so if you are following a minibatch approach to train your network, you are overwriting the weights of the embeddings. So, on my point of view the right way to pre-trained embeddings is:', '2.0 Compatible Answer: There are many Pre-Trained Embeddings, which are developed by Google and which have been Open Sourced.', 'With tensorflow version 2 its quite easy if you use the Embedding layer', 'I was also facing embedding issue, So i wrote detailed tutorial with dataset. \nHere I would like to add what I tried You can also try this method,']"
213,Selecting specific rows and columns from NumPy array,"I've been going crazy trying to figure out what stupid thing I'm doing wrong here.

I'm using NumPy, and I have specific row indices and specific column indices that I want to select from. Here's the ...",https://stackoverflow.com/questions/22927181/selecting-specific-rows-and-columns-from-numpy-array,"[""I've been going crazy trying to figure out what stupid thing I'm doing wrong here."", 'Fancy indexing requires you to provide all indices for each dimension. You are providing 3 indices for the first one, and only 2 for the second one, hence the error. You want to do something like this:', 'As Toan suggests, a simple hack would be to just select the rows first, and then select the columns over that.', 'USE:', 'Using np.ix_ is the most convenient way to do it (as answered by others), but here is another interesting way to do it:']"
214,How to remove all rows in a numpy.ndarray that contain non-numeric values,"Basically, I'm doing some data analysis.  I read in a dataset as a numpy.ndarray and some of the values are missing (either by just not being there, being NaN, or by being a string written ""NA"").

I ...",https://stackoverflow.com/questions/11453141/how-to-remove-all-rows-in-a-numpy-ndarray-that-contain-non-numeric-values,"['Basically, I\'m doing some data analysis.  I read in a dataset as a numpy.ndarray and some of the values are missing (either by just not being there, being NaN, or by being a string written ""NA"").', 'and reassign this to a.']"
215,subsampling every nth entry in a numpy array,"I am a beginner with numpy, and I am trying to extract some data from a long numpy array. What I need to do is start from a defined position in my array, and then subsample every nth data point from ...",https://stackoverflow.com/questions/25876640/subsampling-every-nth-entry-in-a-numpy-array,"['I am a beginner with numpy, and I am trying to extract some data from a long numpy array. What I need to do is start from a defined position in my array, and then subsample every nth data point from that position, until the end of my array.', ""You can use numpy's slicing, simply start:stop:step.""]"
216,Plotting a fast Fourier transform in Python,"I have access to NumPy and SciPy and want to create a simple FFT of a data set. I have two lists, one that is y values and the other is timestamps for those y values.
What is the simplest way to feed ...",https://stackoverflow.com/questions/25735153/plotting-a-fast-fourier-transform-in-python,"['I have access to NumPy and SciPy and want to create a simple FFT of a data set. I have two lists, one that is y values and the other is timestamps for those y values.', 'So I run a functionally equivalent form of your code in an IPython notebook:', 'The important thing about fft is that it can only be applied to data in which the timestamp is uniform (i.e. uniform sampling in time, like what you have shown above).', ""The high spike that you have is due to the DC (non-varying, i.e. freq = 0) portion of your signal. It's an issue of scale. If you want to see non-DC frequency content, for visualization, you may need to plot from the offset 1 not from offset 0 of the FFT of the signal."", 'Just as a complement to the answers already given, I would like to point out that often it is important to play with the size of the bins for the FFT. It would make sense to test a bunch of values and pick the one that makes more sense to your application. Often, it is in the same magnitude of the number of samples. This was as assumed by most of the answers given, and produces great and reasonable results. In case one wants to explore that, here is my code version:', ""I've built a function that deals with plotting FFT of real signals. The extra bonus in my function relative to the messages above is that you get the ACTUAL amplitude of the signal.\nAlso, because of the assumption of a real signal, the FFT is symmetric so we can plot only the positive side of the x axis:"", 'There are already great solutions on this page, but all have assumed the dataset is uniformly/evenly sampled/distributed. I will try to provide a more general example of randomly sampled data. I will also use this MATLAB tutorial as an example:', 'I write this additionnal answer to explain the origins of the diffusion of the spikes when using fft and especially discuss the scipy.fftpack tutorial with which I disagree at some point.']"
217,Replacing Numpy elements if condition is met,I have a large numpy array that I need to manipulate so that each element is changed to either a 1 or 0 if a condition is met (will be used as a pixel mask later). There are about 8 million elements ...,https://stackoverflow.com/questions/19766757/replacing-numpy-elements-if-condition-is-met,"['I have a large numpy array that I need to manipulate so that each element is changed to either a 1 or 0 if a condition is met (will be used as a pixel mask later). There are about 8 million elements in the array and my current method takes too long for the reduction pipeline:', 'You can shorten this with:', 'See, eg, Indexing with boolean arrays.', 'The quickest (and most flexible) way is to use np.where, which chooses between two arrays according to a mask(array of true and false values):', 'You can create your mask array in one step like this', 'I am not sure I understood your question, but if you write:']"
218,sort eigenvalues and associated eigenvectors after using numpy.linalg.eig in python,"I'm using numpy.linalg.eig to obtain a list of eigenvalues and eigenvectors:

A = someMatrixArray
from numpy.linalg import eig as eigenValuesAndVectors

solution = eigenValuesAndVectors(A)

...",https://stackoverflow.com/questions/8092920/sort-eigenvalues-and-associated-eigenvectors-after-using-numpy-linalg-eig-in-pyt,"[""I'm using numpy.linalg.eig to obtain a list of eigenvalues and eigenvectors:"", 'Use numpy.argsort. It returns the indices one would use to sort the array.', 'Above answer by unutbu is very crisp and concise. But, here is another way we can do it which more general and can be used for lists as well.', ""The ubuntu's piece of code doesn't work on my Python 3.6.5. It leads run-time errors. So, I refactored his/her code to this one which works ok on my test cases:""]"
219,How does python numpy.where() work?,"I am playing with numpy and digging through documentation and I have come across some magic. Namely I am talking about numpy.where():

>>> x = np.arange(9.).reshape(3, 3)
>>> np....",https://stackoverflow.com/questions/5642457/how-does-python-numpy-where-work,"['I am playing with numpy and digging through documentation and I have come across some magic. Namely I am talking about numpy.where():', 'How do they achieve internally that you are able to pass something like x > 5 into a method?', 'Old Answer\nit is kind of confusing.  It gives you the LOCATIONS (all of them) of where your statment is true.', ""np.where returns a tuple of length equal to the dimension of the numpy ndarray on which it is called (in other words ndim) and each item of tuple is a numpy ndarray of indices of all those values in the initial ndarray for which the condition is True. (Please don't confuse dimension with shape)""]"
220,Test if numpy array contains only zeros,"We initialize a numpy array with zeros as bellow:

np.zeros((N,N+1))
But how do we check whether all elements in a given n*n numpy array matrix is zero.
The method just need to return a True if all ...",https://stackoverflow.com/questions/18395725/test-if-numpy-array-contains-only-zeros,"['We initialize a numpy array with zeros as bellow:', 'Check out numpy.count_nonzero.', 'The other answers posted here will work, but the clearest and most efficient function to use is numpy.any():', ""I'd use np.all here, if you have an array a:"", 'As another answer says, you can take advantage of truthy/falsy evaluations if you know that 0 is the only falsy element possibly in your array. All elements in an array are falsy iff there are not any truthy elements in it.*', ""If you're testing for all zeros to avoid a warning on another numpy function then wrapping the line in a try, except block will save having to do the test for zeros before the operation you're interested in i.e.""]"
221,How do I calculate r-squared using Python and Numpy?,"I'm using Python and Numpy to calculate a best fit polynomial of arbitrary degree.  I pass a list of x values, y values, and the degree of the polynomial I want to fit (linear, quadratic, etc.).

This ...",https://stackoverflow.com/questions/893657/how-do-i-calculate-r-squared-using-python-and-numpy,"[""I'm using Python and Numpy to calculate a best fit polynomial of arbitrary degree.  I pass a list of x values, y values, and the degree of the polynomial I want to fit (linear, quadratic, etc.)."", ""From the numpy.polyfit documentation, it is fitting linear regression.  Specifically, numpy.polyfit with degree 'd' fits a linear regression with the mean function"", 'A very late reply, but just in case someone needs a ready function for this:', 'From yanl (yet-another-library) sklearn.metrics has an r2_score function;', 'I have been using this successfully, where x and y are array-like.', ""I originally posted the benchmarks below with the purpose of recommending numpy.corrcoef, foolishly not realizing that the original question already uses corrcoef and was in fact asking about higher order polynomial fits.  I've added an actual solution to the polynomial r-squared question using statsmodels, and I've left the original benchmarks, which while off-topic, are potentially useful to someone."", 'R-squared is a statistic that only applies to linear regression.', 'The wikipedia article on r-squareds suggests that it may be used for general model fitting rather than just linear regression.', 'Here is a function to compute  the weighted r-squared with  Python and Numpy (most of the code comes from sklearn):', ""Here's a very simple python function to compute R^2 from the actual and predicted values assuming y and y_hat are pandas series:"", 'From scipy.stats.linregress source. They use the average sum of squares method.', 'You can execute this code directly, this will find you the polynomial, and will find you the R-value you can put a comment down below if you need more explanation.']"
222,ImportError: cannot import name NUMPY_MKL,"I am trying to run the following simple code 

import scipy
scipy.test()
But I am getting the following error

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  ...",https://stackoverflow.com/questions/37267399/importerror-cannot-import-name-numpy-mkl,"['I am trying to run the following simple code', ""If you look at the line which is causing the error, you'll see this:"", 'Reinstall numpy-1.11.0_XXX.whl (for your Python) from www.lfd.uci.edu/~gohlke/pythonlibs. This file has the same name and version if compare with the variant downloaded by me earlier 29.03.2016, but its size and content differ from old variant. After re-installation error disappeared.', ""I'm not sure if this is a good solution but it removed the error. \nI commented out the line:"", 'I had the same problem while installing gensim on windows. Gensim is dependent on scipy and scipy on numpy. Making all three work is real pain. It took me a lot of time to make all there work on same time.', ""I don't have enough reputation to comment but I want to add, that the cp number of the .whl file stands for your python version."", 'The reason for the error is you upgraded your numpy library of which there are some functionalities from scipy that are required by the current version for it to run which may not be found in scipy. Just upgrade your scipy library using python -m pip install scipy --upgrade. I was facing the same error and this solution worked on my python 3.5.', 'From your log its clear that numpy package is missing. As mention in the PyPI package:', 'I recently got the same error when trying to load scipy in jupyter (python3.x, win10), although just having upgraded to numpy-1.13.3+mkl through pip. \nThe solution was to simply upgrade the scipy package (from v0.19 to v1.0.0).', 'yes,Just reinstall numpy,it works.']"
223,"scipy: savefig without frames, axes, only content","In numpy/scipy I have an image stored in an array. I can display it, I want to save it using savefig without any borders, axes, labels, titles,... Just pure image, nothing else.

I want to avoid ...",https://stackoverflow.com/questions/8218608/scipy-savefig-without-frames-axes-only-content,"['In numpy/scipy I have an image stored in an array. I can display it, I want to save it using savefig without any borders, axes, labels, titles,... Just pure image, nothing else.', 'EDIT', 'An easier solution seems to be:', 'You can find the bbox of the image inside the axis (using get_window_extent), and use the bbox_inches parameter to save only that portion of the image:', ""I've tried several options in my case, and the best solution was this:"", 'I will suggest heron13 answer with a slight addition borrowed from here to remove the padding left after setting the bbox to tight mode, therefore:', 'This one work for me', 'I had the same problem while doing some visualization using librosa where I wanted to extract content of the plot without any other information. So this my approach. unutbu answer also helps me to make to work.', 'For anybody trying to do this in Jupyter', ""While the above answers address removing margins and padding, they did not work for me in removing labels. Here's what worked, for anyone who stumbles upon this question later:"", 'I tried to get rid of the border too, using tips here but nothing really worked.\nSome fiddling about and I found that changing the faceolor gave me no border in jupyter labs (Any color resulted in getting rid of the white border). \nHope this helps.', 'Actually I have tried this recently and instead of all these lines, you can use', 'For me, this code made similar the input image size without frame and axes from matehat, unutbu, and WHZW codes:']"
224,Ambiguity in Pandas Dataframe / Numpy Array “axis” definition,"I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:

>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2],...",https://stackoverflow.com/questions/25773245/ambiguity-in-pandas-dataframe-numpy-array-axis-definition,"[""I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:"", ""It's perhaps simplest to remember it as 0=down and 1=across."", 'Another way to explain:', 'There are already proper answers, but I give you another example with > 2 dimensions.', ""It should be more widely known that the string aliases 'index' and 'columns' can be used in place of the integers 0/1. The aliases are much more explicit and help me remember how the calculations take place. Another alias for 'index' is 'rows'."", ""When axis='rows' or axis=0, it means access elements in the direction of the rows, up to down. If applying sum along axis=0, it will give us totals of each column."", ""I find all the other answers confusing. Here's how I think about it:""]"
225,Numpy: Should I use newaxis or None?,"In numpy one can use the 'newaxis' object in the slicing syntax to create an axis of length one, e.g.:

import numpy as np
print np.zeros((3,5))[:,np.newaxis,:].shape
# shape will be (3,1,5)
The ...",https://stackoverflow.com/questions/944863/numpy-should-i-use-newaxis-or-none,"[""In numpy one can use the 'newaxis' object in the slicing syntax to create an axis of length one, e.g.:"", 'None is allowed because numpy.newaxis is merely an alias for None.']"
226,Replace negative values in an numpy array,"Is there a simple way of replacing all negative values in an array with 0?
I'm having a complete block on how to do it using a NumPy array.
E.g.
a = array([1, 2, 3, -4, 5])

I need to return
[1, 2, 3, ...",https://stackoverflow.com/questions/10335090/replace-negative-values-in-an-numpy-array,"['Is there a simple way of replacing all negative values in an array with 0?', 'You are halfway there. Try:', 'Try numpy.clip:', 'Another minimalist Python solution without using numpy:', 'And yet another possibility:', ""Here's a way to do it in Python without NumPy. Create a function that returns what you want and use a list comprehension, or the map function.""]"
227,View onto a numpy array?,"I have a 2D numpy array. Is there a way to create a view onto it that would include the first k rows and all columns?

The point is to avoid copying the underlying data (the array is so large that ...",https://stackoverflow.com/questions/4370745/view-onto-a-numpy-array,"['I have a 2D numpy array. Is there a way to create a view onto it that would include the first k rows and all columns?', 'Sure, just index it as you normally would.  E.g. y = x[:k, :]  This will return a view into the original array. No data will be copied, and any updates made to y will be reflected in x and vice versa.']"
228,What is the difference between np.mean and tf.reduce_mean?,"In the MNIST beginner tutorial, there is the statement 

accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
tf.cast basically changes the type of tensor the object is, but what is the ...",https://stackoverflow.com/questions/34236252/what-is-the-difference-between-np-mean-and-tf-reduce-mean,"['In the MNIST beginner tutorial, there is the statement', 'The functionality of numpy.mean and tensorflow.reduce_mean are the same. They do the same thing. From the documentation, for numpy and tensorflow, you can see that. Lets look at an example,', 'The key here is the word reduce, a concept from functional programming, which makes it possible for reduce_mean in TensorFlow to keep a running average of the results of computations from a batch of inputs.', 'The new documentation states that tf.reduce_mean() produces the same results as np.mean:', '1 usually refers to rows, and 2 usually refers to columns. Reducing ""over"" index 1 means to reduce rowwise.']"
229,Difference between a -= b and a = a - b in Python,"I have recently applied this solution for averaging every N rows of matrix.
Although the solution works in general I had problems when applied to a 7x1 array. I have noticed that the problem is when ...",https://stackoverflow.com/questions/35036126/difference-between-a-b-and-a-a-b-in-python,"['I have recently applied this solution for averaging every N rows of matrix.\nAlthough the solution works in general I had problems when applied to a 7x1 array. I have noticed that the problem is when using the -= operator.\nTo make a small example:', 'Note: using in-place operations on NumPy arrays that share memory in no longer a problem in version 1.13.0 onward (see details here). The two operation will produce the same result. This answer only applies to earlier versions of NumPy.', 'Internally, the difference is that this:', 'The docs say :']"
230,Consistently create same random numpy array,"I am waiting for another developer to finish a piece of code that will return an np array of shape (100,2000) with values of either -1,0, or 1.

In the meantime, I want to randomly create an array of ...",https://stackoverflow.com/questions/5836335/consistently-create-same-random-numpy-array,"['I am waiting for another developer to finish a piece of code that will return an np array of shape (100,2000) with values of either -1,0, or 1.', 'Simply seed the random number generator with a fixed value, e.g.', 'Create your own instance of numpy.random.RandomState() with your chosen seed. Do not use numpy.random.seed() except to work around inflexible libraries that do not let you pass around your own RandomState instance.', ""If you are using other functions relying on a random state, you can't just set and overall seed, but should instead create a function to generate your random list of number and set the seed as a parameter of the function. This will not disturb any other random generators in the code:"", 'It is important to understand what is the seed of a random generator and when/how it is set in your code (check e.g. here for a nice explanation of the mathematical meaning of the seed).', ""I just want to clarify something in regard to @Robert Kern answer just in case that is not clear. Even if you do use the RandomState you would have to initialize it every time you call a numpy random method like in Robert's example otherwise you'll get the following results.""]"
231,How can I remove Nan from list Python/NumPy,"I have a list that countain values, one of the values I got is 'nan'

countries= [nan, 'USA', 'UK', 'France']
I tried to remove it, but I everytime get an error 

cleanedList = [x for x in countries ...",https://stackoverflow.com/questions/21011777/how-can-i-remove-nan-from-list-python-numpy,"[""I have a list that countain values, one of the values I got is 'nan'"", 'The question has changed, so to has the answer:', 'The problem comes from the fact that np.isnan() does not handle string values correctly. For example, if you do:', 'Using your example where...', 'This should remove all NaN. Of course, I assume that it is not a string here but actual NaN (np.nan).', 'use numpy fancy indexing:', 'if you check for the element type', 'I like to remove missing values from a list like this:', ""In your example 'nan' is a string so instead of using isnan() just check for the string"", 'Another way to do it would include using filter like this:', ""I noticed that Pandas for example will return 'nan' for blank values. Since it's not a string you need to convert it to one in order to match it. For example:""]"
232,NumPy selecting specific column index per row by using a list of indexes,"I'm struggling to select the specific columns per row of a NumPy matrix.

Suppose I have the following matrix which I would call X:

[1, 2, 3]
[4, 5, 6]
[7, 8, 9]
I also have a list of column indexes ...",https://stackoverflow.com/questions/23435782/numpy-selecting-specific-column-index-per-row-by-using-a-list-of-indexes,"[""I'm struggling to select the specific columns per row of a NumPy matrix."", ""If you've got a boolean array you can do direct selection based on that like so:"", 'You can do something like this:', 'A simple way might look like:', 'Recent numpy versions have added a take_along_axis (and put_along_axis) that does this indexing cleanly.', 'You can do it by using iterator. Like this:', 'Another clever way is to first transpose the array and index it thereafter. Finally, take the diagonal, its always the right answer.']"
233,Change values on matplotlib imshow() graph axis,"Say I have some input data:

data = np.random.normal(loc=100,scale=10,size=(500,1,32))
hist = np.ones((32,20)) # initialise hist
for z in range(32):
    hist[z],edges = np.histogram(data[:,0,z],bins=...",https://stackoverflow.com/questions/18696122/change-values-on-matplotlib-imshow-graph-axis,"['Say I have some input data:', 'I would try to avoid changing the xticklabels if possible, otherwise it can get very confusing if you for example overplot your histogram with additional data.', 'I had a similar problem and google was sending me to this post. My solution was a bit different and less compact, but hopefully this can be useful to someone.']"
234,Convert 2d numpy array into list of lists [duplicate],"I use an external module (libsvm), which does not support numpy arrays, only tuples, lists and dicts. But my data is in a 2d numpy array. How can I convert it the pythonic way, aka without loops. 

&...",https://stackoverflow.com/questions/9721884/convert-2d-numpy-array-into-list-of-lists,"['I use an external module (libsvm), which does not support numpy arrays, only tuples, lists and dicts. But my data is in a 2d numpy array. How can I convert it the pythonic way, aka without loops.', 'You can simply cast the matrix to list with matrix.tolist(), proof:']"
235,Is there special significance to 16331239353195370.0?,"Using import numpy as np I've noticed that 

np.tan(np.pi/2)
gives the number in the title and not np.inf

16331239353195370.0
I'm curious about this number. Is it related to some system machine ...",https://stackoverflow.com/questions/38295501/is-there-special-significance-to-16331239353195370-0,"[""Using import numpy as np I've noticed that"", ""pi isn't exactly representable as Python float (same as the platform C's double type).  The closest representable approximation is used.""]"
236,how to split column of tuples in pandas dataframe?,"I have a pandas dataframe (this is only a little piece)

>>> d1
   y norm test  y norm train  len(y_train)  len(y_test)  \
0    64.904368    116.151232          1645          549   
1    70....",https://stackoverflow.com/questions/29550414/how-to-split-column-of-tuples-in-pandas-dataframe,"['I have a pandas dataframe (this is only a little piece)', 'You can do this by doing pd.DataFrame(col.tolist()) on that column:', ""On much larger datasets, I found that .apply() is few orders slower than pd.DataFrame(df['b'].values.tolist(), index=df.index)"", 'The str accessor that is available to pandas.Series objects of dtype == object is actually an iterable.', 'I think a simpler way is:', 'I know this is from a while ago, but a caveat of the second solution:']"
237,deleting rows in numpy array,"I have an array that might look like this:

ANOVAInputMatrixValuesArray = [[ 0.96488889, 0.73641667, 0.67521429, 0.592875, 
0.53172222], [ 0.78008333, 0.5938125, 0.481, 0.39883333, 0.]]
Notice that ...",https://stackoverflow.com/questions/3877491/deleting-rows-in-numpy-array,"['I have an array that might look like this:', 'The simplest way to delete rows and columns from arrays is the numpy.delete method.', ""Here's a one liner (yes, it is similar to user333700's, but a little more straightforward):"", ""This is similar to your original approach, and will use less space than unutbu's answer, but I suspect it will be slower."", ""numpy provides a simple function to do the exact same thing:\nsupposing you have a masked array 'a', calling numpy.ma.compress_rows(a) will delete the rows containing a masked value.\nI guess this is much faster this way..."", ""I might be too late to answer this question, but wanted to share my input for the benefit of the community. For this example, let me call your matrix 'ANOVA', and I am assuming you're just trying to remove rows from this matrix with 0's only in the 5th column.""]"
238,float64 with pandas to_csv,"I'm reading a CSV with float numbers like this:

Bob,0.085
Alice,0.005
And import into a dataframe, and write this dataframe to a new place

df = pd.read_csv(orig)
df.to_csv(pandasfile)
Now this ...",https://stackoverflow.com/questions/12877189/float64-with-pandas-to-csv,"[""I'm reading a CSV with float numbers like this:"", 'As mentioned in the comments, it is a general floating point problem.', 'UPDATE: Answer was accurate at time of writing, and floating point precision is still not something you get by default with to_csv/read_csv (precision-performance tradeoff; defaults favor performance).']"
239,"Share Large, Read-Only Numpy Array Between Multiprocessing Processes",I have a 60GB SciPy Array (Matrix) I must share between 5+ multiprocessing Process objects. I've seen numpy-sharedmem and read this discussion on the SciPy list. There seem to be two approaches--numpy-...,https://stackoverflow.com/questions/17785275/share-large-read-only-numpy-array-between-multiprocessing-processes,"[""I have a 60GB SciPy Array (Matrix) I must share between 5+ multiprocessing Process objects. I've seen numpy-sharedmem and read this discussion on the SciPy list. There seem to be two approaches--numpy-sharedmem and using a multiprocessing.RawArray() and mapping NumPy dtypes to ctypes. Now, numpy-sharedmem seems to be the way to go, but I've yet to see a good reference example. I don't need any kind of locks, since the array (actually a matrix) will be read-only. Now, due to its size, I'd like to avoid a copy. It sounds like the correct method is to create the only copy of the array as a sharedmem array, and then pass it to the Process objects? A couple of specific questions:"", '@Velimir Mlaker gave a great answer. I thought I could add some bits of comments and a tiny example.', 'If you are on Linux (or any POSIX-compliant system), you can define this array as a global variable. multiprocessing is using fork() on Linux when it starts a new child process. A newly spawned child process automatically shares the memory with its parent as long as it does not change it (copy-on-write mechanism).', 'You may be interested in a tiny piece of code I wrote: github.com/vmlaker/benchmark-sharedmem', 'If your array is that big you can use numpy.memmap. For example, if you have an array stored in disk, say \'test.array\', you can use simultaneous processes to access the data in it even in ""writing"" mode, but your case is simpler since you only need ""reading"" mode.', 'You may also find it useful to take a look at the documentation for pyro as if you can partition your task appropriately you could use it to execute different sections on different machines as well as on different cores in the same machine.', 'Why not use multithreading?\nResources of main process can be shared by its threads natively, thus multithreading is obviously a better way to share objects owned by the main process.']"
240,How to normalize a 2-dimensional numpy array in python less verbose?,"Given a 3 times 3 numpy array

a = numpy.arange(0,27,3).reshape(3,3)

# array([[ 0,  3,  6],
#        [ 9, 12, 15],
#        [18, 21, 24]])
To normalize the rows of the 2-dimensional array I thought ...",https://stackoverflow.com/questions/8904694/how-to-normalize-a-2-dimensional-numpy-array-in-python-less-verbose,"['Given a 3 times 3 numpy array', 'Broadcasting is really good for this:', 'Scikit-learn has a normalize function that lets you apply various normalizations. The ""make it sum to 1"" is the L1 norm, and to take that do:', 'I think this should work,', ""In case you are trying to normalize each row such that its magnitude is one (i.e. a row's unit length is one or the sum of the square of each element in a row is one):"", 'I think you can normalize the row elements sum to 1 by this:\n    new_matrix = a / a.sum(axis=1, keepdims=1).\nAnd the column normalization can be done with new_matrix = a / a.sum(axis=0, keepdims=1). Hope this can hep.', 'You could use built-in numpy function: \nnp.linalg.norm(a, axis = 1, keepdims = True)', 'it appears that this also works', 'You could also use matrix transposition:', 'Or using lambda function, like', 'where input_data is the name of your 2D array']"
241,Linear regression with matplotlib / numpy,"I'm trying to generate a linear regression on a scatter plot I have generated, however my data is in list format, and all of the examples I can find of using polyfit require using arange. arange doesn'...",https://stackoverflow.com/questions/6148207/linear-regression-with-matplotlib-numpy,"[""I'm trying to generate a linear regression on a scatter plot I have generated, however my data is in list format, and all of the examples I can find of using polyfit require using arange. arange doesn't accept lists though. I have searched high and low about how to convert a list to an array and nothing seems clear. Am I missing something?"", ""arange generates lists (well, numpy arrays); type help(np.arange) for the details.  You don't need to call it on existing lists."", 'This code:', 'USe this ..', 'Another quick and dirty answer is that you can just convert your list to an array using:']"
242,How to determine whether a column/variable is numeric or not in Pandas/NumPy?,"Is there a better way to determine whether a variable in Pandas and/or NumPy is numeric or not ? 

I have a self defined dictionary with dtypes as keys and numeric / not as values.",https://stackoverflow.com/questions/19900202/how-to-determine-whether-a-column-variable-is-numeric-or-not-in-pandas-numpy,"['Is there a better way to determine whether a variable in Pandas and/or NumPy is numeric or not ?', 'In pandas 0.20.2 you can do:', 'You can use np.issubdtype to check if the dtype is a sub dtype of np.number. Examples:', ""Based on @jaime's answer in the comments, you need to check .dtype.kind for the column of interest. For example;"", 'Pandas has select_dtype function. You can easily filter your columns on int64, and float64 like this:', 'This is a pseudo-internal method to return only the numeric type data', ""How about just checking type for one of the values in the column? We've always had something like this:"", 'Just to add to all other answers, one can also use df.info() to get whats the data type of each column.', 'You can check whether a given column contains numeric values or not using dtypes', 'You can also try:']"
243,Why does numpy std() give a different result to matlab std()?,"I try to convert matlab code to numpy and figured out that numpy has a different result with the std function.

in matlab

std([1,3,4,6])
ans =  2.0817
in numpy

np.std([1,3,4,6])
1.8027756377319946
...",https://stackoverflow.com/questions/27600207/why-does-numpy-std-give-a-different-result-to-matlab-std,"['I try to convert matlab code to numpy and figured out that numpy has a different result with the std function.', 'The NumPy function np.std takes an optional parameter ddof: ""Delta Degrees of Freedom"". By default, this is 0. Set it to 1 to get the MATLAB result:', 'The standard deviation is the square root of the variance. The variance of a random variable X is defined as', ""For people who aren't great with statistics, a simplistic guide is:""]"
244,Numpy `logical_or` for more than two arguments,Numpy's logical_or function takes no more than two arrays to compare.  How can I find the union of more than two arrays?  (The same question could be asked with regard to Numpy's logical_and and ...,https://stackoverflow.com/questions/20528328/numpy-logical-or-for-more-than-two-arguments,"[""Numpy's logical_or function takes no more than two arrays to compare.  How can I find the union of more than two arrays?  (The same question could be asked with regard to Numpy's logical_and and obtaining the intersection of more than two arrays.)"", ""If you're asking about numpy.logical_or, then no, as the docs explicitly say, the only parameters are x1, x2, and optionally out:"", 'In case someone still need this - Say you have three Boolean arrays a, b, c with the same shape, this gives and element-wise:', 'As boolean algebras are both commutative and associative by definition, the following statements or equivalent for boolean values of a, b and c.', ""Building on abarnert's answer for n-dimensional case:"", 'using the sum function:', 'I use this workaround which can be extended to n arrays:', ""I've tried the following three different methods to get the logical_and of a list l of k arrays of size n:""]"
245,What is the difference between size and count in pandas?,"That is the difference between groupby(""x"").count and groupby(""x"").size in pandas ?

Does size just exclude nil ?",https://stackoverflow.com/questions/33346591/what-is-the-difference-between-size-and-count-in-pandas,"['That is the difference between groupby(""x"").count and groupby(""x"").size in pandas ?', 'size includes NaN values, count does not:', 'The other answers have pointed out the difference, however, it is not completely accurate to say ""size counts NaNs while count does not"". While size does indeed count NaNs, this is actually a consequence of the fact that size returns the size (or the length) of the object it is called on. Naturally, this also includes rows/values which are NaN.', ""Just to add a little bit to @Edchum's answer, even if the data has no NA values, the result of count() is more verbose, using the example before:"", 'When we are dealing with normal dataframes then only difference will be an inclusion of NAN values, means count does not include NAN values while counting rows.', 'In addition to all above answers, I would like to point out one more diffrence which I seem significant.']"
246,Non-repetitive random number in numpy,"How can I generate non-repetitive random numbers in numpy?

list = np.random.random_integers(20,size=(10))",https://stackoverflow.com/questions/8505651/non-repetitive-random-number-in-numpy,"['How can I generate non-repetitive random numbers in numpy?', 'numpy.random.Generator.choice offers a replace argument to sample without replacement:', ""I think numpy.random.sample doesn't work right, now. This is my way:"", 'Years later, some timeits for choosing 40000 out of 10000^2\n(Numpy 1.8.1, imac 2.7 GHz):', 'You can get this by sorting as well:', ""Simply generate an array that contains the required range of numbers, then shuffle them by repeatedly swapping a random one with the 0th element in the array. This produces a random sequence that doesn't contain duplicate values.""]"
247,Multiplying across in a numpy array,"I'm trying to multiply each of the terms in a 2D array by the corresponding terms in a 1D array. This is very easy if I want to multiply every column by the 1D array, as shown in the numpy.multiply ...",https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array,"[""I'm trying to multiply each of the terms in a 2D array by the corresponding terms in a 1D array. This is very easy if I want to multiply every column by the 1D array, as shown in the numpy.multiply function. But I want to do the opposite, multiply each term in the row.\nIn other words I want to multiply:"", 'Normal multiplication like you showed:', ""I've compared the different options for speed and found that – much to my surprise – all options (except diag) are equally fast. I personally use"", 'You could also use matrix multiplication (aka dot product):', 'Yet another trick (as of v1.6)', 'For those lost souls on google, using numpy.expand_dims then numpy.repeat will work, and will also work in higher dimensional cases (i.e. multiplying a shape (10, 12, 3) by a (10, 12)).', ""Why don't you just do""]"
248,Counting the number of non-NaN elements in a numpy ndarray in Python,"I need to calculate the number of non-NaN elements in a numpy ndarray matrix. How would one efficiently do this in Python? Here is my simple code for achieving this: 

import numpy as np

def ...",https://stackoverflow.com/questions/21778118/counting-the-number-of-non-nan-elements-in-a-numpy-ndarray-in-python,"['I need to calculate the number of non-NaN elements in a numpy ndarray matrix. How would one efficiently do this in Python? Here is my simple code for achieving this:', '~ inverts the boolean matrix returned from np.isnan.', 'Even though is not the fastest choice, if performance is not an issue you can use:', 'To determine if the array is sparse, it may help to get a proportion of nan values', 'An alternative, but a bit slower alternative is to do it over indexing.']"
249,Generate random array of floats between a range,"I haven't been able to find a function to generate an array of random floats of a given length between a certain range.

I've looked at Random sampling but no function seems to do what I need.

random....",https://stackoverflow.com/questions/22071987/generate-random-array-of-floats-between-a-range,"[""I haven't been able to find a function to generate an array of random floats of a given length between a certain range."", 'np.random.uniform fits your use case:', 'Why not use a list comprehension?', 'Why not to combine random.uniform with a list comprehension?', ""There may already be a function to do what you're looking for, but I don't know about it (yet?).\nIn the meantime, I would suggess using:"", 'The for loop in list comprehension takes time and makes it slow.\nIt is better to use numpy parameters (low, high, size, ..etc)', 'Alternatively you could use SciPy', 'This is the simplest way', 'np.random.random_sample(size) will generate random floats in the half-open interval [0.0, 1.0).']"
250,How to copy data from a numpy array to another,"What is the fastest way to copy data from array b to array a, without modifying the address of array a. I need this because an external library (PyFFTW) uses a pointer to my array that cannot change.

...",https://stackoverflow.com/questions/6431973/how-to-copy-data-from-a-numpy-array-to-another,"['What is the fastest way to copy data from array b to array a, without modifying the address of array a. I need this because an external library (PyFFTW) uses a pointer to my array that cannot change.', 'I believe', 'NumPy version 1.7 has the numpy.copyto function that does what you are looking for:', ""is even faster than the suggested solutions up to numpy v1.6 and makes a copy of the array as well. I could however not test it against copyto(a,b), since I don't have the most recent version of numpy."", 'To answer your question, I played with some variants and profiled them.', 'you can easy use:', 'There are many different things you can do:', 'Why not to use']"
251,"ValueError: numpy.dtype has the wrong size, try recompiling","I just installed pandas and statsmodels package on my python 2.7
When I tried  ""import pandas as pd"", this error message comes out.
Can anyone help? Thanks!!!

numpy.dtype has the wrong size, try ...",https://stackoverflow.com/questions/17709641/valueerror-numpy-dtype-has-the-wrong-size-try-recompiling,"['I just installed pandas and statsmodels package on my python 2.7\nWhen I tried  ""import pandas as pd"", this error message comes out.\nCan anyone help? Thanks!!!', '(to expand a bit on my comment)', 'For me (Mac OS X Maverics, Python 2.7)', 'I found it to be a simple version being outdated or mismatch and was fixed with:', 'I had a similar error with another library and realized that I had several versions of numpy installed on my system. The fix for me was to edit my PYTHONPATH and put the site-packages that contained the latest version of numpy in first position.', 'As in here, for me only sudo pip install pandas==0.13.1 worked', 'I also encounter this error when use pandas to access MYSQL.\nThis error message indicates a binary compatible issue and can be resolved by \nusing latest version of pandas and numpy package. \nHere is my steps to resolve this issue, and it works well on my Ubuntu 12.04:', 'In my case, I had installed pandas-0.10.0.win-amd64-py2.7 but was checking to see if a bug had been fixed in a more recent version of pandas.  So I did an easy_install -U to force the upgrade, but then got the above error due to some incompatibilities with numpy etc... when I did', 'For me (Mac OS X Mavericks) it worked to install the version for python2.6:', 'The problem I solved on Webfaction was old numpy library(1.5) which was in conflict with my fresh', ""I just meet this 'ValueError' issue and have addressed it. Definitely there's something wrong with numpy package."", ""Like @user333700 said, required versions of libraries may not meet for each other. You get one library as another's dependency. Then without knowing it was already installed as dependency, you need that specific library and you install one version. With such ways dependencies may mess up."", ""I had a similar issue, and simply re-installing using pip install ... as suggested in previous comments didn't work."", 'There are cases where you want to keep a specific NumPy version and the upgrade option mentioned here will not work. \nAn example that occurred to me was the Python distribution preinstalled with ArcGIS. For ArcPy to work in ArcGIS 10.5.1, that distribution needs to be Python 2.7.12 with NumPy 1.9.3 and any other version of NumPy is probably going to cause issues with your ArcPy functionality.']"
252,What does “three dots” in Python mean when indexing what looks like a number?,"What is the meaning of x[...] below?

a = np.arange(6).reshape(2,3)
for x in np.nditer(a, op_flags=['readwrite']):
    x[...] = 2 * x",https://stackoverflow.com/questions/42190783/what-does-three-dots-in-python-mean-when-indexing-what-looks-like-a-number,"['What is the meaning of x[...] below?', 'While the proposed duplicate What does the Python Ellipsis object do? answers the question in a general python context, its use in an nditer loop requires, I think, added information.']"
253,How can I plot a histogram such that the heights of the bars sum to 1 in matplotlib?,"I'd like to plot a normalized histogram from a vector using matplotlib.  I tried the following:

plt.hist(myarray, normed=True)
as well as:

plt.hist(myarray, normed=1)
but neither option produces a ...",https://stackoverflow.com/questions/3866520/how-can-i-plot-a-histogram-such-that-the-heights-of-the-bars-sum-to-1-in-matplot,"[""I'd like to plot a normalized histogram from a vector using matplotlib.  I tried the following:"", 'It would be more helpful if you posed a more complete working (or in this case non-working) example.', 'If you want the sum of all bars to be equal unity, weight each bin by the total number of values:', 'I know this answer is too late considering the question is dated 2010 but I came across this question as I was facing a similar problem myself. As already stated in the answer, normed=True means that the total area under the histogram is equal to 1 but the sum of heights is not equal to 1. However, I wanted to, for convenience of physical interpretation of a histogram, make one with sum of heights equal to 1.', 'Here is another simple solution using np.histogram() method.']"
254,Numpy: find index of the elements within range,"I have a numpy array of numbers, for example,  

a = np.array([1, 3, 5, 6, 9, 10, 14, 15, 56])  
I would like to find all the indexes of the elements within a specific range. For instance, if the ...",https://stackoverflow.com/questions/13869173/numpy-find-index-of-the-elements-within-range,"['I have a numpy array of numbers, for example,', 'You can use np.where to get indices and np.logical_and to set two conditions:', ""As in @deinonychusaur's reply, but even more compact:"", 'I thought I would add this because the a in the example you gave is sorted:', 'For understanding what is the best answer we can do some timing using the different solution.\nUnfortunately, the question was not well-posed so there are answers to different questions, here I try to point the answer to the same question.  Given the array:', 'This code snippet returns all the numbers in a numpy array between two values:', 'Wanted to add numexpr into the mix:', 'Other way is with:', 'Output:', 'This may not be the prettiest, but works for any dimension', 'You can use np.clip() to achieve the same:']"
255,How can I add new dimensions to a Numpy array?,"I'm starting off with a numpy array of an image.

In[1]:img = cv2.imread('test.jpg')
The shape is what you might expect for a 640x480 RGB image.

In[2]:img.shape
Out[2]: (480, 640, 3)
However, this ...",https://stackoverflow.com/questions/17394882/how-can-i-add-new-dimensions-to-a-numpy-array,"[""I'm starting off with a numpy array of an image."", ""You're asking how to add a dimension to a NumPy array, so that that dimension can then be grown to accommodate new data.  A dimension can be added as follows:"", 'Alternatively to', 'You could just create an array of the correct size up-front and fill it:', 'Pythonic', 'You can use np.concatenate() specifying which axis to append, using np.newaxis:', 'There is no structure in numpy that allows you to append more data later.', 'Consider Approach 1 with reshape method and Approach 2 with np.newaxis method that produce the same outcome:', 'I followed this approach:']"
256,What does x[x < 2] = 0 mean in Python?,"I came across some code with a line similar to

x[x<2]=0
Playing around with variations, I am still stuck on what this syntax does.

Examples:

>>> x = [1,2,3,4,5]
>>> x[x<2]
...",https://stackoverflow.com/questions/36603042/what-does-xx-2-0-mean-in-python,"['I came across some code with a line similar to', 'This only makes sense with NumPy arrays. The behavior with lists is useless, and specific to Python 2 (not Python 3). You may want to double-check if the original object was indeed a NumPy array (see further below) and not a list.', 'The bool is simply converted to an integer. The index is either 0 or 1.', 'The original code in your question works only in Python 2. If x is a list in Python 2, the comparison x < y is False if y is an integer. This is because it does not make sense to compare a list with an integer. However in Python 2, if the operands are not comparable, the comparison is based in CPython on the alphabetical ordering of the names of the types; additionally all numbers come first in mixed-type comparisons. This is not even spelled out in the documentation of CPython 2, and different Python 2 implementations could give different results. That is [1, 2, 3, 4, 5] < 2 evaluates to False because 2 is a number and thus ""smaller"" than a list in CPython. This mixed comparison was eventually deemed to be too obscure a feature, and was removed in Python 3.0.', 'This has one more use: code golf. Code golf is the art of writing programs that solve some problem in as few source code bytes as possible.', 'In general it could mean anything. It was already explained what it means if x is a list or numpy.ndarray but in general it only depends on how the comparison operators (<, >, ...) and also how the get/set-item ([...]-syntax) are implemented.']"
257,extracting days from a numpy.timedelta64 value,"I am using pandas/python and I have two date time series s1 and s2, that have been generated using the 'to_datetime' function on a field of the df containing dates/times.

When I subtract s1 from s2
 ...",https://stackoverflow.com/questions/18215317/extracting-days-from-a-numpy-timedelta64-value,"[""I am using pandas/python and I have two date time series s1 and s2, that have been generated using the 'to_datetime' function on a field of the df containing dates/times."", 'You can convert it to a timedelta with a day precision. To extract the integer value of days you divide it with a timedelta of one day.', 'Use dt.days to obtain the days attribute as integers.', 'Suppose you have a timedelta series:']"
258,String concatenation of two pandas columns,"I have a following DataFrame:

from pandas import *
df = DataFrame({'foo':['a','b','c'], 'bar':[1, 2, 3]})
It looks like this:

    bar foo
0    1   a
1    2   b
2    3   c
Now I want to have ...",https://stackoverflow.com/questions/11858472/string-concatenation-of-two-pandas-columns,"['I have a following DataFrame:', 'df[\'bar\'] = df.bar.map(str) + "" is "" + df.foo.', 'This question has already been answered, but I believe it would be good to throw some useful methods not previously discussed into the mix, and compare all methods proposed thus far in terms of performance.', ""The problem in your code is that you want to apply the operation on every row. The way you've written it though takes the whole 'bar' and 'foo' columns, converts them to strings and gives you back one big string. You can write it like:"", 'You could also use', '@DanielVelkov answer is the proper one BUT\nusing string literals is faster:', 'series.str.cat is the most flexible way to approach this problem:']"
259,"Working with TIFFs (import, export) in Python using numpy",I need a python method to open and import TIFF images into numpy arrays so I can analyze and modify the pixel data and then save them as TIFFs again. (They are basically light intensity maps in ...,https://stackoverflow.com/questions/7569553/working-with-tiffs-import-export-in-python-using-numpy,"['I need a python method to open and import TIFF images into numpy arrays so I can analyze and modify the pixel data and then save them as TIFFs again. (They are basically light intensity maps in greyscale, representing the respective values per pixel)', 'First, I downloaded a test TIFF image from this page called a_image.tif. Then I opened with PIL like this:', 'I use matplotlib for reading TIFF files:', 'You could also use GDAL to do this.  I realize that it is a geospatial toolkit, but nothing requires you to have a cartographic product.', ""pylibtiff worked better for me than PIL, which as of June 2020 doesn't support color images with more than 8 bits per color."", ""You can also use pytiff of which I'm the author."", 'In case of image stacks, I find it easier to use scikit-image to read, and matplotlib to show or save. I have handled 16-bit TIFF image stacks with the following code.', ""I recommend using the python bindings to OpenImageIO, it's the standard for dealing with various image formats in the vfx world. I've ovten found it more reliable in reading various compression types compared to PIL.""]"
260,Most efficient way to find mode in numpy array,"I have a 2D array containing integers (both positive or negative). Each row represents the values over time for a particular spatial site, whereas each column represents values for various spatial ...",https://stackoverflow.com/questions/16330831/most-efficient-way-to-find-mode-in-numpy-array,"['I have a 2D array containing integers (both positive or negative). Each row represents the values over time for a particular spatial site, whereas each column represents values for various spatial sites for a given time.', ""Check scipy.stats.mode() (inspired by @tom10's comment):"", 'Update', 'Expanding on this method, applied to finding the mode of the data where you may need the index of the actual array to see how far away the value is from the center of the distribution.', 'A neat solution that only uses numpy (not scipy nor the Counter class):', 'If you want to use numpy only:', 'I think a very simple way would be to use the Counter class. You can then use the most_common() function of the Counter instance as mentioned here.', 'The Counter(data) counts the frequency and returns a defaultdict. sorted(Counter(data).items()) sorts using the keys, not the frequency. Finally, need to sorted the frequency using another sorted with key = lambda x: x[1]. The reverse tells Python to sort the frequency from the largest to the smallest.', 'simplest way in Python to get the mode of an list or array a']"
261,Interweaving two numpy arrays,"Assume the following arrays are given:

a = array([1,3,5])
b = array([2,4,6])
How would one interweave them efficiently so that one gets a third array like this

c = array([1,2,3,4,5,6])
It can be ...",https://stackoverflow.com/questions/5347065/interweaving-two-numpy-arrays,"['Assume the following arrays are given:', ""I like Josh's answer.  I just wanted to add a more mundane, usual, and slightly more verbose solution.  I don't know which is more efficient.  I expect they will have similar performance."", 'I thought it might be worthwhile to check how the solutions performed in terms of performance. And this is the result:', 'Here is a one-liner:', 'Here is a simpler answer than some of the previous ones', 'This will interleave/interlace the two arrays and I believe it is quite readable:', ""Maybe this is more readable than @JoshAdel's solution:"", ""Improving @xioxox's answer:"", 'vstack sure is an option, but more straightforward solution for your case could be the hstack', 'Another one-liner: np.vstack((a,b)).T.ravel()\nOne more: np.stack((a,b),1).ravel()', 'One can also try np.insert. (Solution migrated from Interleave numpy arrays)', ""I needed to do this but with multidimensional arrays along any axis. Here's a quick general purpose function to that effect. It has the same call signature as np.concatenate, except that all input arrays must have exactly the same shape.""]"
262,Shift elements in a numpy array,"Following-up from this question years ago, is there a canonical ""shift"" function in numpy? I don't see anything from the documentation.

Here's a simple version of what I'm looking for:

def shift(xs, ...",https://stackoverflow.com/questions/30399534/shift-elements-in-a-numpy-array,"['Following-up from this question years ago, is there a canonical ""shift"" function in numpy? I don\'t see anything from the documentation.', 'Not numpy but scipy provides exactly the shift functionality you want,', 'For those who want to just copy and paste the fastest implementation of shift, there is a benchmark and conclusion(see the end). In addition, I introduce fill_value parameter and fix some bugs.', 'There is no single function that does what you want. Your definition of shift is slightly different than what most people are doing. The ways to shift an array are more commonly looped:', 'You can convert ndarray to Series or DataFrame with pandas first, then you can use shift method as you want.', '', 'You can also do this with Pandas:', ""If you want a one-liner from numpy and aren't too concerned about performance, try:"", 'One way to do it without spilt the code into cases']"
263,how to convert 2d list to 2d numpy array?,"I have a 2D list something like 

a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] 
and I want to convert it to a 2d numpy array. Can we do it without allocating memory like 

numpy.zeros((3,3))
and then ...",https://stackoverflow.com/questions/7717380/how-to-convert-2d-list-to-2d-numpy-array,"['I have a 2D list something like', 'Just pass the list to np.array:', 'I am using large data sets exported to a python file in the form', 'just use following code', 'np.array() is even more powerful than what unutbu said above.\nYou also could use it to convert a list of np arrays to a higher dimention array, the following is a simple example:']"
264,How can numpy be so much faster than my Fortran routine?,"I get a 512^3 array representing a Temperature distribution from a simulation (written in Fortran). The array is stored in a binary file that's about 1/2G in size. I need to know the minimum, maximum ...",https://stackoverflow.com/questions/33723771/how-can-numpy-be-so-much-faster-than-my-fortran-routine,"[""I get a 512^3 array representing a Temperature distribution from a simulation (written in Fortran). The array is stored in a binary file that's about 1/2G in size. I need to know the minimum, maximum and mean of this array and as I will soon need to understand Fortran code anyway, I decided to give it a go and came up with the following very easy routine."", 'Your Fortran implementation suffers two major shortcomings:', 'The numpy is faster because you wrote much more efficient code in python (and much of the numpy backend is written in optimized Fortran and C) and terribly inefficient code in Fortran.']"
265,In Python NumPy what is a dimension and axis?,"I am coding with Pythons NumPy module.  If coordinates of a point in 3D space are described as [1, 2, 1], wouldn't that be three dimensions, three axis, a rank of three?  Or if that is one dimension ...",https://stackoverflow.com/questions/19389910/in-python-numpy-what-is-a-dimension-and-axis,"[""I am coding with Pythons NumPy module.  If coordinates of a point in 3D space are described as [1, 2, 1], wouldn't that be three dimensions, three axis, a rank of three?  Or if that is one dimension then shouldn't it be points (plural), not point?"", 'In numpy arrays, dimensionality refers to the number of axes needed to index it, not the dimensionality of any geometrical space.  For example, you can describe the locations of points in 3D space with a 2D array:', 'If someone need this visual description:', 'It is of rank one, as you need one index to index it. That one axis has the length 3, as the index indexing it can take three different values: v[i], i=0..2.', 'Just paste part of answer from this answer:', 'You can also use axis parameter in group operations, in case of axis=0 Numpy performs the action on elements of each column, and if axis=1, it performs the action on rows.', 'This is how I understand it. \nA point is a 1D object. You can only define its position. It has no dimensions. \nA line or surface is a 2D object. You can define it by both its position and length or area respectively e.g. Rectangle, Square, Circle\nA volume is a 3D object. You can define it by its position, surface area/lengths and volume e.g. Sphere, Cube.']"
266,What exactly does numpy.exp() do? [closed],"I'm very confused as to what np.exp() actually does. In the documentation it says that it: ""Calculates the exponential of all elements in the input array."" I'm confused as to what exactly this means. ...",https://stackoverflow.com/questions/31951980/what-exactly-does-numpy-exp-do,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", ""The exponential function is e^x where e is a mathematical constant called Euler's number, approximately 2.718281. This value has a close mathematical relationship with pi and the slope of the curve e^x is equal to its value at every point. np.exp() calculates e^x for each value of x in your input array."", ""It calculates ex for each x in your list where e is Euler's number (approximately 2.718). In other words, np.exp(range(5)) is similar to [math.e**x for x in range(5)]."", 'exp(x) = e^x where e=  2.718281(approx)']"
267,How to transform numpy.matrix or array to scipy sparse matrix,"For SciPy sparse matrix, one can use todense() or toarray() to transform to NumPy matrix or array. What are the functions to do the inverse?

I searched, but got no idea what keywords should be the ...",https://stackoverflow.com/questions/7922487/how-to-transform-numpy-matrix-or-array-to-scipy-sparse-matrix,"['For SciPy sparse matrix, one can use todense() or toarray() to transform to NumPy matrix or array. What are the functions to do the inverse?', 'You can pass a numpy array or matrix as an argument when initializing a sparse matrix. For a CSR matrix, for example, you can do the following.', 'There are several sparse matrix classes in scipy.', ""As for the inverse, the function is inv(A), but I won't recommend using it, since for huge matrices it is very computationally costly and unstable. Instead, you should use an approximation to the inverse, or if you want to solve Ax = b you don't really need A-1.""]"
268,Why does corrcoef return a matrix?,"It seems strange to me that np.corrcoef returns a matrix.

 correlation1 = corrcoef(Strategy1Returns,Strategy2Returns)

[[ 1.         -0.99598935]
 [-0.99598935  1.        ]]
Does anyone know why ...",https://stackoverflow.com/questions/3425439/why-does-corrcoef-return-a-matrix,"['It seems strange to me that np.corrcoef returns a matrix.', 'It allows you to compute correlation coefficients of >2 data sets, e.g.', 'corrcoef returns the normalised covariance matrix.', 'The correlation matrix is the standard way to express correlations between an arbitrary finite number of variables. The correlation matrix of N data vectors is a symmetric N × N matrix with unity diagonal. Only in the case N = 2 does this matrix have one free parameter.', 'You can use the following function to return only the correlation coefficient:', 'Consider using matplotlib.cbook pieces', 'The function Correlate of numpy works with 2 1D arrays that you want to correlate and returns one correlation value.']"
269,Get the position of the largest value in a multi-dimensional NumPy array,How can I get get the position (indices) of the largest value in a multi-dimensional NumPy array?,https://stackoverflow.com/questions/3584243/get-the-position-of-the-largest-value-in-a-multi-dimensional-numpy-array,"['How can I get get the position (indices) of the largest value in a multi-dimensional NumPy array?', 'The argmax() method should help.', '(edit) I was referring to an old answer which had been deleted. And the accepted answer came after mine. I agree that argmax is better than my answer.', 'You can simply write a function (that works only in 2d):', 'An alternative way is change numpy array to list and use max and index methods:']"
270,Two-sample Kolmogorov-Smirnov Test in Python Scipy,"I can't figure out how to do a Two-sample KS test in Scipy.

After reading the documentation scipy kstest

I can see how to test where a distribution is identical to standard normal distribution

from ...",https://stackoverflow.com/questions/10884668/two-sample-kolmogorov-smirnov-test-in-python-scipy,"[""I can't figure out how to do a Two-sample KS test in Scipy."", 'You are using the one-sample KS test.  You probably want the two-sample test  ks_2samp:', 'This is what the scipy docs say:']"
271,Efficient thresholding filter of an array with numpy,"I need to filter an array to remove the elements that are lower than a certain threshold. My current code is like this:

threshold = 5
a = numpy.array(range(10)) # testing data
b = numpy.array(filter(...",https://stackoverflow.com/questions/7994394/efficient-thresholding-filter-of-an-array-with-numpy,"['I need to filter an array to remove the elements that are lower than a certain threshold. My current code is like this:', 'b = a[a>threshold] this should do']"
272,How to make scipy.interpolate give an extrapolated result beyond the input range?,I'm trying to port a program which uses a hand-rolled interpolator (developed by a mathematician colleage) over to use the interpolators provided by scipy. I'd like to use or wrap the scipy ...,https://stackoverflow.com/questions/2745329/how-to-make-scipy-interpolate-give-an-extrapolated-result-beyond-the-input-range,"[""I'm trying to port a program which uses a hand-rolled interpolator (developed by a mathematician colleage) over to use the interpolators provided by scipy. I'd like to use or wrap the scipy interpolator so that it has as close as possible behavior to the old interpolator."", 'You can use interp function from scipy, it extrapolates left and right values as constant beyond the range:', 'You can take a look at InterpolatedUnivariateSpline', ""As of SciPy version 0.17.0, there is a new option for scipy.interpolate.interp1d that allows extrapolation. Simply set fill_value='extrapolate' in the call. Modifying your code in this way gives:"", 'What about scipy.interpolate.splrep (with degree 1 and no smoothing):', ""Here's an alternative method that uses only the numpy package. It takes advantage of numpy's array functions, so may be faster when interpolating/extrapolating large arrays:"", 'It may be faster to use boolean indexing with large datasets, since the algorithm checks if every point is in outside the interval, whereas boolean indexing allows an easier and faster comparison.', 'I did it by adding a point to my initial arrays. In this way I avoid defining self-made functions, and the linear extrapolation (in the example below: right extrapolation) looks ok.', ""I'm afraid that there is no easy to do this in Scipy to my knowledge. You can, as I'm fairly sure that you are aware, turn off the bounds errors and fill all function values beyond the range with a constant, but that doesn't really help. See this question on the mailing list for some more ideas. Maybe you could use some kind of piecewise function, but that seems like a major pain."", 'The below code gives you the simple extrapolation module. k is the value to which the data set y has to be extrapolated based on the data set x. The numpy module is required.', 'Standard interpolate + linear extrapolate:']"
273,Save / load scipy sparse csr_matrix in portable data format,"How do you save/load a scipy sparse csr_matrix in a portable format?  The scipy sparse matrix is created on Python 3 (Windows 64-bit) to run on Python 2 (Linux 64-bit).  Initially, I used pickle (with ...",https://stackoverflow.com/questions/8955448/save-load-scipy-sparse-csr-matrix-in-portable-data-format,"[""How do you save/load a scipy sparse csr_matrix in a portable format?  The scipy sparse matrix is created on Python 3 (Windows 64-bit) to run on Python 2 (Linux 64-bit).  Initially, I used pickle (with protocol=2 and fix_imports=True) but this didn't work going from Python 3.2.2 (Windows 64-bit) to Python 2.7.2 (Windows 32-bit) and got the error:"", 'edit: SciPy 1.19 now has scipy.sparse.save_npz and scipy.sparse.load_npz.', ""Though you write, scipy.io.mmwrite and scipy.io.mmread don't work for you, I just want to add how they work. This question is the no. 1 Google hit, so I myself started with np.savez and pickle.dump before switching to the simple and obvious scipy-functions. They work for me and shouldn't be overseen by those who didn't tried them yet."", 'Here is performance comparison of the three most upvoted answers using Jupyter notebook. The input is a 1M x 100K random sparse matrix with density 0.001, containing 100M non-zero values:', 'Now you can use scipy.sparse.save_npz :\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.save_npz.html', 'Assuming you have scipy on both machines, you can just use pickle.', 'As of scipy 0.19.0, you can save and load sparse matrices this way:', 'EDIT Apparently it is simple enough to:', 'This is what I used to save a lil_matrix.', 'This works for me:', 'I was asked to send the matrix in a simple and generic format:']"
274,Finding index of nearest point in numpy arrays of x and y coordinates,"I have two 2d numpy arrays: x_array contains positional information in the x-direction, y_array contains positions in the y-direction.
I then have a long list of x,y points.
For each point in the list,...",https://stackoverflow.com/questions/10818546/finding-index-of-nearest-point-in-numpy-arrays-of-x-and-y-coordinates,"['I have two 2d numpy arrays: x_array contains positional information in the x-direction, y_array contains positions in the y-direction.', 'scipy.spatial also has a k-d tree implementation: scipy.spatial.KDTree.', 'Here is a scipy.spatial.KDTree example', 'If you can massage your data into the right format, a fast way to go is to use the methods in scipy.spatial.distance:']"
275,Add numpy array as column to Pandas data frame,"I have a Pandas data frame object of shape (X,Y) that looks like this:

[[1, 2, 3],
[4, 5, 6],
[7, 8, 9]]
and a numpy sparse matrix (CSC) of shape (X,Z) that looks something like this

[[0, 1, 0],
[0,...",https://stackoverflow.com/questions/18646076/add-numpy-array-as-column-to-pandas-data-frame,"['I have a Pandas data frame object of shape (X,Y) that looks like this:', 'yields', 'Consider using a higher dimensional datastructure (a Panel), rather than storing an array in your column:', 'Here is other example:', 'For normal numpy arrays, to add and retrieve from dataframe, you can do this. It builds on the previous answer that confused me because of the sparse part when I just had a normal numpy array.']"
276,How does NumPy's transpose() method permute the axes of an array?,"In [28]: arr = np.arange(16).reshape((2, 2, 4))

In [29]: arr
Out[29]: 
array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7]],

       [[ 8,  9, 10, 11],
        [12, 13, 14, 15]]])
In [32]: arr....",https://stackoverflow.com/questions/32034237/how-does-numpys-transpose-method-permute-the-axes-of-an-array,"['When we pass a tuple of integers to the transpose() function, what happens?', 'To transpose an array, NumPy just swaps the shape and stride information for each axis. Here are the strides:', 'As explained in the documentation:', 'In C notation, your array would be:', 'It seems the question and the example originates from the book Python for Data Analysis by Wes McKinney. This feature of transpose is mentioned in Chapter 4.1. Transposing Arrays and Swapping Axes.', 'To summarise a.transpose()[i,j,k] = a[k,j,i]']"
277,Could not install packages due to a “Environment error :[error 13]: permission denied : 'usr/local/bin/f2py'”,"I am trying to install numpy on macOS X but after executing the command pip install numpy I am getting the error:
  Environment error :[error 13]: permission denied : 'usr/local/bin/f2py
How do I ...",https://stackoverflow.com/questions/50807507/could-not-install-packages-due-to-a-environment-error-error-13-permission-d,"['I am trying to install numpy on macOS X but after executing the command pip install numpy I am getting the error:', 'This worked for me.', 'Seems to work, but the package is install the the path of user. such as :', 'I am also a Windows user. And I have installed Python 3.7 and when I try to install any package it throws the same error that you are getting.', 'I too had to face the same problem. This worked for me. Right click and run as admin than run usual command to install. But first run update command to update the pip', 'Well, in my case the problem had a different cause, the Windows path Length Check this.', 'I just ran the command with sudo:', 'I had the same problem for different package. I was installing pyinstaller in conda on Mac Mojave. I did', 'On Windows this has worked for me. From the command line, specify the path to the exe for Python:\n& ""C:/Program Files (x86)/Python37-32/python.exe"" -m pip install --upgrade pip --user', 'As a windows user, run an Admin powershell and launch :', 'It is always preferred to use a virtual environment ,Create your virtual environment using :']"
278,Calculate mean across dimension in a 2D array,"I have an array a like this:

a = [[40, 10], [50, 11]]
I need to calculate the mean for each dimension separately, the result should be this:

[45, 10.5]
45 being the mean of a[*][0] and 10.5 the ...",https://stackoverflow.com/questions/15819980/calculate-mean-across-dimension-in-a-2d-array,"['I have an array a like this:', 'a.mean() takes an axis argument:', 'Here is a non-numpy solution:', 'If you do this a lot, NumPy is the way to go.']"
279,Fastest way to grow a numpy numeric array,"Requirements:
I need to grow an array arbitrarily large from data.  
I can guess the size (roughly 100-200) with no guarantees that the array will fit every time
Once it is grown to its final size, I ...",https://stackoverflow.com/questions/7133885/fastest-way-to-grow-a-numpy-numeric-array,"['Requirements:', 'I tried a few different things, with timing.', 'np.append() copy all the data in the array every time, but list grow the capacity by a factor (1.125). list is fast, but memory usage is larger than array. You can use array module of the python standard library if you care about the memory.', ""Using the class declarations in Owen's post, here is a revised timing with some effect of the finalize."", 'there is a big performance difference in the function that you use for finalization. Consider the following code:', 'If you want improve performance with list operations, have a look to blist library. It is a optimized implementation of python list and other structures.']"
280,Immutable numpy array?,"Is there a simple way to create an immutable NumPy array?

If one has to derive a class from ndarray to do this, what's the minimum set of methods that one has to override to achieve immutability?",https://stackoverflow.com/questions/5541324/immutable-numpy-array,"['Is there a simple way to create an immutable NumPy array?', 'You can make a numpy array unwriteable:']"
281,Is there a multi-dimensional version of arange/linspace in numpy?,"I would like a list of 2d NumPy arrays (x,y) , where each x is in {-5, -4.5, -4, -3.5, ..., 3.5, 4, 4.5, 5} and the same for y.

I could do 

x = np.arange(-5, 5.1, 0.5)
y = np.arange(-5, 5.1, 0.5)
...",https://stackoverflow.com/questions/32208359/is-there-a-multi-dimensional-version-of-arange-linspace-in-numpy,"['I would like a list of 2d NumPy arrays (x,y) , where each x is in {-5, -4.5, -4, -3.5, ..., 3.5, 4, 4.5, 5} and the same for y.', ""You can use np.mgrid for this, it's often more convenient than np.meshgrid because it creates the arrays in one step:"", 'This is just what you are looking for:', 'I think you want np.meshgrid:', 'If you just want to iterate through pairs (and not do calculations on the whole set of points at once), you may be best served by itertools.product to iterate through all possible pairs:', 'We can use arrange function as:', 'Not sure if I understand the question - to make a list of 2-element NumPy arrays, this works:', 'It is not super fast solution, but works for any dimension', 'I still did it with Linspace because I prefer to stick to this command.', 'Based on this example, you can make any dim you want']"
282,Performance of Pandas apply vs np.vectorize to create new column from existing columns,"I am using Pandas dataframes and want to create a new column as a function of existing columns. I have not seen a good discussion of the speed difference between df.apply() and np.vectorize(), so I ...",https://stackoverflow.com/questions/52673285/performance-of-pandas-apply-vs-np-vectorize-to-create-new-column-from-existing-c,"['I am using Pandas dataframes and want to create a new column as a function of existing columns. I have not seen a good discussion of the speed difference between df.apply() and np.vectorize(), so I thought I would ask here.', 'I will start by saying that the power of Pandas and NumPy arrays is derived from high-performance vectorised calculations on numeric arrays.1 The entire point of vectorised calculations is to avoid Python-level loops by moving calculations to highly optimised C code and utilising contiguous memory blocks.2', ""The more complex your functions get (i.e., the less numpy can move to its own internals), the more you will see that the performance won't be that different. For example:"", ""I am new to python. But in the example below 'apply' seems to work faster than 'vectorize', or am I missing something.""]"
283,how is axis indexed in numpy's array?,"From Numpy's tutorial, axis can be indexed with integers, like 0 is for column, 1 is for row, but I don't grasp why they are indexed this way? And How do I figure out each axis' index when coping with ...",https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array,"[""From Numpy's tutorial, axis can be indexed with integers, like 0 is for column, 1 is for row, but I don't grasp why they are indexed this way? And How do I figure out each axis' index when coping with multidimensional array?"", ""By definition, the axis number of the dimension is the index of that dimension within the array's shape. It is also the position used to access that dimension during indexing."", 'If at all anyone need this visual description:', 'You can grasp axis in this way:', 'In general, axis = 0, means all cells with first dimension varying with each value of 2nd dimension and 3rd dimension and so on']"
284,Mean Squared Error in Numpy?,"Is there a method in numpy for calculating the Mean Squared Error between two matrices?

I've tried searching but found none. Is it under a different name?

If there isn't, how do you overcome this? ...",https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy,"['Is there a method in numpy for calculating the Mean Squared Error between two matrices?', 'You can use:', ""This isn't part of numpy, but it will work with numpy.ndarray objects. A numpy.matrix can be converted to a numpy.ndarray and a numpy.ndarray can be converted to a numpy.matrix."", 'Even more numpy', 'Another alternative to the accepted answer that avoids any issues with matrix multiplication:', 'Just for kicks', 'The standard numpy methods for calculation mean squared error (variance) and its square root (standard deviation) are numpy.var() and numpy.std(), see here and here. They apply to matrices and have the same syntax as numpy.mean().']"
285,load csv into 2D matrix with numpy for plotting,"Given this CSV file:

""A"",""B"",""C"",""D"",""E"",""F"",""timestamp""
611.88243,9089.5601,5133.0,864.07514,1715.37476,765.22777,1.291111964948E12
611.88243,9089.5601,5133.0,864.07514,1715.37476,765.22777,1....",https://stackoverflow.com/questions/4315506/load-csv-into-2d-matrix-with-numpy-for-plotting,"['Given this CSV file:', 'Pure numpy', 'I think using dtype where there is a name row is confusing the routine.  Try', 'You can read a CSV file with headers into a NumPy structured array with np.genfromtxt. For example:']"
286,How to get correlation of two vectors in python [duplicate],"In matlab I use

a=[1,4,6]
b=[1,2,3]
corr(a,b)
which returns .9934. I've tried numpy.correlate but it returns something completely different. What is the simplest way to get the correlation of two ...",https://stackoverflow.com/questions/19428029/how-to-get-correlation-of-two-vectors-in-python,"['In matlab I use', 'The docs indicate that numpy.correlate is not what you are looking for:']"
287,How to apply numpy.linalg.norm to each row of a matrix?,"I have a 2D matrix and I want to take norm of each row. But when I use numpy.linalg.norm(X) directly, it takes the norm of the whole matrix. 

I can take norm of each row by using a for loop and then ...",https://stackoverflow.com/questions/7741878/how-to-apply-numpy-linalg-norm-to-each-row-of-a-matrix,"['I have a 2D matrix and I want to take norm of each row. But when I use numpy.linalg.norm(X) directly, it takes the norm of the whole matrix.', 'Note that, as perimosocordiae shows, as of NumPy version 1.9, np.linalg.norm(x, axis=1) is the fastest way to compute the L2-norm.', 'Resurrecting an old question due to a numpy update. As of the 1.9 release, numpy.linalg.norm now accepts an axis argument. [code, documentation]', 'Much faster than the accepted answer is', 'Try the following:']"
288,How to have logarithmic bins in a Python histogram,"As far as I know the option Log=True in the histogram function only refers to the y-axis.

P.hist(d,bins=50,log=True,alpha=0.5,color='b',histtype='step')
I need the bins to be equally spaced in log10....",https://stackoverflow.com/questions/6855710/how-to-have-logarithmic-bins-in-a-python-histogram,"['As far as I know the option Log=True in the histogram function only refers to the y-axis.', 'use logspace() to create a geometric sequence, and pass it to bins parameter. And set the scale of xaxis to log scale.', 'The most direct way is to just compute the log10 of the limits, compute linearly spaced bins, and then convert back by raising to the power of 10, as below:', ""The following code indicates how you can use bins='auto' with the log scale."", 'In addition to what was stated, performing this on pandas dataframes works as well:']"
289,shuffle vs permute numpy,"What is the difference between numpy.random.shuffle(x) and numpy.random.permutation(x)?

I have read the doc pages but I could not understand if there was any difference between the two when I just ...",https://stackoverflow.com/questions/15474159/shuffle-vs-permute-numpy,"['What is the difference between numpy.random.shuffle(x) and numpy.random.permutation(x)?', 'np.random.permutation has two differences from np.random.shuffle:', 'Adding on to what @ecatmur said, np.random.permutation is useful when you need to shuffle ordered pairs, especially for classification:', 'Adding on @ecatmur, Here is a brief explanation. To start with I have created an array which is of shape 3,3 and has numbers from 0 to 8', 'The permutation() method returns a re-arranged array (and leaves the original array un-changed),this method will keep the original array intact and will return a shuffled array, for example x = [1,4,2,8] is the original array and the permutation method will return the rearranged array (lets say [8,4,1,2]).Now,you have two arrays, original array and the rearranged array.']"
290,NumPy: Logarithm with base n,"From the numpy documentation on logarithms, I have found functions to take the logarithm with base e, 2, and 10:

import numpy as np
np.log(np.e**3) #3.0
np.log2(2**3)   #3.0
np.log10(10**3) #3.0
...",https://stackoverflow.com/questions/25169297/numpy-logarithm-with-base-n,"['From the numpy documentation on logarithms, I have found functions to take the logarithm with base e, 2, and 10:', 'To get the logarithm with a custom base using math.log:']"
291,numpy division with RuntimeWarning: invalid value encountered in double_scalars,"I wrote the following script:

import numpy

d = numpy.array([[1089, 1093]])
e = numpy.array([[1000, 4443]])
answer = numpy.exp(-3 * d)
answer1 = numpy.exp(-3 * e)
res = answer.sum()/answer1.sum()
...",https://stackoverflow.com/questions/27784528/numpy-division-with-runtimewarning-invalid-value-encountered-in-double-scalars,"['I wrote the following script:', ""You can't solve it. Simply answer1.sum()==0, and you can't perform a division by zero."", ""You can use np.logaddexp (which implements the idea in @gg349's answer):""]"
292,How to get reproducible results in keras,"I get different results (test accuracy) every time I run the imdb_lstm.py example from Keras framework (https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)
The code contains np.random....",https://stackoverflow.com/questions/32419510/how-to-get-reproducible-results-in-keras,"['I get different results (test accuracy) every time I run the imdb_lstm.py example from Keras framework (https://github.com/fchollet/keras/blob/master/examples/imdb_lstm.py)\nThe code contains np.random.seed(1337) in the top, before any keras imports. It should prevent it from generating different numbers for every run. What am I missing?', 'You can find the answer at the Keras docs: https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development.', ""Theano's documentation talks about the difficulties of seeding random variables and why they seed each graph instance with its own random number generator."", ""I finally got reproducible results with my code. It's a combination of answers I saw around the web. The first thing is doing what @alex says:"", ""The problem is now solved in Tensorflow 2.0 ! I had the same issue with TF 1.x (see  If Keras results are not reproducible, what's the best practice for comparing models and choosing hyper parameters? ) but"", 'I would like to add something to the previous answers. If you use python 3 and you want to get reproducible results for every run, you have to', 'I have trained and tested Sequential() kind of neural networks using Keras. I performed non linear regression on noisy speech data. I used the following code to generate random seed :', 'This works for me:', 'I agree with the previous comment, but reproducible results sometimes needs the same environment(e.g. installed packages, machine characteristics and so on). So that, I recommend to copy your environment to other place in case to have reproducible results. Try to use one of the next technologies:', 'The Conference Paper: Non-Random Weight Initialisation in Deep Learning Networks for Repeatable Determinism, publication date Jun 5, 2019  presented at 10th IEEE International Conference Dependable Systems, Services and Technologies (DESSERT-19) at Leeds Beckett University (LBU), United Kingdom, UK, Ireland and the Ukrainian section of IEEE June 5-7, 2019', 'In Tensorflow 2.0 you can set random seed like this :']"
293,when should i use hstack/vstack vs append vs concatenate vs column_stack,Simple question: what is the advantage of each of these methods. It seems that given the right parameters (and ndarray shapes) they all work seemingly equivalently. Do some work in place? have better ...,https://stackoverflow.com/questions/33356442/when-should-i-use-hstack-vstack-vs-append-vs-concatenate-vs-column-stack,"['Simple question: what is the advantage of each of these methods. It seems that given the right parameters (and ndarray shapes) they all work seemingly equivalently. Do some work in place? have better performance? which functions should I use when?', 'All the functions are written in Python except np.concatenate.  With an IPython shell you just use ??.', 'numpy.vstack: stack arrays in sequence vertically (row wise).Equivalent to np.concatenate(tup, axis=0)  example see: https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html', ""In IPython you can look at the source code of a function by typing its name followed by ??. Taking a look at hstack we can see that it's actually just a wrapper around concatenate (similarly with vstack and column_stack):""]"
294,Is there any way to use pythonappend with SWIG's new builtin feature?,"I have a little project that works beautifully with SWIG.  In particular, some of my functions return std::vectors, which get translated to tuples in Python.  Now, I do a lot of numerics, so I just ...",https://stackoverflow.com/questions/9270052/is-there-any-way-to-use-pythonappend-with-swigs-new-builtin-feature,"[""I have a little project that works beautifully with SWIG.  In particular, some of my functions return std::vectors, which get translated to tuples in Python.  Now, I do a lot of numerics, so I just have SWIG convert these to numpy arrays after they're returned from the c++ code.  To do this, I use something like the following in SWIG."", 'I agree with you that using typemap gets a little messy, but it is the right way to accomplish this task. You are also right that the SWIG documentation does not directly say that %pythonappend is incompatible with -builtin, but it is strongly implied: %pythonappend adds to the Python proxy class, and the Python proxy class does not exist at all in conjunction with the -builtin flag.']"
295,shuffling/permutating a DataFrame in pandas,"What's a simple and efficient way to shuffle a dataframe in pandas, by rows or by columns? I.e. how to write a function shuffle(df, n, axis=0) that takes a dataframe, a number of shuffles n, and an ...",https://stackoverflow.com/questions/15772009/shuffling-permutating-a-dataframe-in-pandas,"[""What's a simple and efficient way to shuffle a dataframe in pandas, by rows or by columns? I.e. how to write a function shuffle(df, n, axis=0) that takes a dataframe, a number of shuffles n, and an axis (axis=0 is rows, axis=1 is columns) and returns a copy of the dataframe that has been shuffled n times."", ""Use numpy's random.permuation function:"", 'Sampling randomizes, so just sample the entire data frame.', 'You can use  sklearn.utils.shuffle() (requires sklearn 0.16.1 or higher to support Pandas data frames):', 'From the docs use sample():', 'A simple solution in pandas is to use the sample method independently on each column. Use apply to iterate over each column:', ""I resorted to adapting @root 's answer slightly and using the raw values directly. Of course, this means you lose the ability to do fancy indexing but it works perfectly for just shuffling the data."", 'This might be more useful when you want your index shuffled.', 'I know the question is for a pandas df but in the case the shuffle occurs by row (column order changed, row order unchanged), then the columns names do not matter anymore and it could be interesting to use an np.array instead, then np.apply_along_axis() will be what you are looking for.', 'Here is a work around I found if you want to only shuffle a subset of the DataFrame:']"
296,Weighted standard deviation in NumPy,"numpy.average() has a weights option, but numpy.std() does not.  Does anyone have suggestions for a workaround?",https://stackoverflow.com/questions/2413522/weighted-standard-deviation-in-numpy,"['numpy.average() has a weights option, but numpy.std() does not.  Does anyone have suggestions for a workaround?', 'How about the following short ""manual calculation""?', 'There is a class in statsmodels that makes it easy to calculate weighted statistics: statsmodels.stats.weightstats.DescrStatsW.', ""Here's one more option:"", ""There doesn't appear to be such a function in numpy/scipy yet, but there is a ticket proposing this added functionality. Included there you will find Statistics.py which implements weighted standard deviations."", 'There is a very good example proposed by gaborous:']"
297,How to create a numpy array of arbitrary length strings?,"I'm a complete rookie to Python, but it seems like a given string is able to be (effectively) arbitrary length.  i.e. you can take a string str and keeping adding to it: str += ""some stuff..."".  Is ...",https://stackoverflow.com/questions/14639496/how-to-create-a-numpy-array-of-arbitrary-length-strings,"['I\'m a complete rookie to Python, but it seems like a given string is able to be (effectively) arbitrary length.  i.e. you can take a string str and keeping adding to it: str += ""some stuff..."".  Is there a way to make an array of such strings?', 'You can do so by creating an array of dtype=object. If you try to assign a long string to a normal numpy array, it truncates the string:', 'You could use the object data type:']"
298,numpy get index where value is true,">>> ex=np.arange(30)
>>> e=np.reshape(ex,[3,10])
>>> e
array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
       [20, 21, 22, ...",https://stackoverflow.com/questions/16094563/numpy-get-index-where-value-is-true,"['I need to find the rows that have true or rows in e whose value are more than 15. I could iterate using a for loop, however, I would like to know if there is a way numpy could do this more efficiently?', 'To get the row numbers where at least one item is larger than 15:', 'You can use nonzero function. it returns the nonzero indices of the given input.', 'A simple and clean way: use np.argwhere to group the indices by element, rather than dimension as in np.nonzero(a) (i.e., np.argwhere returns a row for each non-zero element).']"
299,Mesh grid functions in Python (meshgrid mgrid ogrid ndgrid),"I'm looking for a clear comparison of meshgrid-like functions. Unfortunately I don't find it!

Numpy http://docs.scipy.org/doc/numpy/reference/ provides
mgrid
ogrid
meshgrid
Scitools http://hplgit....",https://stackoverflow.com/questions/12402045/mesh-grid-functions-in-python-meshgrid-mgrid-ogrid-ndgrid,"[""I'm looking for a clear comparison of meshgrid-like functions. Unfortunately I don't find it!"", ""numpy.meshgrid is modelled after Matlab's meshgrid command. It is used to vectorise functions of two variables, so that you can write"", 'np.mgrid and np.meshgrid() do the same thing but the first and the second axis are swapped:']"
300,What does .shape[] do in “for i in range(Y.shape[0])”?,"I'm trying to break down a program line by line. Y is a matrix of data but I can't find any concrete data on what .shape[0] does exactly.

for i in range(Y.shape[0]):
    if Y[i] == -1:
This program ...",https://stackoverflow.com/questions/10200268/what-does-shape-do-in-for-i-in-rangey-shape0,"[""I'm trying to break down a program line by line. Y is a matrix of data but I can't find any concrete data on what .shape[0] does exactly."", 'The shape attribute for numpy arrays returns the dimensions of the array. If Y has n rows and m columns, then Y.shape is (n,m). So Y.shape[0] is n.', 'shape is a tuple that gives dimensions of the array..', 'shape is a tuple that gives you an indication of the number of dimensions in the array. So in your case, since the index value of Y.shape[0] is 0, your are working along the first dimension of your array.', 'In python, Suppose you have loaded up the data in some variable train:', 'In Python shape() is use in pandas to give number of row/column:', 'shape() consists of array having two arguments rows and columns.']"
