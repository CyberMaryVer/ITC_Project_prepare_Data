,question,summary,link,answers
0,,,,
1,Options for HTML scraping? [closed],"I'm thinking of trying Beautiful Soup, a Python package for HTML scraping. Are there any other HTML scraping packages I should be looking at? Python is not a requirement, I'm actually interested in ...",https://stackoverflow.com/questions/2861/options-for-html-scraping,"['Want to improve this question? Update the question so it focuses on one problem only by editing this post.', ""The Ruby world's equivalent to Beautiful Soup is why_the_lucky_stiff's Hpricot."", ""In the .NET world, I recommend the HTML Agility Pack. Not near as simple as some of the above options (like HTMLSQL), but it's very flexible. It lets you maniuplate poorly formed HTML as if it were well formed XML, so you can use XPATH or just itereate over nodes."", ""BeautifulSoup is a great way to go for HTML scraping. My previous job had me doing a lot of scraping and I wish I knew about BeautifulSoup when I started. It's like the DOM with a lot more useful options and is a lot more pythonic. If you want to try Ruby they ported BeautifulSoup calling it RubyfulSoup but it hasn't been updated in a while."", 'I found HTMLSQL to be a ridiculously simple way to screenscrape. It takes literally minutes to get results with it.', ""The Python lxml library acts as a Pythonic binding for the libxml2 and libxslt libraries. I like particularly its XPath support and pretty-printing of the in-memory XML structure. It also supports parsing broken HTML. And I don't think you can find other Python libraries/bindings that parse XML faster than lxml."", ""For Perl, there's WWW::Mechanize."", 'Python has several options for HTML scraping in addition to Beatiful Soup. Here are some others:', ""'Simple HTML DOM Parser' is a good option for PHP, if your familiar with jQuery or JavaScript selectors then you will find yourself at home."", 'Why has no one mentioned JSOUP yet for Java? http://jsoup.org/', 'The templatemaker utility from Adrian Holovaty (of Django fame) uses a very interesting approach: You feed it variations of the same page and it ""learns"" where the ""holes"" for variable data are. It\'s not HTML specific, so it would be good for scraping any other plaintext content as well. I\'ve used it also for PDFs and HTML converted to plaintext (with pdftotext and lynx, respectively).', 'I know and love Screen-Scraper.', 'I would first find out if the site(s) in question provide an API server or RSS Feeds for access the data you require.', 'Scraping Stack Overflow is especially easy with Shoes and Hpricot.', ""Another option for Perl would be Web::Scraper which is based on Ruby's Scrapi. In a nutshell, with nice and concise syntax, you can get a robust scraper directly into data structures."", ""I've had some success with HtmlUnit, in Java. It's a simple framework for writing unit tests on web UI's, but equally useful for HTML scraping."", 'Yahoo! Query Language or YQL can be used alongwith jQuery, AJAX, JSONP to screen scrape web pages', 'Another tool for .NET is MhtBuilder', 'There is this solution too: netty HttpClient', ""I use Hpricot on Ruby. As an example this is a snippet of code that I use to retrieve all book titles from the six pages of my HireThings account (as they don't seem to provide a single page with this information):"", ""I've used Beautiful Soup a lot with Python. It is much better than regular expression checking, because it works like using the DOM, even if the HTML is poorly formatted. You can quickly find HTML tags and text with simpler syntax than regular expressions. Once you find an element, you can iterate over it and its children, which is more useful for understanding the contents in code than it is with regular expressions. I wish Beautiful Soup existed years ago when I had to do a lot of screenscraping -- it would have saved me a lot of time and headache since HTML structure was so poor before people started validating it."", ""Although it was designed for .NET web-testing, I've been using the WatiN framework for this purpose. Since it is DOM-based, it is pretty easy to capture HTML, text, or images. Recentely, I used it to dump a list of links from a MediaWiki All Pages namespace query into an Excel spreadsheet. The following VB.NET code fragement is pretty crude, but it works."", 'Implementations of the HTML5 parsing algorithm: html5lib (Python, Ruby), Validator.nu HTML Parser (Java, JavaScript; C++ in development), Hubbub (C), Twintsam (C#; upcoming).', 'You would be a fool not to use Perl.. Here come the flames..', 'I have used LWP and HTML::TreeBuilder with Perl and have found them very useful.', 'In Java, you can use TagSoup.', 'Well, if you want it done from the client side using only a browser you have jcrawl.com. After having designed your scrapping service from the web application (http://www.jcrawl.com/app.html), you only need to add the generated script to an HTML page to start using/presenting your data.', 'You probably have as much already, but I think this is what you are trying to do:', ""I've had mixed results in .NET using SgmlReader which was originally started by Chris Lovett and appears to have been updated by MindTouch."", ""I like Google Spreadsheets' ImportXML(URL, XPath) function."", ""I've also had great success using Aptana's Jaxer + jQuery to parse pages. It's not as fast or 'script-like' in nature, but jQuery selectors + real JavaScript/DOM is a lifesaver on more complicated (or malformed) pages.""]"
2,Headless Browser and scraping - solutions [closed],"I'm trying to put list of possible solutions for browser automatic tests suits and headless browser platforms capable of scraping.
BROWSER TESTING / SCRAPING:
Selenium - polyglot flagship in browser ...",https://stackoverflow.com/questions/18539491/headless-browser-and-scraping-solutions,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'If Ruby is your thing, you may also try:', 'http://triflejs.org/ is like phantomjs but based on IE', 'A kind of JS-based Selenium is Dalek.js. It not only aims for automated frontend-tests, you can also do screenshots with it. It has webdrivers for all important browsers. Unfortunately those webdrivers seem to be worth improving (just not to say ""buggy"" to Firefox).']"
3,How can I get the Google cache age of any URL or web page? [closed],"In my project I need the Google cache age to be added as important information. I tried to search sources for the Google cache age, that is, the number of days since Google last re-indexed the page ...",https://stackoverflow.com/questions/4560400/how-can-i-get-the-google-cache-age-of-any-url-or-web-page,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'Use the URL', ""You'll need to scrape the resulting page, but you can view the most recent cache page using this URL:"", 'you can Use CachedPages website', 'its too simple, you can just type ""cache:"" before the URL of the page. for example\nif you want to check the last webcache of this page simply type on URL bar cache:http://stackoverflow.com/questions/4560400/how-can-i-get-the-google-cache-age-of-any-url-or-web-page', 'You can use this site: https://cachedviews.com/ . Cache View or Cached Pages of Any Website - Google Cached Pages of Any Website', 'This one good also to view cachepage http://www.cachepage.net']"
4,Which HTML Parser is the best? [closed],"I code a lot of parsers. Up until now, I was using HtmlUnit headless browser for parsing and browser automation.

Now, I want to separate both the tasks.

As 80% of my work involves just parsing, I ...",https://stackoverflow.com/questions/2168610/which-html-parser-is-the-best,"['I code a lot of parsers. Up until now, I was using HtmlUnit headless browser for parsing and browser automation.', 'Self plug: I have just released a new Java HTML parser: jsoup. I mention it here because I think it will do what you are after.', ""The best I've seen so far is HtmlCleaner:"", ""I suggest Validator.nu's parser, based on the HTML5 parsing algorithm. It is the parser used in Mozilla from 2010-05-03""]"
5,Web-scraping JavaScript page with Python,"I'm trying to develop a simple web scraper. I want to extract text without the HTML code. In fact, I achieve this goal, but I have seen that in some pages where JavaScript is loaded I didn't obtain ...",https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python,"[""I'm trying to develop a simple web scraper. I want to extract text without the HTML code. In fact, I achieve this goal, but I have seen that in some pages where JavaScript is loaded I didn't obtain good results."", 'EDIT 30/Dec/2017: This answer appears in top results of Google searches, so I decided to update it. The old answer is still at the end.', 'We are not getting the correct results because any javascript generated content needs to be rendered on the DOM. When we fetch an HTML page, we fetch the initial, unmodified by javascript, DOM.', 'Maybe selenium can do it.', 'If you have ever used the Requests module for python before, I recently found out that the developer created a new module called Requests-HTML which now also has the ability to render JavaScript.', 'This seems to be a good solution also, taken from a great blog post', ""It sounds like the data you're really looking for can be accessed via secondary URL called by some javascript on the primary page."", 'Selenium is the best for scraping JS and Ajax content.', 'You can also execute javascript using webdriver.', ""I personally prefer using scrapy and selenium and dockerizing both in separate containers. This way you can install both with minimal hassle and crawl modern websites that almost all contain javascript in one form or another. Here's an example:"", 'A mix of BeautifulSoup and Selenium works very well for me.', ""You'll want to use urllib, requests, beautifulSoup and selenium web driver in your script for different parts of the page, (to name a few).\nSometimes you'll get what you need with just one of these modules.\nSometimes you'll need two, three, or all of these modules.\nSometimes you'll need to switch off the js on your browser.\nSometimes you'll need header info in your script.\nNo websites can be scraped the same way and no website can be scraped in the same way forever without having to modify your crawler, usually after a few months. But they can all be scraped! Where there's a will there's a way for sure.\nIf you need scraped data continuously into the future just scrape everything you need and store it in .dat files with pickle.\nJust keep searching how to try what with these modules and copying and pasting your errors into the Google."", 'Using PyQt5', 'I\'ve been trying to find answer to this questions for two days. Many answers direct you to different issues. But serpentr\'s answer above is really to the point. It is the shortest, simplest solution. Just a reminder the last word ""var"" represents the variable name, so should be used as:', 'As mentioned, Selenium is a good choice for rendering the results of the JavaScript:']"
6,Web scraping with Python [closed],I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?,https://stackoverflow.com/questions/2081586/web-scraping-with-python,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'Use urllib2 in combination with the brilliant BeautifulSoup library:', ""I'd really recommend Scrapy."", 'I collected together scripts from my web scraping work into this bit-bucket library.', 'I would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.', ""You can use urllib2 to make the HTTP requests, and then you'll have web content."", 'I use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.', 'Make your life easier by using CSS Selectors', 'If we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:', ""Here is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who's class name is _3NFO0d. I used Flipkar.com, it is an online retailing store."", 'Python has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. \n1. Install python above 3.5 (lower ones till 2.7 will work). \n2. Create a environment in conda ( I did this). \n3. Install scrapy at a location and run in from there. \n4. Scrapy shell will give you an interactive interface to test you code. \n5. Scrapy startproject projectname will create a framework.\n6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory.']"
7,How to save an image locally using Python whose URL address I already know?,"I know the URL of an image on Internet.

e.g. http://www.digimouth.com/news/media/2011/09/google-logo.jpg, which contains the logo of Google.

Now, how can I download this image using Python without ...",https://stackoverflow.com/questions/8286352/how-to-save-an-image-locally-using-python-whose-url-address-i-already-know,"['I know the URL of an image on Internet.', 'Here is a more straightforward way if all you want to do is save it as a file:', 'file01.jpg will contain your image.', 'I wrote a script that does just this, and it is available on my github for your use.', 'This can be done with requests. Load the page and dump the binary content to a file.', 'Python 3', 'A solution which works with Python 2 and Python 3:', ""I made a script expanding on Yup.'s script. I fixed some things. It will now bypass 403:Forbidden problems. It wont crash when an image fails to be retrieved. It tries to avoid corrupted previews. It gets the right absolute urls. It gives out more information. It can be run with an argument from the command line."", 'Using requests library', 'This is very short answer.', 'I adjusted the code of @madprops for Python 3', 'Something fresh for Python 3 using Requests:', ""If you don't already have the url for the image, you could scrape it with gazpacho:"", 'Late answer, but for python>=3.6 you can use dload, i.e.:', 'Use a simple python wget module to download the link. Usage below:']"
8,Scraping html tables into R data frames using the XML package,"How do I scrape html tables using the XML package?

Take, for example, this wikipedia page on the Brazilian soccer team. I would like to read it in R and get the ""list of all matches Brazil have ...",https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package,"['How do I scrape html tables using the XML package?', '…or a shorter try:', 'Edited to add:', 'Another option using Xpath.', 'The rvest along with xml2 is another popular package for parsing html web pages.']"
9,retrieve links from web page using python and BeautifulSoup [closed],How can I retrieve the links of a webpage and copy the url address of the links using Python?,https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup,"['Want to improve this question? Add details and clarify the problem by editing this post.', ""Here's a short snippet using the SoupStrainer class in BeautifulSoup:"", 'For completeness sake, the BeautifulSoup 4 version, making use of the encoding supplied by the server as well:', 'Others have recommended BeautifulSoup, but it\'s much better to use lxml. Despite its name, it is also for parsing and scraping HTML. It\'s much, much faster than BeautifulSoup, and it even handles ""broken"" HTML better than BeautifulSoup (their claim to fame). It has a compatibility API for BeautifulSoup too if you don\'t want to learn the lxml API.', 'The following code is to retrieve all the links available in a webpage using urllib2 and BeautifulSoup4:', 'Under the hood BeautifulSoup now uses lxml. Requests, lxml & list comprehensions makes a killer combo.', 'just for getting the links, without B.soup and regex:', 'This script does what your looking for, But also resolves the relative links to absolute links.', 'To find all the links, we will in this example use the urllib2 module together\nwith the re.module\n*One of the most powerful function in the re module is ""re.findall()"".\nWhile re.search() is used to find the first match for a pattern, re.findall() finds all\nthe matches and returns them as a list of strings, with each string representing one match*', 'Why not use regular expressions:', 'Links can be within a variety of attributes so you could pass a list of those attributes to select', ""Here's an example using @ars accepted answer and the BeautifulSoup4, requests, and wget modules to handle the downloads."", 'I found the answer by @Blairg23 working , after the following correction (covering the scenario where it failed to work correctly):', ""BeatifulSoup's own parser can be slow. It might be more feasible to use lxml which is capable of parsing directly from a URL (with some limitations mentioned below)."", 'There can be many duplicate links together with both external and internal links.  To differentiate between the two and just get unique links using sets:']"
10,Puppeteer: pass variable in .evaluate(),"I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined.

I'm new to Puppeteer and can't ...",https://stackoverflow.com/questions/46088351/puppeteer-pass-variable-in-evaluate,"[""I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined."", 'You have to pass the variable as an argument to the pageFunction like this:', ""I encourage you to stick on this style, because it's more convenient and readable."", 'You can pass one variable to page.evaluate() using the following syntax:', ""It took me quite a while to figure out that console.log() in evaluate() can't show in node console."", 'For pass a function, there are two ways you can do it.', 'I have a typescript example that could help someone new in typescript.', 'With page.$$eval']"
11,How to use Python requests to fake a browser visit?,"I want to get the content from the below website. If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get ...",https://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit,"['I want to get the content from the below website. If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get it, it returns a totally different HTML page. I thought the developer of the website had made some blocks for this, so the question is:', 'Provide a User-Agent header:', 'if this question is still valid', ""Try doing this, using firefox as fake user agent (moreover, it's a good startup script for web scraping with the use of cookies):"", 'The root of the answer is that the person asking the question needs to have a JavaScript interpreter to get what they are after. What I have found is I am able to get all of the information I wanted on a website in json before it was interpreted by JavaScript. This has saved me a ton of time in what would be parsing html hoping each webpage is in the same format.']"
12,Scraping: SSL: CERTIFICATE_VERIFY_FAILED error for http://en.wikipedia.org,"I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:

from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

pages = set()
def ...",https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org,"[""I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:"", 'Once upon a time I stumbled  with this issue. If you\'re using macOS go to Macintosh HD > Applications > Python3.6 folder (or whatever version of python you\'re using) > double click on ""Install Certificates.command"" file. :D', 'To solve this:', 'to use unverified ssl you can add this to your code:', 'This terminal command:', 'For novice users, you can go in the Applications folder and expand the Python 3.7 folder. Now first run (or double click) the Install Certificates.command and then Update Shell Profile.command', 'Two steps worked for me :\n- going Macintosh HD > Applications > Python3.7 folder \n- click on ""Install Certificates.command""', 'Take a look at this post, it seems like for later versions of Python, certificates are not pre installed which seems to cause this error. You should be able to run the following command to install the certifi package: /Applications/Python\\ 3.6/Install\\ Certificates.command', 'For anyone who is using anaconda, you would install the certifi package, see more at:', 'I could find this solution and is working fine:', 'i didn\'t solve the problem, sadly.\nbut managed to make to codes work (almost all of my codes have this probelm btw)\nthe local issuer certificate problem happens under python3.7\nso i changed back to python2.7 QAQ\nand all that needed to change including ""from urllib2 import urlopen"" instead of ""from urllib.request import urlopen""\nso sad...', ""If you're running on a Mac you could just search for Install Certificates.command on the spotlight and hit enter."", 'For me the problem was that I was setting REQUESTS_CA_BUNDLE in my .bash_profile', 'Use requests library.\nTry this solution, or just add https:// before the URL:', ""I'm a relative novice compared to all the experts on Stack Overflow."", 'This will work. Set the environment variable PYTHONHTTPSVERIFY to 0.', 'Install Certificates.command on your mac.', 'The only solution that worked for me:']"
13,What's the best way of scraping data from a website? [closed],"I need to extract contents from a website, but the application doesn’t provide any application programming interface or another mechanism to access that data programmatically.

I found a useful third-...",https://stackoverflow.com/questions/22168883/whats-the-best-way-of-scraping-data-from-a-website,"['Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.', 'You will definitely want to start with a good web scraping framework. Later on you may decide that they are too limiting and you can put together your own stack of libraries but without a lot of scraping experience your design will be much worse than pjscrape or scrapy.', 'Yes you can do it yourself. It is just a matter of grabbing the sources of the page and parsing them the way you want.']"
14,What is the difference between web-crawling and web-scraping? [duplicate],"Is there a difference between Crawling and Web-scraping?

If there's a difference, what's the best method to use in order to collect some web data to supply a database for later use in a customised ...",https://stackoverflow.com/questions/4327392/what-is-the-difference-between-web-crawling-and-web-scraping,"['Is there a difference between Crawling and Web-scraping?', 'Crawling would be essentially what Google, Yahoo, MSN, etc. do, looking for ANY information.  Scraping is generally targeted at certain websites, for specfic data, e.g. for price comparison, so are coded quite differently.', 'Yes, they are different. In practice, you may need to use both.', 'AFAIK Web Crawling is what Google does - it goes around a website looking at links and building a database of the layout of that site and sites it links to', ""There's a fundamental difference between these two. \nFor those looking to dig deeper, I suggest you read this - \nWeb scraper, Web Crawler"", ""There's definitely a difference between these two. One refers to visiting a site, the other to extracting."", 'We crawl sites to have broad perspective how the site is structured, what are connections between pages, to estimate how much time we need to visit all pages we are interested in. Scraping is often harder to implement, but it’s an essence of data extraction. Let’s think of scraping as of covering website with sheet of paper with some rectangles cut out. We can now see only things we need, completely ignoring parts of website that are common for all pages (like navigation, footer, ads), or extraneous informations as comments or breadcrumbs.\nMore about differences between crawling and scrapping you find here: https://tarantoola.io/web-scraping-vs-web-crawling/']"
15,XPath:: Get following Sibling,"I have following HTML Structure: I am trying to build a robust method to extract second color digest element since there will be many of these tag within the DOM.

<table>
  <tbody>
    &...",https://stackoverflow.com/questions/11657223/xpath-get-following-sibling,"['I have following HTML Structure: I am trying to build a robust method to extract second color digest element since there will be many of these tag within the DOM.', ""You should be looking for the second tr that has the td that equals ' Color Digest ', then you need to look at either the following sibling of the first td in the tr, or the second td."", 'You can go for identifying a list of elements with xPath:', '/html/body/table/tbody/tr[9]/td[1]']"
16,selenium with scrapy for dynamic page,"I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:
starts with a product_list page with 10 products
a click on ""next""  button loads the ...",https://stackoverflow.com/questions/17975471/selenium-with-scrapy-for-dynamic-page,"[""I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:"", 'It really depends on how do you need to scrape the site and how and what data do you want to get.', ""If (url doesn't change between the two pages) then you should add dont_filter=True with your scrapy.Request() or scrapy will find this url as a duplicate after processing first page.""]"
17,Web scraping with Java,I'm not able to find any good web scraping Java based API. The site which I need to scrape does not provide any API as well; I want to iterate over all web pages using some pageID and extract the HTML ...,https://stackoverflow.com/questions/3202305/web-scraping-with-java,"[""I'm not able to find any good web scraping Java based API. The site which I need to scrape does not provide any API as well; I want to iterate over all web pages using some pageID and extract the HTML titles / other stuff in their DOM trees."", 'Extracting the title is not difficult, and you have many options, search here on Stack Overflow for ""Java HTML parsers"". One of them is Jsoup.', 'Your best bet is to use Selenium Web Driver since it', 'HTMLUnit can be used to do web scraping, it supports invoking pages, filling & submitting forms. I have used this in my project. It is good java library for web scraping.\nread here for more', 'mechanize for Java would be a good fit for this, and as Wadjy Essam mentioned it uses JSoup for the HMLT. mechanize is a stageful HTTP/HTML client that supports navigation, form submissions, and page scraping.', 'There is also Jaunt Java Web Scraping & JSON Querying - http://jaunt-api.com', 'You might look into jwht-scrapper!', 'Look at an HTML parser such as TagSoup, HTMLCleaner or NekoHTML.', 'If you wish to automate scraping of large amount pages or data, then you could try Gotz ETL.', 'For tasks of this type I usually use Crawller4j + Jsoup.', 'Normally I use selenium, which is software for testing automation.\nYou can control a browser through a webdriver, so you will not have problems with javascripts and it is usually not very detected if you use the full version. Headless browsers can be more identified.']"
18,Web Scraping in a Google Chrome Extension (JavaScript + Chrome APIs),What are the best options for performing Web Scraping of a not currently open tab from within a Google Chrome Extension with JavaScript and whatever more technologies are available. Other JavaScript-...,https://stackoverflow.com/questions/6508393/web-scraping-in-a-google-chrome-extension-javascript-chrome-apis,"['What are the best options for performing Web Scraping of a not currently open tab from within a Google Chrome Extension with JavaScript and whatever more technologies are available. Other JavaScript-libraries are also accepted.', 'Attempt to use XHR2 responseType = ""document"" and fall back on (new DOMParser).parseFromString(responseText, getResponseHeader(""Content-Type"")) with my text/html patch. See https://gist.github.com/1138724 for an example of how I detect responseType = ""document support (synchronously checking response === null on an object URL created from a text/html blob).', ""If you are fine looking at something beyond a Google Chrome Plugin, look at phantomjs which uses Qt-Webkit in background and runs just like a browser incuding making ajax requests. You can call it a headless browser as it doesn't display the output on a screen and can quitely work in background while you are doing other stuff. If you want, you can export out images, pdf out of the pages it fetches. It provides JS interface to load pages, clicking on buttons etc much like you have in a browser. You can also inject custom JS for example jQuery on any of the pages you want to scrape and use it to access the dom and export out desired data. As its using Webkit its rendering behaviour is exactly like Google Chrome."", 'A lot of tools have been released since this question was asked.', 'Web scraping is kind of convoluted in a Chrome Extension. Some points:', ""I'm not sure it's entirely possible with just JavaScript, but if you can set up a dedicated PHP script for your extension that uses cURL to fetch the HTML for a page, the PHP script could scrape the page for you and your extension could read it in through an AJAX request."", 'I think you can start from this example.', ""couldn't you just do some iframe trickery? if you load the url into a dedicated frame, you have the dom in a document object and can do your jquery selections, no?""]"
19,How to run Scrapy from within a Python script,"I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:

http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/

http://snipplr.com/...",https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script,"[""I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:"", 'All other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:', ""Though I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:"", 'In scrapy 0.19.x you should do this:', 'Simply we can use', 'When there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted.', 'it  is an improvement of\nScrapy throws an error when run using crawlerprocess', 'Put this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6)', ""If you want to run a simple crawling, It's easy by just running command:""]"
20,Is it ok to scrape data from Google results? [closed],"I'd like to fetch results from Google using curl to detect potential duplicate content.
Is there a high risk of being banned by Google?",https://stackoverflow.com/questions/22657548/is-it-ok-to-scrape-data-from-google-results,"['Want to improve this question? Update the question so it focuses on one problem only by editing this post.', 'Google will eventually block your IP when you exceed a certain amount of requests.', 'Google disallows automated access in their TOS, so if you accept their terms you would break them.', 'Google thrives on scraping websites of the world...so if it was ""so illegal"" then even Google won\'t survive ..of course other answers mention ways of mitigating IP blocks by Google. One more way to explore avoiding captcha could be scraping at random times (dint try) ..Moreover, I have a feeling, that if we provide novelty or some significant processing of data then it sounds fine at least to me...if we are simply copying a website.. or hampering its business/brand in some way...then it is bad and should be avoided..on top of it all...if you are a startup then no one will fight you as there is no benefit.. but if your entire premise is on scraping even when you are funded then you should think of more sophisticated ways...alternative APIs..eventually..Also Google keeps releasing (or depricating)  fields for its API so what you want to scrap now may be in roadmap of new Google API releases..']"
21,How to manage a 'pool' of PhantomJS instances,"I'm planning a webservice for my own use internally that takes one argument, a URL, and returns html representing the resolved DOM from that URL. By resolved I mean that the webservice will firstly ...",https://stackoverflow.com/questions/9961254/how-to-manage-a-pool-of-phantomjs-instances,"[""I'm planning a webservice for my own use internally that takes one argument, a URL, and returns html representing the resolved DOM from that URL. By resolved I mean that the webservice will firstly get the page at that URL, then use PhantomJS to 'render' the page, and then return the resulting source after all DHTML, AJAX calls etc are executed. However launching phantom on a per-request basis (which I'm doing now) is way too sluggish. I would rather have a pool of PhantomJS instances with one always available to serve the latest call to my webservice."", 'I setup a PhantomJs Cloud Service, and it pretty much does what you are asking.  It took me about 5 weeks of work implement.', 'The async JavaScript library works in Node and has a queue function that is quite handy for this kind of thing:', 'For my master thesis, I developed the library phantomjs-pool which does exactly this. It allows to provide jobs which are then mapped to PhantomJS workers. The library handles the job distribution, communication, error handling, logging, restarting and some more stuff. The library was successfully used to crawl more than one million pages.', 'As an alternative to @JasonS great answer you can try PhearJS, which I built. PhearJS is a supervisor written in NodeJS for PhantomJS instances and provides an API via HTTP. It is available open-source from Github.', 'if you are using nodejs why not use selenium-webdriver', 'If you are using nodejs, you can use https://github.com/sgentle/phantomjs-node, which will allow you to connect an arbitrary number of phantomjs process to your main NodeJS process, hence, the ability to use async.js and lots of node goodies.']"
22,Scrape web pages in real time with Node.js,"What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several ...",https://stackoverflow.com/questions/5211486/scrape-web-pages-in-real-time-with-node-js,"[""What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several different sites, the results scraped, and returned to the client as they become available."", 'Node.io seems to take the cake :-)', 'All aforementioned solutions presume running the scraper locally. This means you will be severely limited in performance (due to running them in sequence or in a limited set of threads). A better approach, imho, is to rely on an existing, albeit commercial, scraping grid.', ""I've been doing research myself, and https://npmjs.org/package/wscraper boasts itself as a"", ""You don't always need to jQuery. If you play with the DOM returned from jsdom for example you can easily take what you need yourself (also considering you dont have to worry about xbrowser issues.) See: https://gist.github.com/1335009 that's not taking away from node.io at all, just saying you might be able to do it yourself depending..."", ""Usually when you're scraping you want to use some method to"", 'It is my easy to use general purpose scrapper https://github.com/harish2704/html-scrapper written for Node.JS\nIt can extract information based on predefined schemas.\nA schema defnition includes a css selector and a data extraction function.\nIt currently using cheerio for dom parsing..', 'check out https://github.com/rc0x03/node-promise-parser', ""I see most answers the right path with cheerio and so forth, however once you get to the point where you need to parse and execute JavaScript (ala SPA's and more), then I'd check out https://github.com/joelgriffith/navalia (I'm the author). Navalia is built to support scraping in a headless-browser context, and it's pretty quick. Thanks!""]"
23,Extracting an information from web page by machine learning,"I would like to extract a specific type of information from web pages in Python. Let's say postal address. It has thousands of forms, but still, it is somehow recognizable. As there is a large number ...",https://stackoverflow.com/questions/13336576/extracting-an-information-from-web-page-by-machine-learning,"[""I would like to extract a specific type of information from web pages in Python. Let's say postal address. It has thousands of forms, but still, it is somehow recognizable. As there is a large number of forms, it would be probably very difficult to write regular expression or even something like a grammar and to use a parser generator for parsing it out."", 'First, your task fits into the information extraction area of research. There are mainly 2 levels of complexity for this task:', ""tl;dr: The problem might solvable using ML, but it's not straightforward if you're new to the topic"", 'As per i know there are two ways to do this task using machine learning approach.', 'Firstly, Machine Learning is not magic. These algorithms perform specific tasks, even if these can be a bit complex sometimes.', ""I would suggest you look at the field of information extraction. A lot of people have been researching how to do exactly what you're asking. There are some techniques for information extraction that are machine learning based, some techniques that are not machine learning based."", 'The approach needs to be a supervised learning algorithm (typically, they yield much better results than unsupervised or semi-supervised methods). Also, notice that you need to basically extract chunks of text. Intuitively, your algorithm needs to say something like, ""from this character onward, for the next three lines, is a postal address"".', 'I had built a solution exactly for this. My goal was to extract all the information related to competitions available on the internet. I used a tweak.\nWhat I did is that I detected the pattern in which the information are listed on the websites. In my case, they were listed one by one below the order,\nI detected that using the html table tags and got the information related to \nthe competitions.']"
24,crawler vs scraper,Can somebody distinguish between a crawler and scraper in terms of scope and functionality.,https://stackoverflow.com/questions/3207418/crawler-vs-scraper,"['Can somebody distinguish between a crawler and scraper in terms of scope and functionality.', 'A crawler gets web pages -- i.e., given a starting address (or set of starting addresses) and some conditions (e.g., how many links deep to go, types of files to ignore) it downloads whatever is linked to from the starting point(s).', ""Crawlers surf the web, following links.  An example would be the Google robot that gets pages to index.  Scrapers extract values from forms, but don't necessarily have anything to do with the web."", 'Web crawler gets links (Urls - Pages) in a logic and scrapper get values (extracting) from HTML.', 'Generally, crawlers would follow the links to reach numerous pages while scrapers is, in some sense, just pulling the contents displayed online and would not reach the deeper links.', 'Scrapers and crawlers do not always distinguish, I mean - you can find crawlers which scrape, in fact, Scraper Crawler is doing both and is named accordingly:']"
25,What should I use to open a url instead of urlopen in urllib3,"I wanted to write a piece of code like the following:

from bs4 import BeautifulSoup
import urllib2

url = 'http://www.thefamouspeople.com/singers.php'
html = urllib2.urlopen(url)
soup = BeautifulSoup(...",https://stackoverflow.com/questions/36516183/what-should-i-use-to-open-a-url-instead-of-urlopen-in-urllib3,"['I wanted to write a piece of code like the following:', 'urllib3 is a different library from urllib and urllib2. It has lots of additional features to the urllibs in the standard library, if you need them, things like re-using connections. The documentation is here: https://urllib3.readthedocs.org/', 'You do not have to install urllib3. You can choose any HTTP-request-making library that fits your needs and feed the response to BeautifulSoup. The choice is though usually requests because of the rich feature set and convenient API. You can install requests by entering pip install requests in the command line. Here is a basic example:', 'The new urllib3 library has a nice documentation here\nIn order to get your desired result you shuld follow that:', 'With gazpacho you could pipeline the page straight into a parse-able soup object:']"
26,Save and render a webpage with PhantomJS and node.js,"I'm looking for an example of requesting a webpage, waiting for the JavaScript to render (JavaScript modifies the DOM), and then grabbing the HTML of the page.

This should be a simple example with an ...",https://stackoverflow.com/questions/9966826/save-and-render-a-webpage-with-phantomjs-and-node-js,"[""I'm looking for an example of requesting a webpage, waiting for the JavaScript to render (JavaScript modifies the DOM), and then grabbing the HTML of the page."", ""From your comments, I'd guess you have 2 options"", ""With v2 of phantomjs-node it's pretty easy to print the HTML after it has been processed."", ""I've used two different ways in the past, including the page.evaluate() method that queries the DOM that Declan mentioned. The other way I've passed info from the web page is to spit it out to console.log() from there, and in the phantomjs script use:"", 'Why not just use this ?', 'Late update in case anyone stumbles on this question:', ""Here's an old version that I use running node, express and phantomjs which saves out the page as a .png. You could tweak it fairly quickly to get the html.""]"
27,"How to “scan” a website (or page) for info, and bring it into my program?","Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java). 

For example, if I know the exact page I want info from, for the sake of ...",https://stackoverflow.com/questions/2835505/how-to-scan-a-website-or-page-for-info-and-bring-it-into-my-program,"[""Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java)."", ""Use a HTML parser like Jsoup. This has my preference above the other HTML parsers available in Java since it supports jQuery like CSS selectors. Also, its class representing a list of nodes, Elements, implements Iterable so that you can iterate over it in an enhanced for loop (so there's no need to hassle with verbose Node and NodeList like classes in the average Java DOM parser)."", ""This is referred to as screen scraping, wikipedia has this article on the more specific web scraping. It can be a major challenge because there's some ugly, mess-up, broken-if-not-for-browser-cleverness HTML out there, so good luck."", ""I would use JTidy - it is simlar to JSoup, but I don't know JSoup well. JTidy handles broken HTML and returns a w3c Document, so you can use this as a source to XSLT to extract the content you are really interested in. If you don't know XSLT, then you might as well go with JSoup, as the Document model is nicer to work with than w3c."", 'You may use an html parser (many useful links here: java html parser).', 'jsoup supports java 1.5', ""You'd probably want to look at the HTML to see if you can find strings that are unique and near your text, then you can use line/char-offsets to get to the data."", 'You could also try jARVEST.', ""My answer won't probably be useful to the writer of this question (I am 8 months late so not the right timing I guess) but I think it will probably be useful for many other developers that might come across this answer."", 'JSoup solution is great, but if you need to extract just something really simple it may be easier to use regex or String.indexOf', ""Look into the cURL library.  I've never used it in Java, but I'm sure there must be bindings for it.  Basically, what you'll do is send a cURL request to whatever page you want to 'scrape'.  The request will return a string with the source code to the page.  From there, you will use regex to parse whatever data you want from the source code.  That's generally how you are going to do it.""]"
28,How to scrape a website which requires login using python and beautifulsoup?,"If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login. 
...",https://stackoverflow.com/questions/23102833/how-to-scrape-a-website-which-requires-login-using-python-and-beautifulsoup,"['If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login.', 'You can use mechanize:', 'You can use selenium to log in and retrieve the page source, which you can then pass to Beautiful Soup to extract the data you want.', 'There is a simpler way, from my pov, that gets you there without selenium or mechanize, or other 3rd party tools, albeit it is semi-automated.', 'If you go for selenium, then you can do something like below:', ""Since Python version wasn't specified, here is my take on it for Python 3, done without any external libraries (StackOverflow). After login use BeautifulSoup as usual, or any other kind of scraping.""]"
29,Simple jQuery selector only selects first element in Chrome..?,"I'm a bit new to jQuery so forgive me for being dense. I want to select all <td> elements on a particular page via Chrome's JS console:

$('td')
Yet when I do this, I get the following output:

...",https://stackoverflow.com/questions/14308588/simple-jquery-selector-only-selects-first-element-in-chrome,"[""I'm a bit new to jQuery so forgive me for being dense. I want to select all <td> elements on a particular page via Chrome's JS console:"", ""If jQuery isn't present on the webpage, and of course no other code assigns something to $, Chrome's JS console assigns $ a shortcut to document.querySelector()."", 'It seems jQuery is not properly included to run on your target page. I had a similar problem and solved as follows  for Google Chrome.', ""If jQuery is installed and if the $ symbol is shorthand for jQuery, then $('td') returns a jQuery object.  But, in the w3schools page you linked, I don't see that jQuery is even present."", ""Also if you are trying to do something with each of the td's you would need to use a .each() to loop through them. For example:""]"
30,How do you scrape AJAX pages?,Please advise how to scrape AJAX pages.,https://stackoverflow.com/questions/260540/how-do-you-scrape-ajax-pages,"['Please advise how to scrape AJAX pages.', 'Overview:', 'In my opinion the simpliest solution is to use Casperjs, a framework based on the WebKit headless browser phantomjs.', 'If you can get at it, try examining the DOM tree. Selenium does this as a part of testing a page. It also has functions to click buttons and follow links, which may be useful.', 'The best way to scrape web pages using Ajax or in general pages using Javascript is with a browser itself or a headless browser (a browser without GUI). Currently phantomjs is a well promoted headless browser using WebKit. An alternative that I used with success is HtmlUnit (in Java or .NET via IKVM, which is a simulated browser. Another known alternative is using a web automation tool like Selenium.', ""Depends on the ajax page.  The first part of screen scraping is determining how the page works.  Is there some sort of variable you can iterate through to request all the data from the page?  Personally I've used Web Scraper Plus for a lot of screen scraping related tasks because it is cheap, not difficult to get started, non-programmers can get it working relatively quickly."", 'I like PhearJS, but that might be partially because I built it.', 'I think Brian R. Bondy\'s answer is useful when the source code is easy to read. I prefer an easy way using tools like Wireshark or HttpAnalyzer to capture the packet and get the url from  the ""Host"" field and the ""GET"" field.', 'As a low cost solution you can also try SWExplorerAutomation (SWEA).  The program creates an automation API for any Web application developed with HTML, DHTML or AJAX.', 'Selenium WebDriver is a good solution: you program a browser and you automate what needs to be done in the browser. Browsers (Chrome, Firefox, etc) provide their own drivers that work with Selenium. Since it works as an automated REAL browser, the pages (including javascript and Ajax) get loaded as they do with a human using that browser.', ""I have previously linked to MIT's solvent and EnvJS as my answers to scrape off Ajax pages. These projects seem no longer accessible.""]"
31,How to print an exception in Python 3?,"Right now, I catch the exception in the except Exception: clause, and do print(exception). The result provides no information since it always prints <class 'Exception'>. I knew this used to work ...",https://stackoverflow.com/questions/41596810/how-to-print-an-exception-in-python-3,"[""Right now, I catch the exception in the except Exception: clause, and do print(exception). The result provides no information since it always prints <class 'Exception'>. I knew this used to work in python 2, but how do I do it in python3?"", ""I'm guessing that you need to assign the Exception to a variable. As shown in the Python 3 tutorial:"", 'These are the changes since python 2:', 'Try', 'Here is the way I like that prints out all of the error stack.', ""I've use this :"", 'Although if you want a code that is compatible with both python2 and python3 you can use this:', '[In Python3]']"
32,Java HTML Parsing [closed],I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS ...,https://stackoverflow.com/questions/238036/java-html-parsing,"[""I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS class - Currently (for testing purposes) I'm just checking for"", 'Several years ago I used JTidy for the same purpose:', 'Another library that might be useful for HTML processing is jsoup.\nJsoup tries to clean malformed HTML and allows html parsing in Java using jQuery like tag selector syntax.', 'The main problem as stated by preceding coments is malformed HTML, so an html cleaner or HTML-XML converter is a must. Once you get the XML code (XHTML) there are plenty of tools to handle it. You could get it with a simple SAX handler that extracts only the data you need or any tree-based method (DOM, JDOM, etc.) that let you even modify original code.', 'You might be interested by TagSoup, a Java HTML parser able to handle malformed HTML. XML parsers would work only on well formed XHTML.', 'The HTMLParser project (http://htmlparser.sourceforge.net/) might be a possibility.  It seems to be pretty decent at handling malformed HTML.  The following snippet should do what you need:', 'Jericho: http://jericho.htmlparser.net/docs/index.html', ""Let's not forget Jerry, its jQuery in java: a fast and concise Java Library that simplifies HTML document parsing, traversing and manipulating; includes usage of css3 selectors."", 'HTMLUnit might be of help. It does a lot more stuff too.', ""The nu.validator project is an excellent, high performance HTML parser that doesn't cut corners correctness-wise."", 'You can also use XWiki HTML Cleaner:', ""If your HTML is well-formed, you can easily employ an XML parser to do the job for you... If you're only reading, SAX would be ideal.""]"
33,Web Scraping With Haskell,"What is the current state of libraries for scraping websites with Haskell?

I'm trying to make myself do more of my quick oneoff tasks in Haskell, in order to help increase my comfort level with the ...",https://stackoverflow.com/questions/4838138/web-scraping-with-haskell,"['What is the current state of libraries for scraping websites with Haskell?', 'From my searching on the Haskell mailing lists, it appears that TagSoup is the dominant choice for parsing pages. For example:\nhttp://www.haskell.org/pipermail/haskell-cafe/2008-August/045721.html', 'http://hackage.haskell.org/package/shpider', ""Although I'm still for now a beginner in Haskell, I have the strong opinion that HTML parsing in 2012 must be done using CSS selectors, and it seems the libraries recommended so far don't use that principle."", 'I wrote another answer to this question already, suggesting CSS selectors-based parsing, however that answer is now a year and a half old, and nowadays I think lenses might be a better approach in haskell. In effect you get something like type-safe compiled selectors.']"
34,Change IP address dynamically?,"Consider the case, 
I want to crawl websites frequently, but my IP address got blocked after some day/limit.

So, how can change my IP address dynamically or any other ideas?",https://stackoverflow.com/questions/28852057/change-ip-address-dynamically,"['Consider the case, \nI want to crawl websites frequently, but my IP address got blocked after some day/limit.', 'An approach using Scrapy will make use of two components, RandomProxy and RotateUserAgentMiddleware.', 'You can try using proxy servers to prevent being blocked. There are services providing working proxies. The best I tried is https://gimmeproxy.com - they frequently check proxies for various parameters.', ""If you are using R, you could do the web crawling through TOR. I think TOR resets its IP-adress every 10 minutes(?) automatically. I think there is a way forcing TOR to change the IP in shorter intervals, but that didn't work for me. Instead you could set up multiple instances of TOR and then switch between the independent instances (here you can find a good explaination of how to set up multiple instances of TOR: https://tor.stackexchange.com/questions/2006/how-to-run-multiple-tor-browsers-with-different-ips)"", 'Some VPN applications allow you to automatically change your IP address to a new random IP address at a set interval such as: every 2 minutes. Both HMA! Pro VPN and VPN4ALL software support this feature.', 'Word of warning about VPNs, check their Terms and Conditions carefully because scraping using them goes against their user policy ( One such example would be Astrill). I tried a scraping tool and got my account locked', 'If you have public IPs. Add them on your interface and if you are using Linux use Iptables for switching those public IPs.']"
35,"Selenium-Debugging: Element is not clickable at point (X,Y)","I try to scrape this site by Selenium.

I want to click in ""Next Page"" buttom, for this I do:

 driver.find_element_by_class_name('pagination-r').click()
it works for many pages but not for all, I ...",https://stackoverflow.com/questions/37879010/selenium-debugging-element-is-not-clickable-at-point-x-y,"['I try to scrape this site by Selenium.', 'Another element is covering the element you are trying to click. You could use execute_script() to click on this.', 'I had a similar issue where using ActionChains was not solving my error:\nWebDriverException: Message: unknown error: Element is not clickable at point (5\n74, 892)', 'I have written logic to handle these type of exception .', 'Use explicit wait instead of implicit.', 'If you are receiving an element not clickable error, even after using wait on the element, try one of these workarounds:']"
36,Using BeautifulSoup to extract text without tags,"My webpage looks like this:

<p>
  <strong class=""offender"">YOB:</strong> 1987<br/>
  <strong class=""offender"">RACE:</strong> WHITE<br/>
  <strong class=""...",https://stackoverflow.com/questions/23380171/using-beautifulsoup-to-extract-text-without-tags,"['My webpage looks like this:', 'Just loop through all the <strong> tags and use next_sibling to get what you want. Like this:', 'I think you can get it using subc1.text.', 'you can try this indside findall for loop:', 'I think you could solve this with .strip() in gazpacho:']"
37,Converting html to text with Python,"I am trying to convert an html block to text using Python. 

Input:

<div class=""body""><p><strong></strong></p>
<p><strong></strong>Lorem ipsum dolor ...",https://stackoverflow.com/questions/14694482/converting-html-to-text-with-python,"['I am trying to convert an html block to text using Python.', 'What am I missing? soup.get_text() gives exactly the same output you wanted...', ""It's possible using python standard html.parser:"", 'You can use regular expression... but not recommended...', ""The '\\n' places a newline between the paragraphs."", ""I liked @FrBrGeorge's no dependency answer so much that I expanded it to only extract the body tag and added a convenience method so that HTML to text is a single line:"", ""I was in need of a way of doing this on a client's system without having to download additional libraries. I never found a good solution, so I created my own. Feel free to use this if you like."", ""It's possible to use BeautifulSoup to remove unwanted scripts and similar, though you may need to experiment with a few different sites to make sure you've covered the different types of things you wish to exclude.  Try this:"", 'There are some nice things here, and i might as well throw in my solution:', 'gazpacho might be a good choice for this!']"
38,Web scraping - how to identify main content on a webpage,"Given a news article webpage (from any major news source such as times or bloomberg), I want to  identify the main article content on that page and throw out the other misc elements such as ads, menus,...",https://stackoverflow.com/questions/4672060/web-scraping-how-to-identify-main-content-on-a-webpage,"['Given a news article webpage (from any major news source such as times or bloomberg), I want to  identify the main article content on that page and throw out the other misc elements such as ads, menus, sidebars, user comments.', ""There's no way to do this that's guaranteed to work, but one strategy you might use is to try to find the element with the most visible text inside of it."", 'There are a number of ways to do it, but, none will always work.  Here are the two easiest:', ""A while ago I wrote a simple Python script for just this task. It uses a heuristic to group text blocks together based on their depth in the DOM. The group with the most text is then assumed to be the main content. It's not perfect, but works generally well for news sites, where the article is generally the biggest grouping of text, even if broken up into multiple div/p tags."", ""Diffbot offers a free(10.000 urls) API to do that, don't know if that approach is what you are looking for, but it might help someone http://www.diffbot.com/"", 'For a solution in Java have a look at https://code.google.com/p/boilerpipe/ :', 'It might be more useful to extract the RSS feeds (<link type=""application/rss+xml"" href=""...""/>) on that page and parse the data in the feed to get the main content.', 'Check the following script. It is really amazing:', 'Another possibility of separating ""real"" content from noise is by measuring HTML density of the parts of a HTML page.', 'There is a recent (early 2020) comparison of various methods of extracting article body, without and ads, menus, sidebars, user comments, etc. - see https://github.com/scrapinghub/article-extraction-benchmark. A report, data and evaluation scripts are available. It compares many options mentioned in the answers here, as well as some options which were not mentioned:', ""I wouldn't try to scrape it from the web page - too many things could mess it up - but instead see which web sites publish RSS feeds. For example, the Guardian's RSS feed has most of the text from their leading articles:""]"
39,How to manage log in session through headless chrome?,"I want to create a scraper that:

opens a headless browser,
goes to a url,
logs in (there is steam oauth),
fills some inputs,
and clicks 2 buttons.

My problem is that every new instance of headless ...",https://stackoverflow.com/questions/48608971/how-to-manage-log-in-session-through-headless-chrome,"['I want to create a scraper that:', 'In puppeter you have access to the session cookies through page.cookies().', 'There is an option to save user data using the userDataDir option when launching puppeteer. This stores the session and other things related to launching chrome.', ""For a version of the above solution that actually works and doesn't rely on jsonfile (instead using the more standard fs) check this out:"", 'For writing Cookies']"
40,csv.writer writing each character of word in separate column/cell,"Objective: To extract the text from the anchor tag inside all lines in models and put it in a csv.

I'm trying this code: 

with open('Sprint_data.csv', 'ab') as csvfile:
  spamwriter = csv.writer(...",https://stackoverflow.com/questions/15129567/csv-writer-writing-each-character-of-word-in-separate-column-cell,"['Objective: To extract the text from the anchor tag inside all lines in models and put it in a csv.', ""writerow accepts a sequence.  You're giving it a single string, so it's treating that as a sequence, and strings act like sequences of characters."", "".writerow() requires a sequence ('', (), []) and places each index in it's own column of the row, sequentially.  If your desired string is not an item in a sequence, writerow() will iterate over each letter in your string and each will be written to your CSV in a separate cell."", 'This is usually the solution I use:', 'Just surround it with a list sign (i.e [])']"
41,Using python Requests with javascript pages,"I am trying to use the Requests framework with python (http://docs.python-requests.org/en/latest/) but the page I am trying to get to uses javascript to fetch the info that I want. 

I have tried to ...",https://stackoverflow.com/questions/26393231/using-python-requests-with-javascript-pages,"['I am trying to use the Requests framework with python (http://docs.python-requests.org/en/latest/) but the page I am trying to get to uses javascript to fetch the info that I want.', 'You are going to have to make the same request (using the Requests library) that the javascript is making.  You can use any number of tools (including those built into Chrome and Firefox) to inspect the http request that is coming from javascript and simply make this request yourself from Python.', 'Good news: there is now a requests module that supports javascript:  https://pypi.org/project/requests-html/', ""While Selenium might seem tempting and useful, it has one main problem that can't be fixed: performance. By calculating every single thing a browser does, you will need a lot more power. Even PhantomJS does not compete with a simple request. I recommend that you will only use Selenium when you really need to click buttons. If you only need javascript, I recommend PyQt (check https://www.youtube.com/watch?v=FSH77vnOGqU to learn it)."", 'its a wrapper around pyppeteer or smth? :( i thought its something different']"
42,Click a Button in Scrapy,"I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).

I found out that Scrapy ...",https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy,"[""I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking)."", 'Scrapy cannot interpret javascript.', 'Selenium browser provide very nice solution. Here is an example (pip install -U selenium):', 'To properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.']"
43,Scrape An Entire Website [closed],"I'm looking for recommendations for a program to scrape and download an entire corporate website.

The site is powered by a CMS that has stopped working and getting it fixed is expensive and we are ...",https://stackoverflow.com/questions/9265172/scrape-an-entire-website,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", ""Consider HTTrack. It's a free and easy-to-use offline browser utility."", 'Read more about it here.', 'None of the above got exactly what I needed (the whole site and all assets). This worked though.', 'I know this is super old and I just wanted to put my 2 cents in.', ""The best way is to scrape it with wget as suggested in @Abhijeet Rastogi's answer.  If you aren't familiar with is then Blackwidow is a decent scraper.  I've used it in the past.  http://www.sbl.net/""]"
44,How to scroll down with Phantomjs to load dynamic content,I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able ...,https://stackoverflow.com/questions/16561582/how-to-scroll-down-with-phantomjs-to-load-dynamic-content,"['I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-', ""Found a way to do it and tried to adapt to your situation. I didn't test the best way of finding the bottom of the page because I had a different context, but check the solution below. The thing here is that you have to wait a little for the page to load and javascript works asynchronously so you have to use setInterval or setTimeout (see) to achieve this."", 'I know that it has been answered a long time ago, but I also found a solution to my specific scenario. The result is a piece of javascript that scrolls to the bottom of the page. It is optimized to reduce waiting time.', 'The ""correct"" solution didn\'t work for me. And, from what I\'ve read CasperJS doesn\'t use window (but I may be wrong on that), which makes me doubt that window works.', 'The code snippet below work just fine for pinterest. I researched a lot to scrape pinterest without phantomjs but it is impossible to find the infinite scroll trigger link. I think the code below will help other infinite scroll web page to scrape.']"
45,How to get the scrapy failure URLs?,"I'm a newbie of scrapy and it's amazing crawler framework i have known! 

In my project, I sent more than 90, 000 requests, but there are some of them failed. 
I set the log level to be INFO, and i ...",https://stackoverflow.com/questions/13724730/how-to-get-the-scrapy-failure-urls,"[""I'm a newbie of scrapy and it's amazing crawler framework i have known!"", 'Yes, this is possible.', ""Here's another example how to handle and collect 404 errors (checking github help pages):"", 'Scrapy ignores 404 by default and does not parse it. If you are getting an error code 404 in response, you can handle this with a very easy way.', 'The answers from @Talvalin and @alecxe helped me a great deal, but they do not seem to capture downloader events that do not generate a response object (for instance, twisted.internet.error.TimeoutError and twisted.web.http.PotentialDataLoss). These errors show up in the stats dump at the end of the run, but without any meta info.', ""As of scrapy 0.24.6, the method suggested by alecxe won't catch errors with the start URLs. To record errors with the start URLs you need to override parse_start_urls. Adapting alexce's answer for this purpose, you'd get:"", ""This is an update on this question. I ran in to a similar problem and needed to use the scrapy signals to call a function in my pipeline. I have edited @Talvalin's code, but wanted to make an answer just for some more clarity."", ""In addition to some of these answers, if you want to track Twisted errors, I would take a look at using the Request object's errback parameter, on which you can set a callback function to be called with the Twisted Failure on a request failure.\nIn addition to the url, this method can allow you to track the type of failure."", 'You can capture failed urls in two ways.', 'Basically Scrapy Ignores 404 Error by Default, It was defined in httperror middleware.']"
46,Python: Disable images in Selenium Google ChromeDriver,"I spend a lot of time searching about this.
At the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier ...",https://stackoverflow.com/questions/28070315/python-disable-images-in-selenium-google-chromedriver,"[""I spend a lot of time searching about this.\nAt the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this."", 'Here is another way to disable images:', 'Java:\nWith this Chrome nor Firefox would load images. The syntax is different but the strings on the parameters are the same.', 'There is another way that comes probably to mind to everyone to access chrome://settings and then go through the settings with selenium I started this way just for didactic curiosity, but then I hit a forest of shadow-roots elements now when you encounter more than 3 shadow root element combined with dynamic content is clearly a way to obfuscate and make it impossible to automate, although might sound at least theoretically possible this approach looks more like a dead end, I will leave this answer with the example code, just for purely learning purposes to advert the people tempted to go to the challenge..  Not only was hard to find just the content settings due to the shadowroots and dynamic change when you find the button is not clickable at this point.']"
47,Get meta tag content property with BeautifulSoup and Python,"I am trying to use python and beautiful soup to extract the content part of the tags below:

<meta property=""og:title"" content=""Super Fun Event 1"" />
<meta property=""og:url"" content=""http://...",https://stackoverflow.com/questions/36768068/get-meta-tag-content-property-with-beautifulsoup-and-python,"['I am trying to use python and beautiful soup to extract the content part of the tags below:', 'Provide the meta tag name as the first argument to find(). Then, use keyword arguments to check the specific attributes:', 'try this :', 'A way I like to solve this is as follows:\n(Is neater when using with lists of properties to look up...)', 'You could grab the content inside the meta tag with gazpacho:']"
48,How can I download a file on a click event using selenium?,"I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.  

from selenium import webdriver
from selenium.common.exceptions import ...",https://stackoverflow.com/questions/18439851/how-can-i-download-a-file-on-a-click-event-using-selenium,"['I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.', 'Find the link using find_element(s)_by_*, then call click method.', 'I\'ll admit this solution is a little more ""hacky"" than the Firefox Profile saveToDisk alternative, but it works across both Chrome and Firefox, and doesn\'t rely on a browser-specific feature which could change at any time. And if nothing else, maybe this will give someone a little different perspective on how to solve future challenges.', 'In chrome what I do is downloading the files by clicking on the links, then I open chrome://downloads page and then retrieve the downloaded files list from shadow DOM like this:', 'Here is the full working code. You can use web scrapping to enter the username password and other field. For getting the field names appearing on the webpage, use inspect element. Element name(Username,Password or Click Button) can be entered through class or name.']"
49,Scraping a JSON response with Scrapy,"How do you use Scrapy to scrape web requests that return JSON? For example, the JSON would look like this:

{
    ""firstName"": ""John"",
    ""lastName"": ""Smith"",
    ""age"": 25,
    ""address"": {
        ""...",https://stackoverflow.com/questions/18171835/scraping-a-json-response-with-scrapy,"['How do you use Scrapy to scrape web requests that return JSON? For example, the JSON would look like this:', ""It's the same as using Scrapy's HtmlXPathSelector for html responses. The only difference is that you should use json module to parse the response:"", 'The possible reason JSON is not loading is that it has single-quotes before and after. Try this:']"
50,Scraping dynamic content using python-Scrapy,"Disclaimer: I've seen numerous other similar posts on StackOverflow and tried to do it the same way but was they don't seem to work on this website.

I'm using Python-Scrapy for getting data from ...",https://stackoverflow.com/questions/30345623/scraping-dynamic-content-using-python-scrapy,"[""Disclaimer: I've seen numerous other similar posts on StackOverflow and tried to do it the same way but was they don't seem to work on this website."", 'You can also solve it with ScrapyJS (no need for selenium and a real browser):', 'From what I understand, the size availability is determined dynamically in javascript being executed in the browser. Scrapy is not a browser and cannot execute javascript.', 'I faced that problem and solved easily by following these steps', 'You have to interpret the json of the website, examples\nscrapy.readthedocs and \ntestingcan.github.io']"
51,BeautifulSoup: extract text from anchor tag,"I want to extract:
text from following src of the image tag and
text of the anchor tag which is inside the div class data
I successfully manage to extract the img src, but am having trouble ...",https://stackoverflow.com/questions/11716380/beautifulsoup-extract-text-from-anchor-tag,"['I want to extract:', 'This will help:', 'In my case, it worked like that:', 'I would suggest going the lxml route and using xpath.', 'All the above answers really help me to construct my answer, because of this I voted for all the answers that other users put it out: But I finally put together my own answer to exact problem I was dealing with:']"
52,Headless browser for C# (.NET)? [closed],"I am (was) a Python developer who is building a GUI web scraping application. Recently I've decided to migrate to .NET framework and write the same application in C# (this decision wasn't mine).

In ...",https://stackoverflow.com/questions/10161413/headless-browser-for-c-sharp-net,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'There are some options:', 'More solutions:', 'You may be after TrifleJS (currently in beta), or something similar using the .NET WebBrowser class which communicates with IE via a windowless ActiveX/COM API.']"
53,How to find tag with particular text with Beautiful Soup?,"I have the following html (line breaks marked with \n):

...
<tr>
  <td class=""pos"">\n
      ""Some text:""\n
      <br>\n
      <strong>some value</strong>\n
  </td>
...",https://stackoverflow.com/questions/9007653/how-to-find-tag-with-particular-text-with-beautiful-soup,"['I have the following html (line breaks marked with \\n):', 'You can pass a regular expression to the text parameter of findAll, like so:', 'This post got me to my answer even though the answer is missing from this post. I felt I should give back.', 'With bs4 4.7.1+ you can use :contains pseudo class to specify the td containing your search string', 'Since Beautiful Soup 4.4.0. a parameter called string does the work that text used to do in the previous versions.', 'A solution for finding a anchor tag if having a particular keyword would be the following:', 'You could solve this with some simple gazpacho parsing:']"
54,How to scrape a website that requires login first with Python,"First of all, I think it's worth saying that, I know there are a bunch of similar questions but NONE of them works for me...

I'm a newbie on Python, html and web scraper. I'm trying to scrape user ...",https://stackoverflow.com/questions/20039643/how-to-scrape-a-website-that-requires-login-first-with-python,"[""First of all, I think it's worth saying that, I know there are a bunch of similar questions but NONE of them works for me..."", 'This works for me:', 'would love to add my solution alongside . this answer  mainly follows the hacky / lazy approach i always follow in everything i do. went on with mainly because , i was too lazy to handle the cookies, session data etc .', 'The classic way to approach this problem is:']"
55,How do I call a Javascript function from Python?,"I am working on a web-scraping project. One of the websites I am working with has the data coming from Javascript.

There was a suggestion on one of my earlier questions that I can directly call the ...",https://stackoverflow.com/questions/8284765/how-do-i-call-a-javascript-function-from-python,"['I am working on a web-scraping project. One of the websites I am working with has the data coming from Javascript.', 'Find a JavaScript interpreter that has Python bindings. (Try Rhino? V8? SeaMonkey?). When you have found one, it should come with examples of how to use it from python.', 'An interesting alternative I discovered recently is the Python bond module, which can be used to communicate with a NodeJs process (v8 engine).', 'To interact with JavaScript from Python I use webkit, which is the browser renderer behind Chrome and Safari. There are Python bindings to webkit through Qt. In particular there is a function for executing JavaScript called evaluateJavaScript().', 'You can eventually get the JavaScript from the page and execute it through some interpreter (such as v8 or Rhino). However, you can get a good result in a way easier way by using some functional testing tools, such as Selenium or Splinter. These solutions launch a browser and effectively load the page - it can be slow but assures that the expected browser displayed content will be available.', 'You might call node through Popen.', 'Did a whole run-down of the different methods recently.']"
56,Is it possible to use Selenium WebDriver to drive PhantomJS?,"I’m going through the documentation for the Selenium WebDriver, and it can drive Chrome for example. I was thinking, wouldn't it be far more efficient to ‘drive’ PhantomJS?

Is there a way to use ...",https://stackoverflow.com/questions/11226648/is-it-possible-to-use-selenium-webdriver-to-drive-phantomjs,"[""I’m going through the documentation for the Selenium WebDriver, and it can drive Chrome for example. I was thinking, wouldn't it be far more efficient to ‘drive’ PhantomJS?"", 'PhantomJS now includes the GhostDriver project.', '@Joseph, since the 1.8 release GhostDriver is included in the stable release of PhantomJS. Here is the exact release notes: http://phantomjs.org/release-1.8.html. \nYou can simply start PhantomJS process to listen on some port, like this:']"
57,Web Scraping with Scala [closed],"Just wondering if anyone knows of a web-scraping library that takes advantage of Scala's succinct syntax. So far, I've found Chafe, but this seems poorly-documented and maintained. I'm wondering if ...",https://stackoverflow.com/questions/14745634/web-scraping-with-scala,"[""Just wondering if anyone knows of a web-scraping library that takes advantage of Scala's succinct syntax. So far, I've found Chafe, but this seems poorly-documented and maintained. I'm wondering if anyone out there has done scraping with Scala and has advice. (I'm trying to integrate into an existing Scala framework rather than use a scraper written in, say, Python.)"", 'First there is a plethora of HTML scraping libs in JVM all you need to do is pimp one of them (pimp my library pattern).', ""I don't have a Scala-specific recommendation, but for the JVM in general I've had good success with:"", ""I'd recommend Goose: https://github.com/jiminoc/goose""]"
58,Clicking on a link via selenium,I am trying to do some webscraping via Selenium.  My question is very simple: How do you find a link and then how do you click on it?  For instance: The following is the HTML that I am trying to web-...,https://stackoverflow.com/questions/18597735/clicking-on-a-link-via-selenium,"['I am trying to do some webscraping via Selenium.  My question is very simple: How do you find a link and then how do you click on it?  For instance: The following is the HTML that I am trying to web-scrape:', 'You can use find_element_by_link_text:', 'Then you can try something like this.', 'You can try to click link by using xpath locator\ne.g.', 'One thing is missed by everyone. Its a list by the below statement. You need to take select an element from this list.']"
59,Scrape a webpage and navigate by clicking buttons,"I want to perform following actions at the server side:

1) Scrape a webpage 
2) Simulate a click on that page and then navigate to the new page. 
3) Scrape the new page 
4) Simulate some button ...",https://stackoverflow.com/questions/18160635/scrape-a-webpage-and-navigate-by-clicking-buttons,"['I want to perform following actions at the server side:', 'Zombie.js and Node.io run on JSDOM, hence your options are either going with JSDOM (or any equivalent wrapper), a headless browser (PhantomJS, SlimerJS) or Cheerio.', 'If you need a full (headless) browser, use puppeteer instead of PhantomJS as it offers an up-to-date Chromium browser with a rich API to automate any browser crawling and scraping tasks. If you only want to parse a HTML document (without executing JavaScript inside the page) you should check out jsdom and cheerio.', 'The modules you listed do the following:']"
60,Python follow redirects and then download the page?,"I have the following python script and it works beautifully. 

import urllib2

url = 'http://abc.com' # write the url here

usock = urllib2.urlopen(url)
data = usock.read()
usock.close()

print data
...",https://stackoverflow.com/questions/8827545/python-follow-redirects-and-then-download-the-page,"['I have the following python script and it works beautifully.', 'You might be better off with Requests library which has better APIs for controlling redirect handling:', 'Use requests as the other answer states, here is an example. The redirect will be in r.url. In the example below the http is redirected to https', ""Do you absolutely have to do it that way? How about using something like twill ( http://twill.idyll.org/) - makes what you want to do very easy (and it's Python).""]"
61,Android Web Scraping with a Headless Browser [closed],"I have spent a day on researching a library that can be used to accomplish the following:
Retrieve the full contents of a webpage like in the background without rendering result to a view.
The lib ...",https://stackoverflow.com/questions/17399055/android-web-scraping-with-a-headless-browser,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'Ok after 2 weeks I admit defeat and are using a workaround which works great for me at the moment.', ""I have taken the implementation mentioned above (injecting JavaScript) and that works for me. All I do is simply set the visibility of the webview to be hidden under other UI elements. I was also thinking of doing the same with selenium. I have used selenium with Chrome in Python and it's great but like you mentioned it is not easy to not show the browser window. But I think it might be possible to just not show the component in Android. I'll have to try.""]"
62,Python - Download Images from google Image search?,"I want to download all Images of google image search using python . The code I am using seems to have some problem some times .My code is 

import os
import sys
import time
from urllib import ...",https://stackoverflow.com/questions/20716842/python-download-images-from-google-image-search,"['I want to download all Images of google image search using python . The code I am using seems to have some problem some times .My code is', 'I have modified my code. Now the code can download 100 images for a given query, and images are full high resolution that is original images are being downloaded.', 'The Google Image Search API is deprecated, you need to use the Google Custom Search for what you want to achieve. To fetch the images you need to do this:', 'Google deprecated their API, scraping Google is complicated, so I would suggest using Bing API instead:', ""Haven't looked into your code but this is an example solution made with selenium to try to get 400 pictures from the search term"", ""Adding to Piees's answer, for downloading any number of images from the search results, we need to simulate a click on 'Show more results' button after first 400 results are loaded."", 'You can also use Selenium with Python. Here is how:', ""Here's my latest google image snarfer, written in Python, using Selenium and headless Chrome."", 'I know this question is old, but I ran across it recently and none of the previous answers work anymore. So I wrote this script to gather images from google. As of right now it can download as many images as are available.']"
63,Html Agility Pack. Load and scrape webpage,"Is this the best way to get a webpage when scraping?

HttpWebRequest oReq = (HttpWebRequest)WebRequest.Create(url);
HttpWebResponse resp = (HttpWebResponse)oReq.GetResponse();

var doc = new ...",https://stackoverflow.com/questions/10558149/html-agility-pack-load-and-scrape-webpage,"['Is this the best way to get a webpage when scraping?', 'Much easier to use HtmlWeb.']"
64,scrape websites with infinite scrolling,"I have written many scrapers but I am not really sure how to handle infinite scrollers. These days most website etc, Facebook, Pinterest has infinite scrollers.",https://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling,"['I have written many scrapers but I am not really sure how to handle infinite scrollers. These days most website etc, Facebook, Pinterest has infinite scrollers.', 'You can use selenium to scrap the infinite scrolling website like twitter or facebook.', 'Most sites that have infinite scrolling do (as Lattyware notes) have a proper API as well, and you will likely be better served by using this rather than scraping.', 'Finding the url of the ajax source will be the best option but it can be cumbersome for certain sites. Alternatively you could use a headless browser like QWebKit from PyQt and send keyboard events while reading the data from the DOM tree. QWebKit has a nice and simple api.']"
65,unable to call firefox from selenium in python on AWS machine,"I am trying to use selenium from python to scrape some dynamics pages with javascript. However, I cannot call firefox after I followed the instruction of selenium on the pypi page(http://pypi.python....",https://stackoverflow.com/questions/13039530/unable-to-call-firefox-from-selenium-in-python-on-aws-machine,"['I am trying to use selenium from python to scrape some dynamics pages with javascript. However, I cannot call firefox after I followed the instruction of selenium on the pypi page(http://pypi.python.org/pypi/selenium). I installed firefox on AWS ubuntu 12.04. The error message I got is:', ""The problem is Firefox requires a display. I've used pyvirtualdisplay in my example to simulate a display. The solution is:"", 'I too had faced same problem.I was on Firefox 47 and Selenium 2.53. So what I did was downgraded Firefox to 45. This worked.', ""This is already in the comment of OP's question, but to lay it out as an answer. You can have Selenium run in the background without opening an actual browser window."", 'For Debian 10 and Ubuntu 18.04 this is a complete running example:']"
66,Is web scraping allowed? [closed],"I'm working on a project that requires certain statistics from another website, and I've created an HTML scraper that gets this data every 15 minutes, automatically. However, I stopped the bot now, as ...",https://stackoverflow.com/questions/32429445/is-web-scraping-allowed,"['Want to improve this question? Update the question so it focuses on one problem only by editing this post.', 'I\'ll quote Pablo Hoffman\'s (Scrapinghub co-founder) answer to ""What is the legality of web scraping?"", I found on other site:', 'There must be robots.txt file in root folder of that site.']"
67,How to handle IncompleteRead: in python,I am trying to fetch some data from a website. However it returns me incomplete read. The data I am trying to get is a huge set of nested links. I did some research online and found that this might be ...,https://stackoverflow.com/questions/14442222/how-to-handle-incompleteread-in-python,"['I am trying to fetch some data from a website. However it returns me incomplete read. The data I am trying to get is a huge set of nested links. I did some research online and found that this might be due to a server error (A chunked transfer encoding finishing before\nreaching the expected size). I also found a workaround for above on this link', ""The link you included in your question is simply a wrapper that executes urllib's read() function, which catches any incomplete read exceptions for you.  If you don't want to implement this entire patch, you could always just throw in a try/catch loop where you read your links.  For example:"", 'I find out in my case : send HTTP/1.0 request , adding this , fix the problem.', 'What worked for me is catching IncompleteRead as an exception and harvesting the data you managed to read in each iteration by putting this into a loop like below: (Note, I am using Python 3.4.1 and the urllib library has changed between 2.7 and 3.4)', 'You can use requests instead of urllib2. requests is based on urllib3 so it rarely have any problem. Put it in a loop to try it 3 times, and it will be much stronger. You can use it this way:', 'I found that my virus detector/firewall was causing this problem.  ""Online Shield"" part of AVG.', 'python3 FYI', 'I tried all these solutions and none of them worked for me. Actually, what did work is instead of using urllib, I just used http.client (Python 3)', 'I just add a more exception to pass this problem.\njust like']"
68,I need a Powerful Web Scraper library [closed],I need a powerful web scraper library  for mining contents from web. That can be paid or free both will be fine for me. Please suggest me a library or better way for mining the data  and store in my ...,https://stackoverflow.com/questions/4377355/i-need-a-powerful-web-scraper-library,"[""I need a powerful web scraper library  for mining contents from web. That can be paid or free both will be fine for me. Please suggest me a library or better way for mining the data  and store in my preferred database. I have searched but i didn't find any good solution for this. I need a good suggestion from experts. Please help me out."", 'Scraping is easy really, you just have to parse the content you are downloading and get all the associated links.', 'For simple websites ( = plain html only), Mechanize works really well and fast. For sites that use Javascript, AJAX or even Flash, you need a real browser solution such as iMacros.', 'My Advice:']"
69,Extract Links from Webpage using R,"The two posts below are great examples of different approaches of extracting data from websites and parsing it into R.  

Scraping html tables into R data frames using the XML package

How can I use R ...",https://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r,"['The two posts below are great examples of different approaches of extracting data from websites and parsing it into R.', ""The documentation for htmlTreeParse shows one method. Here's another:"", 'Even easier with rvest:']"
70,CrawlerProcess vs CrawlerRunner,"Scrapy 1.x documentation explains that there are two ways to run a Scrapy spider from a script:
using CrawlerProcess
using CrawlerRunner
What is the difference between the two? When should I use ""...",https://stackoverflow.com/questions/39706005/crawlerprocess-vs-crawlerrunner,"['Scrapy 1.x documentation explains that there are two ways to run a Scrapy spider from a script:', ""Scrapy's documentation does a pretty bad job at giving examples on real applications of both."", 'CrawlerRunner:']"
71,How to perform unauthenticated Instagram web scraping in response to recent private API changes?,"Months ago, Instagram began rendering their public API inoperable by removing most features and refusing to accept new applications for most permissions scopes. Further changes were made this week ...",https://stackoverflow.com/questions/49786980/how-to-perform-unauthenticated-instagram-web-scraping-in-response-to-recent-priv,"['Months ago, Instagram began rendering their public API inoperable by removing most features and refusing to accept new applications for most permissions scopes. Further changes were made this week which further constricts developer options.', ""You aren't persisting the User Agent (a requirement) in the first query to Instagram:"", 'So in order to call instagram query you need to generate x-instagram-gis header.', ""Uhm... I don't have Node installed on my machine, so I cannot verify for sure, but looks like to me that you are missing a crucial part of the parameters in querystring, that is the after field:"", 'query_hash is not constant and keep changing over time.']"
72,What is the fastest way to scrape HTML webpage in Android?,"I need to extract information from an unstructured web page in Android. The information I want is embedded in a table that doesn't have an id.

<table> 
<tr><td>Description</td>...",https://stackoverflow.com/questions/2971155/what-is-the-fastest-way-to-scrape-html-webpage-in-android,"[""I need to extract information from an unstructured web page in Android. The information I want is embedded in a table that doesn't have an id."", 'I think in this case it makes no sense to look for a fast way to extract the information as there is virtually no performance difference between the methods already suggested in answers when you compare it to the time it will take to download the HTML.', ""The fastest way will be parsing the specific information yourself. You seem to know the HTML structure precisely beforehand. The BufferedReader, String and StringBuilder methods should suffice. Here's a kickoff example which displays the first paragraph of your own question:"", 'When you Scrap Html webPage. Two things you can do for it. First One is using REGEX. Another One is Html parsers.', ""Why don't you just write"", ""Why don't you create a script that does the scraping with cURL and simple html dom parser and just grab the value you need from that page?  These tools work with PHP, but other tools exist for exist for any language you need."", 'One way of doing this is to put the html into a String and then manually search and parse through the String. If you know that the tags will come in a specific order then you should be able to crawl through it and find the data. This however is kinda sloppy, so its a question of do you want it to work now? or work well?']"
73,Web scraping in PHP,"I'm looking for a way to make a small preview of another page from a URL given by the user in PHP.

I'd like to retrieve only the title of the page, an image (like the logo of the website) and a bit ...",https://stackoverflow.com/questions/9813273/web-scraping-in-php,"[""I'm looking for a way to make a small preview of another page from a URL given by the user in PHP."", 'I recommend you consider simple_html_dom for this. It will make it very easy.', 'You may use either of these libraries. As you know each one has pros & cons, so you may consult notes about each one or take time & try it on your own:', 'You can use SimpleHtmlDom for this. and then look for the title and img tags or what ever else you need to do.', ""This question is fairly old but still ranks very highly on Google Search results for web scraping tools in PHP.  Web scraping in PHP has advanced considerably in the intervening years since the question was asked.  I actively maintain the Ultimate Web Scraper Toolkit, which hasn't been mentioned yet but predates many of the other tools listed here except for Simple HTML DOM."", 'I like the Dom Crawler library. Very easy to use, has lots of options like:']"
74,Fetch all href link using selenium in python,"I am practicing Selenium in Python and I wanted to fetch all the links on a web page using Selenium.

For example, I want all the links in the href= property of all the <a> tags on http://...",https://stackoverflow.com/questions/34759787/fetch-all-href-link-using-selenium-in-python,"['I am practicing Selenium in Python and I wanted to fetch all the links on a web page using Selenium.', 'Well, you have to simply loop through the list:', 'I have checked and tested that there is a function named find_elements_by_tag_name() you can use. This example works fine for me.', 'You can import the HTML dom using html dom library in python. You can find it over here and install it using PIP:', 'You can try something like:', 'Unfortunately, the original link posted by OP is dead...']"
75,BeautifulSoup webscraping find_all( ): finding exact match,"I'm using Python and BeautifulSoup for web scraping.

Lets say I have the following html code to scrape:

<body>
    <div class=""product"">Product 1</div>
    <div class=""product""&...",https://stackoverflow.com/questions/22726860/beautifulsoup-webscraping-find-all-finding-exact-match,"[""I'm using Python and BeautifulSoup for web scraping."", 'In BeautifulSoup 4, the class attribute (and several other attributes, such as accesskey and the headers attribute on table cell elements) is treated as a set; you match against individual elements listed in the attribute. This follows the HTML standard.', 'You can use CSS selectors like so:', ""This code matches anything that doesn't have the product at the end of its class."", 'You could solve this problem and capture just Product 1 and Product 2 with gazpacho by enforcing exact matching:', 'change your code from']"
76,Scraping Real Time Visitors from Google Analytics,I have a lot of sites and want to build a dashboard showing the number of real time visitors on each of them on a single page. (would anyone else want this?) Right now the only way to view this ...,https://stackoverflow.com/questions/11021554/scraping-real-time-visitors-from-google-analytics,"['I have a lot of sites and want to build a dashboard showing the number of real time visitors on each of them on a single page. (would anyone else want this?) Right now the only way to view this information is to open a new tab for each site.', 'To get the same, Google has  launched new Real Time API. With this API you can easily retrieve real time online visitors as well as several Google Analytics with following dimensions and metrics. https://developers.google.com/analytics/devguides/reporting/realtime/dimsmets/', 'With Google Chrome I can see the data on the Network Panel.', 'Having google in the loop seems pretty redundant. Suggest you use a common element delivered on demand from the dashboard server and include this item by absolute URL on all pages to be monitored for a given site. The script outputting the item can read the IP of the browser asking and these can all be logged into a database and filtered for uniqueness giving a real time head count.', 'I needed/wanted realtime data for personal use so I reverse-engineered their system a little bit.']"
77,Android: Using WebView outside an Activity context,"I am trying to achieve Web Scraping through a background IntentService that periodically scrape a website without a view displaying on the users phone.  
Since I have to do call some javascript on ...",https://stackoverflow.com/questions/18865035/android-using-webview-outside-an-activity-context,"['I am trying to achieve Web Scraping through a background IntentService that periodically scrape a website without a view displaying on the users phone.', ""You can display a webview from a service. Code below creates a window which your service has access to. The window isn't visible because the size is 0 by 0."", 'Correct me if I am wrong but the correct answer to this question is that there is NO possible way to use a WebView in the background while the user is doing other things on the phone without interrupting the user by means of an Activity.', 'You can use this to hide the Activity', 'the solution was like this, but with Looper.getMainLooper() :', 'I used the following code to get round this problem:', 'A WebView cannot exist outside of an Activity or Fragment due to it being a UI.\nHowever, this means that an Activity is only needed to create the WebView, not handle all its requests.', 'or a substitute for a WebView that can accomplish the same <=== if you do not wish to show the loaded info on UI, maybe you can try to use HTTP to call the url directly, and process on the returned response from HTTP', ""I know it'a been a year and a half, but I'm now facing the same issue. I solved it eventually by running my Javascript code inside a Node engine that is running inside my Android App. It's called JXCore. You can take a look. Also, take a look at this sample that runs Javascript without a WebView. I really would like to know what did you end up using?"", ""Why don't you create a Backend Service that does the scraping for you?"", ""I am not sure if this is a silver bullet to the given problem. \nAs per @Pierre's accepted answer (sounds correct to me)""]"
78,BeautifulSoup: object of type 'Response' has no len(),"Issue: when I try to execute the script, BeautifulSoup(html, ...) gives the error message ""TypeError: object of type 'Response' has no len(). I tried passing the actual html as a parameter, but it ...",https://stackoverflow.com/questions/36709165/beautifulsoup-object-of-type-response-has-no-len,"['Issue: when I try to execute the script, BeautifulSoup(html, ...) gives the error message ""TypeError: object of type \'Response\' has no len(). I tried passing the actual html as a parameter, but it still doesn\'t work.', 'You are getting response.content. But it return response body as bytes (docs). But you should pass str to BeautifulSoup constructor (docs). So you need to use the response.text instead of getting content.', 'Try to pass the HTML text directly', ""If you're using requests.get('https://example.com') to get the HTML, you should use requests.get('https://example.com').text."", ""you are getting only response code in 'response'\nand always use browser header for security otherwise \nyou will face many issues"", 'It worked for me:', 'you should use .text to get content of response']"
79,CasperJS/PhantomJS doesn't load https page,"I know there are certain web pages PhantomJS/CasperJS can't open, and I was wondering if this one was one of them: https://maizepages.umich.edu. CasperJS gives an error: PhantomJS failed to open page ...",https://stackoverflow.com/questions/26415188/casperjs-phantomjs-doesnt-load-https-page,"[""I know there are certain web pages PhantomJS/CasperJS can't open, and I was wondering if this one was one of them: https://maizepages.umich.edu. CasperJS gives an error: PhantomJS failed to open page status=fail."", 'The problem may be related to the recent discovery of a SSLv3 vulnerability (POODLE). Website owners were forced to remove SSLv3 support from their websites. Since PhantomJS < v1.9.8 uses SSLv3 by default, you should use TLSv1:']"
80,How to generate the start_urls dynamically in crawling?,"I am crawling a site which may contain a lot of start_urls, like:

http://www.a.com/list_1_2_3.htm
I want to populate start_urls like [list_\d+_\d+_\d+\.htm],
and extract items from URLs like [node_\...",https://stackoverflow.com/questions/9322219/how-to-generate-the-start-urls-dynamically-in-crawling,"['I am crawling a site which may contain a lot of start_urls, like:', 'The best way to generate URLs dynamically is to override the start_requests method of the spider:', 'There are two questions:']"
81,asyncio web scraping 101: fetching multiple urls with aiohttp,"In earlier question, one of authors of aiohttp kindly suggested way to fetch multiple urls with aiohttp using the new async with syntax from Python 3.5:

import aiohttp
import asyncio

async def fetch(...",https://stackoverflow.com/questions/35926917/asyncio-web-scraping-101-fetching-multiple-urls-with-aiohttp,"['In earlier question, one of authors of aiohttp kindly suggested way to fetch multiple urls with aiohttp using the new async with syntax from Python 3.5:', 'I would use gather instead of wait, which can return exceptions as objects, without raising them. Then you can check each result, if it is instance of some exception.', 'I am far from an asyncio expert but you want to catch the  error you need to catch a socket error:']"
82,Scraping and parsing Google search results using Python,"I asked a question on realizing a general idea to crawl and save webpages.
Part of the original question is: how to crawl and save a lot of ""About"" pages from the Internet.

With some further research,...",https://stackoverflow.com/questions/7746832/scraping-and-parsing-google-search-results-using-python,"['I asked a question on realizing a general idea to crawl and save webpages.\nPart of the original question is: how to crawl and save a lot of ""About"" pages from the Internet.', 'You may find xgoogle useful... much of what you seem to be asking for is there...', ""There is a twill lib for emulating browser. I used it when had a necessity to login with google email account. While it's a great tool with a great idea, it's pretty old and seems to have a lack of support nowadays (the latest version is released in 2007).\nIt might be useful if you want to retrieve results that require cookie-handling or authentication. Likely that twill is one of the best choices for that purposes.\nBTW, it's based on mechanize."", 'Have a look at this awesome urllib wrapper for web scraping https://github.com/mattseh/python-web/blob/master/web.py', ""This one works good for this moment. If any search is made, the scraper is able to fetch 100 items of that search by going through several pages. I tried to use function to complete the code flawlessly but ipv4 issue comes up and the page gets redirected to the one with captcha. Still confused why this one works but if it is wrapped within function then it won't work anymore. Btw, the scraper looks a bit awkward cause I used the same for loop twice in my scraper so that It can't skip the content of first page."", 'Another option to scrape Google search results using Python is the one by ZenSERP.', 'this should be handy....for more go to - \n  https://github.com/goyal15rajat/Crawl-google-search.git', 'Here is a Python script using requests and BeautifulSoup to scrape Google results.']"
83,Python - make a POST request using Python 3 urllib,"I am trying to make a POST request to the following page:http://search.cpsa.ca/PhysicianSearch

In order to simulate clicking the 'Search' button without filling out any of the form, which adds data ...",https://stackoverflow.com/questions/36484184/python-make-a-post-request-using-python-3-urllib,"['I am trying to make a POST request to the following page:http://search.cpsa.ca/PhysicianSearch', 'This is how you do it.', 'Thank you C Panda. You really made it easy for me to learn this module.', 'The above code encoded the JSON string with some extra \\"" that caused me a lot of problems. This looks like a better way of doing it:', 'It failed when I use urlencode. So I use the following code to make a POST call in Python3:']"
84,Getting the nth element using BeautifulSoup,"From a large table I want to read rows 5, 10, 15, 20 ... using BeautifulSoup. How do I do this? Is findNextSibling and an incrementing counter the way to go?",https://stackoverflow.com/questions/8724352/getting-the-nth-element-using-beautifulsoup,"['From a large table I want to read rows 5, 10, 15, 20 ... using BeautifulSoup. How do I do this? Is findNextSibling and an incrementing counter the way to go?', 'You could also use findAll to get all the rows in a list and after that just use the slice syntax to access the elements that you need:', 'This can be easily done with select in beautiful soup if you know the row numbers to be selected. (Note : This is in bs4)', 'As a general solution, you can convert the table to a nested list and iterate...', 'Another option, if you prefer raw html...', ""Here's how you could scrape every 5th distribution link on this Wikipedia page with gazpacho:""]"
85,Beautiful Soup if Class “Contains” or Regex?,"If my class names are constantly different say for example:

listing-col-line-3-11 dpt 41
listing-col-block-1-22 dpt 41
listing-col-line-4-13 CWK 12
Normally I could do:

for EachPart in soup....",https://stackoverflow.com/questions/34660417/beautiful-soup-if-class-contains-or-regex,"['If my class names are constantly different say for example:', 'BeautifulSoup supports CSS selectors which allow you to select elements based on the content of particular attributes. This includes the selector *= for contains.', 'Yu can try this:', 'You could avoid regex by using partial matching with gazpacho...']"
86,Obtaining reddit data [closed],I am interested in obtaining data from different reddit subreddits. Does anyone know if there is a reddit/other api similar like twitter does to crawl all the pages?,https://stackoverflow.com/questions/14322834/obtaining-reddit-data,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'Yes, reddit has an API that can be used for a variety of purposes such as data collection, automatic commenting bots, or even to assist in subreddit moderation.', ""Note that if you are only reading data, and not interested into posting back to reddit, you can get quite a bit of data from the json feeds associated with each subreddit.  With this method, you don't need to worry about an API at all -- you simply request the relevant json file and parse it in your language of choice."", 'To parse JSON data from reddit with ajax/javascript.']"
87,Python selenium multiprocessing,I've written a script in python in combination with selenium to scrape the links of different posts from its landing page and finally get the title of each post by tracking the url leading to its ...,https://stackoverflow.com/questions/53475578/python-selenium-multiprocessing,"[""I've written a script in python in combination with selenium to scrape the links of different posts from its landing page and finally get the title of each post by tracking the url leading to its inner page. Although the content I parsed here are static ones, I used selenium to see how it works in multiprocessing."", 'how can I reduce the execution time using selenium when it is made to run using multiprocessing', 'My question: how can I reduce the execution time?', 'The one potential problem I see with the clever one-driver-per-thread answer is that it omits any mechanism for ""quitting"" the drivers and thus leaving the possibility of processes hanging around. I would make the following changes:']"
88,How to use CrawlSpider from scrapy to click a link with javascript onclick?,"I want scrapy to crawl pages where going on to the next link looks like this:

<a href=""#"" onclick=""return gotoPage('2');""> Next </a>
Will scrapy be able to interpret javascript code of ...",https://stackoverflow.com/questions/2454998/how-to-use-crawlspider-from-scrapy-to-click-a-link-with-javascript-onclick,"['I want scrapy to crawl pages where going on to the next link looks like this:', 'The actual methodology will be as follows:', 'I built a quick crawler that executes JS via selenium. Feel free to copy / modify https://github.com/rickysahu/seleniumjscrawl']"
89,How to connect via HTTPS using Jsoup?,"It's working fine over HTTP, but when I try and use an HTTPS source it throws the following exception:

10-12 13:22:11.169: WARN/System.err(332): javax.net.ssl.SSLHandshakeException: java.security....",https://stackoverflow.com/questions/7744075/how-to-connect-via-https-using-jsoup,"[""It's working fine over HTTP, but when I try and use an HTTPS source it throws the following exception:"", 'If you want to do it the right way, and/or you need to deal with only one site, then you basically need to grab the SSL certificate of the website in question and import it in your Java key store. This will result in a JKS file which you in turn set as SSL trust store before using Jsoup (or java.net.URLConnection).', 'In my case, all I needed to do was to add the .validateTLSCertificates(false) in my connection', ""I stumbled over the answers here and in the linked question in my search and want to add two pieces of information, as the accepted answer doesn't fit my quite similar scenario, but there is an additional solution that fits even in that case (cert and hostname don't match for test systems)."", 'To suppress certificate warnings for specific JSoup connection can use following approach:', ""I've had the same problem but took the lazy route - tell your app to ignore the cert and carry on anyway."", ""I'm no expert in this field but I ran into a similar exception when trying to connect to a website over HTTPS using java.net APIs.  The browser does a lot of work for you regarding SSL certificates when you visit a site using HTTPS.  However, when you are manually connecting to sites (using HTTP requests manually), all that work still needs to be done.  Now I don't know what all this work is exactly, but it has to do with downloading certificates and putting them where Java can find them.  Here's a link that will hopefully point you in the right direction."", 'I was facing the same issue with Jsoup, I was not able to connect and get the document for https urls but when I changed my JDK version from 1.7 to 1.8, the issue got resolved.', ""I've had that problem only in dev environment. The solution to solve it was just to add a few flags to ignore SSL to VM:"", 'Try following (just put it before Jsoup.connect(""https://example.com""):']"
90,BeautifulSoup: Get the contents of a specific table,"My local airport disgracefully blocks users without IE, and looks awful. I want to write a Python scripts that would get the contents of the Arrival and Departures pages every few minutes, and show ...",https://stackoverflow.com/questions/2935658/beautifulsoup-get-the-contents-of-a-specific-table,"['My local airport disgracefully blocks users without IE, and looks awful. I want to write a Python scripts that would get the contents of the Arrival and Departures pages every few minutes, and show them in a more readable manner.', 'This is not the specific code you need, just a demo of how to work with BeautifulSoup. It finds the table who\'s id is ""Table1"" and gets all of its tr elements.', 'Here is a working example for a generic <table>. (Though not using your page due javascript execution needed to load table data)']"
91,“SSL: certificate_verify_failed” error when scraping https://www.thenewboston.com/,"So I started learning Python recently using ""The New Boston's"" videos on youtube, everything was going great until I got to his tutorial of making a simple web crawler. While I understood it with no ...",https://stackoverflow.com/questions/34503206/ssl-certificate-verify-failed-error-when-scraping-https-www-thenewboston-co,"['So I started learning Python recently using ""The New Boston\'s"" videos on youtube, everything was going great until I got to his tutorial of making a simple web crawler. While I understood it with no problem, when I run the code I get errors all seemingly based around ""SSL: CERTIFICATE_VERIFY_FAILED."" I\'ve been searching for an answer since last night trying to figure out how to fix it, it seems no one else in the comments on the video or on his website are having the same problem as me and even using someone elses code from his website I get the same results. I\'ll post the code from the one I got from the website as it\'s giving me the same error and the one I coded is a mess right now.', 'The problem is not in your code but in the web site you are trying to access. When looking at the analysis by SSLLabs you will note:', 'You can tell requests not to verify the SSL certificate:', 'You are probably missing the stock certificates in your system. E.g. if running on Ubuntu, check that ca-certificates package is installed.', ""if you want to use the Python dmg installer, you also have to read Python 3's ReadMe and run the bash command to get new certificates."", ""I'm posting this as an answer because I've gotten past your issue thus far, but there's still issues in your code (which when fixed, I can update)."", 'I spent several hours trying to fix some Python and update certs on a VM.  In my case I was working against a server that someone else had set up.  It turned out that the wrong cert had been uploaded to the server.  I found this command on another SO answer.', 'It\'s worth shedding a bit more ""hands-on"" light about what happens here, adding upon @Steffen Ullrich\'s answer here and elsewhere:']"
92,How to set value of an input tag in casperJs,"I have input element as shown :

<input type=""text"" class=""bg-white"" id=""couponCode"" value="""">

How can i set/fill its value using casperJs",https://stackoverflow.com/questions/18172040/how-to-set-value-of-an-input-tag-in-casperjs,"['I have input element as shown :', ""Using casper.sendKeys('selector', value);"", 'There are a few different methods available for accomplishing this task.']"
93,Can Scrapy be replaced by pyspider?,"I've been using Scrapy web-scraping framework pretty extensively, but, recently I've discovered that there is another framework/system called pyspider, which, according to it's github page, is fresh, ...",https://stackoverflow.com/questions/27243246/can-scrapy-be-replaced-by-pyspider,"[""I've been using Scrapy web-scraping framework pretty extensively, but, recently I've discovered that there is another framework/system called pyspider, which, according to it's github page, is fresh, actively developed and popular."", 'pyspider and Scrapy have the same purpose, web scraping, but a different view about doing that.', 'Since I use both scrapy and pyspider, I would like to suggest the following:']"
94,Reading dynamically generated web pages using python,"I am trying to scrape a web site using python and beautiful soup. I encountered that in some sites, the image links although seen on the browser is cannot be seen in the source code. However on using ...",https://stackoverflow.com/questions/13960567/reading-dynamically-generated-web-pages-using-python,"['I am trying to scrape a web site using python and beautiful soup. I encountered that in some sites, the image links although seen on the browser is cannot be seen in the source code. However on using Chrome Inspect or Fiddler, we can see the the corresponding codes. \nWhat I see in the source code is:', 'You need JavaScript Engine to parse and run JavaScript code inside the page.\nThere are a bunch of headless browsers that can help you', 'The Content of the website may be generated after load via javascript, In order to obtain the generated script via python refer to this answer', 'A regular scraper gets just the HTML document. To get any content generated by JavaScript logic, you rather need a Headless browser that would also generate the DOM, load and run the scripts like a regular browser would. The Wikipedia article and some other pages on the Net have lists of those and their capabilities.', 'TRY THIS FIRST!']"
95,ScrapyRT vs Scrapyd,"We've been using Scrapyd service for a while up until now. It provides a nice wrapper around a scrapy project and its spiders letting to control the spiders via an HTTP API:
  Scrapyd is a service ...",https://stackoverflow.com/questions/37283531/scrapyrt-vs-scrapyd,"[""We've been using Scrapyd service for a while up until now. It provides a nice wrapper around a scrapy project and its spiders letting to control the spiders via an HTTP API:"", ""They don't have thaaat much in common. As you have already seen you have to deploy your spiders to scrapyd and then schedule crawls. scrapyd is a standalone service running on a server where you can deploy and run every project/spider you like.""]"
96,YouTube comment scraper returns limited results,"The task:

I wanted to scrape all the YouTube comments from a given video.

I successfully adapted the R code from a previous question (Scraping Youtube comments in R).

Here is the code:

library(...",https://stackoverflow.com/questions/29692972/youtube-comment-scraper-returns-limited-results,"['The task:', 'I was (for the most part) able to accomplish this by using the latest version of the Youtube Data API and the R package httr. The basic approach I took was to send multiple GET requests to the appropriate URL and grab the data in batches of 100 (the maximum the API allows) - i.e.', ""An alternative to the XML package is the rvest package. Using the URL that you've provided, scraping comments would look like this:"", 'Your issue lies with getting max results.', 'I tried for different videos with ""tuber"" package in R and my results here.\nIf one author has only replies (doesnt have comment about video) ,then according to number of replies behave.If the author has not more than 5 replies then dont scrape anyone.But if has more than 5 replies then some comments are scraping.\nAnd if one author has both himself comments and replies then more than second man (up I told) comments are scraping.']"
97,Wait page to load before getting data with requests.get in python 3,"I have a page that i need to get the source to use with BS4, but the middle of the page takes 1 second(maybe less) to load the content, and requests.get catches the source of the page before the ...",https://stackoverflow.com/questions/45448994/wait-page-to-load-before-getting-data-with-requests-get-in-python-3,"['I have a page that i need to get the source to use with BS4, but the middle of the page takes 1 second(maybe less) to load the content, and requests.get catches the source of the page before the section loads, how can I wait a second before getting the data?', ""It doesn't look like a problem of waiting, it looks like the element is being created by JavaScript, requests can't handle dynamically generated elements by JavaScript. A suggestion is to use selenium together with PhantomJS to get the page source, then you can use BeautifulSoup for your parsing, the code shown below will do exactly that:"", 'In Python 3, Using the module urllib in practice works better when loading dynamic webpages than the requests module.', 'I found a way to that !!!', 'Just to list my way of doing it, maybe it can be of value for someone:']"
98,scrapy from script output in json,"I am running scrapy in a python script

def setup_crawler(domain):
    dispatcher.connect(stop_reactor, signal=signals.spider_closed)
    spider = ArgosSpider(domain=domain)
    settings = ...",https://stackoverflow.com/questions/23574636/scrapy-from-script-output-in-json,"['I am running scrapy in a python script', 'You need to set FEED_FORMAT and FEED_URI settings manually:', 'I managed to make it work simply by adding the FEED_FORMAT and FEED_URI to the CrawlerProcess constructor, using the basic Scrapy API tutorial code as follows:', 'Easy!']"
99,scrapy: convert html string to HtmlResponse object,"I have a raw html string that I want to convert to scrapy HTML response object so that I can use the selectors css and xpath, similar to scrapy's response. How can I do it?",https://stackoverflow.com/questions/27323740/scrapy-convert-html-string-to-htmlresponse-object,"[""I have a raw html string that I want to convert to scrapy HTML response object so that I can use the selectors css and xpath, similar to scrapy's response. How can I do it?"", 'First of all, if it is for debugging or testing purposes, you can use the Scrapy shell:', ""alecxe's answer is right, but this is the correct way to instantiate a Selector from text in scrapy:""]"
100,How to programmatically log in to a website to screenscape?,"I need some information from a website that's not mine, in order to get this information I need to login to the website to gather the information, this happens through a HTML form. How can I do this ...",https://stackoverflow.com/questions/975426/how-to-programmatically-log-in-to-a-website-to-screenscape,"[""I need some information from a website that's not mine, in order to get this information I need to login to the website to gather the information, this happens through a HTML form. How can I do this authenticated screenscaping in C#?"", ""You'd make the request as though you'd just filled out the form. Assuming it's POST for example, you make a POST request with the correct data. Now if you can't login directly to the same page you want to scrape, you will have to track whatever cookies are set after your login request, and include them in your scraping request to allow you to stay logged in."", ""You can use a WebBrowser control. Just feed it the URL of the site, then use the DOM to set the username and password into the right fields, and eventually send a click to the submit button. This way you don't care about anything but the two input fields and the submit button. No cookie handling, no raw HTML parsing, no HTTP sniffing - all that is done by the browser control."", 'For some cases, httpResponse.Cookies will be blank. Use the CookieContainer instead.', 'As an addition to dlambin answer\nIt is necessary to have', ""You need to use the HTTPWebRequest and do a POST. This link should help you get started. The key is, you need to look at the HTML Form of the page you're trying to post from to see all the parameters the form needs in order to submit the post.""]"
101,Crawling multiple URL in a loop using puppeteer,"I have 

urls = ['url','url','url'...]
this is what I'm doing 

urls.map(async (url)=>{
  await page.goto(url);
  await page.waitForNavigation({ waitUntil: 'networkidle' });
})
This seems to not ...",https://stackoverflow.com/questions/46293216/crawling-multiple-url-in-a-loop-using-puppeteer,"['I have', 'map, forEach, reduce, etc, does not wait for the asynchronous operation within them, before they proceed to the next element of the iterator they are iterating over.', 'If you find that you are waiting on your promise indefinitely, the proposed solution is to use the following:', 'Best way I found to achieve this.']"
102,Scraping a dynamic ecommerce page with infinite scroll,"I'm using rvest in R to do some scraping. I know some HTML and CSS.

I want to get the prices of every product of a URI:

http://www.linio.com.co/tecnologia/celulares-telefonia-gps/

The new items ...",https://stackoverflow.com/questions/29861117/scraping-a-dynamic-ecommerce-page-with-infinite-scroll,"[""I'm using rvest in R to do some scraping. I know some HTML and CSS."", 'As @nrussell suggested, you can use RSelenium to programatically scroll down the page before getting the source code.', 'Loop through the website https://www.linio.com.co/c/celulares-y-tablets?page=2 and 3 and so on and it will be easy for you to scrape the data']"
103,HTML scraping using lxml and requests gives a unicode error [duplicate],"I'm trying to use HTML scraper like the one provided here. It works fine for the example they provided. However, when I try using it with my webpage, I receive this error - Unicode strings with ...",https://stackoverflow.com/questions/25023237/html-scraping-using-lxml-and-requests-gives-a-unicode-error,"[""I'm trying to use HTML scraper like the one provided here. It works fine for the example they provided. However, when I try using it with my webpage, I receive this error - Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.\n\nI've tried googling but couldn't find a solution. I'd truly appreciate any help. I'd like to know if there's a way to copy it as HTML using Python."", 'Short answer: use page.content, not page.text.']"
104,“Failed to decode response from marionette” message in Python/Firefox headless scraping script,"Good Day, I've done a number of searches on here and google and yet to find a solution that address this problem.

The scenario is: 

I have a Python script (2.7) that loops through an number of URLs (...",https://stackoverflow.com/questions/49734915/failed-to-decode-response-from-marionette-message-in-python-firefox-headless-s,"[""Good Day, I've done a number of searches on here and google and yet to find a solution that address this problem."", 'For anyone else experiencing this issue when running selenium webdriver in a Docker container, increasing the container size to 2gb fixes this issue.', ""I still don't know why this is happening but I may have found a work around.  I read in some documentation there may be a race condition (on what, I am not sure since there shouldn't be two items competing for the same resources)."", 'This error message...', ""The likely real issue behind this is that the DOM has not loaded yet and you are triggering searches on the next page. That's why the sleep(3) is working in most cases. The proper fix is to use a wait class."", 'The problem is that you do not close the driver. I made the same mistake. Whit htop in Linux, I have noticed that I occupy all the 26 GB of my pc whit firefox unclosed process.', 'Try this, for Ubuntu 16.04:', 'I hope this saves some other poor soul the hours I just spent on this.']"
105,How does recaptcha 3 know I'm using selenium/chromedriver?,"I'm curious how Recaptcha v3 works. Specifically the browser fingerprinting.

When I launch a instance of chrome through selenium/chromedriver and test against ReCaptcha 3 (https://recaptcha-demo....",https://stackoverflow.com/questions/55501524/how-does-recaptcha-3-know-im-using-selenium-chromedriver,"[""I'm curious how Recaptcha v3 works. Specifically the browser fingerprinting."", 'Websites can easily detect the network traffic and identify your program as a BOT. Google have already released 5(five) reCAPTCHA to choose from when creating a new site. While four of them are active and reCAPTCHA v1 being shutdown.', 'Selenium and Puppeteer have some browser configs that is different from a non-automated browser. Also, since some JS functions are injected into browser to manipulate elements, you need to create some override to avoid detections.']"
106,Web scraping - how to access content rendered in JavaScript via Angular.js?,"I'm trying to scrape data from the public site asx.com.au

The page http://www.asx.com.au/asx/research/company.do#!/ACB/details contains a div with class 'view-content', which has the information I ...",https://stackoverflow.com/questions/35050746/web-scraping-how-to-access-content-rendered-in-javascript-via-angular-js,"[""I'm trying to scrape data from the public site asx.com.au"", 'This page use JavaScript to read data from server and fill page.']"
107,Can a Telegram bot read messages of channel,"Can a telegram bot read/access a telegram channel that neither I or the bot is administrator of? 

I know that up to last November it was not possible, but I have heard some people have done this, but ...",https://stackoverflow.com/questions/42672034/can-a-telegram-bot-read-messages-of-channel,"['Can a telegram bot read/access a telegram channel that neither I or the bot is administrator of?', 'The FAQ reads:']"
108,"Save complete web page (incl css, images) using python/selenium","I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want:

from selenium ...",https://stackoverflow.com/questions/53729201/save-complete-web-page-incl-css-images-using-python-selenium,"['I am using Python/Selenium to submit genetic sequences to an online database, and want to save the full page of results I get back. Below is the code that gets me to the results I want:', ""As you noted, Selenium cannot interact with the browser's context menu to use Save as..., so instead to do so, you could use an external automation library like pyautogui."", 'This is not a perfect solution, but it will get you most of what you need.  You can replicate the behavior of ""save as full web page (complete)"" by parsing the html and downloading any loaded files (images, css, js, etc.) to their same relative path.', ""Inspired by FThompson's answer above, I came up with the following tool that can download full/complete html for a given page url (see: https://github.com/markfront/SinglePageFullHtml)"", 'I\'ll advise u to have a try on sikulix which is an image based automation tool for operate any widgets within PC OS, it supports python grammar and run with command line and maybe the simplest way to solve ur problem.\nAll u need to do is just give it a screenshot, call sikulix script in ur python automation script(with OS.system(""xxxx"") or subprocess...).']"
109,Beautiful Soup Using Regex to Find Tags?,"I'd really like to be able to allow Beautiful Soup to match any list of tags, like so. I know attr accepts regex, but is there anything in beautiful soup that allows you to do so?

soup.findAll(""(a|...",https://stackoverflow.com/questions/24748445/beautiful-soup-using-regex-to-find-tags,"[""I'd really like to be able to allow Beautiful Soup to match any list of tags, like so. I know attr accepts regex, but is there anything in beautiful soup that allows you to do so?"", 'find_all() is the most favored method in the Beautiful Soup search API.', 'Note that you can also use regular expressions to search in attributes of tags. For example:', 'yes see docs...']"
110,Scrapy Very Basic Example,"Hi I have Python Scrapy installed on my mac and I was trying to follow the very first example on their web. 

They were trying to run the command:

scrapy crawl mininova.org -o scraped_data.json -t ...",https://stackoverflow.com/questions/18838494/scrapy-very-basic-example,"['Hi I have Python Scrapy installed on my mac and I was trying to follow the very first example on their web.', 'You may have better luck looking through the tutorial first, as opposed to the ""Scrapy at a glance"" webpage.', 'TL;DR: see Self-contained minimum example script to run scrapy.']"
111,How do screen scrapers work? [closed],"I hear people writing these programs all the time and I know what they do, but how do they actually do it? I'm looking for general concepts.",https://stackoverflow.com/questions/156083/how-do-screen-scrapers-work,"['Want to improve this question? Update the question so it focuses on one problem only by editing this post.', ""Technically, screenscraping is any program that grabs the display data of another program and ingests it for it's own use."", 'Lots of accurate answers here.', 'In general a screen scraper is a program that captures output from a server program by mimicing the actions of a person sitting in front of the workstation using a browser or terminal access program. at certain key points the program would interpret the output and then take an action or extract certain amounts of information from the output.', 'A screen scraper downloads the html page, and pulls out the data interested either by searching for known tokens or parsing it as XML or some such.', ""In the early days of PC's, screen scrapers would emulate a terminal (e.g. IBM 3270) and pretend to be a user in order to interactively extract, update information on the mainframe.  In more recent times, the concept is applied to any application that provides an interface via web pages."", ""Here's a tiny bit of screen scraping implemented in Javascript, using jQuery (not a common choice, mind you, since scraping is usually a client-server activity):"", ""Technically, screenscraping is any program that grabs the display data of another program and ingests it for it's own use.In the early days of PC's, screen scrapers would emulate a terminal (e.g. IBM 3270) and pretend to be a user in order to interactively extract, update information on the mainframe. In more recent times, the concept is applied to any application that provides an interface via web pages."", 'You have an HTML page that contains some data you want. What you do is you write a program that will fetch that web page and attempt to extract that data. This can be done with XML parsers, but for simple applications I prefer to use regular expressions to match a specific spot in the HTML and extract the necessary data. Sometimes it can be tricky to create a good regular expression, though, because the surrounding HTML appears multiple times in the document. You always want to match a unique item as close as you can to the data you need.', ""Screen scraping is what you do when nobody's provided you with a reasonable machine-readable interface. It's hard to write, and brittle.""]"
112,Browser-based client-side scraping,"I wonder if its possible to scrape an external (cross-domain) page through the user's IP? 

For a shopping comparison site, I need to scrape pages of an e-com site but several requests from the server ...",https://stackoverflow.com/questions/31581051/browser-based-client-side-scraping,"[""I wonder if its possible to scrape an external (cross-domain) page through the user's IP?"", ""No, you won't be able to use the browser of your clients to scrape content from other websites using JavaScript because of a security measure called Same-origin policy."", 'Basically browsers are made to avoid doing this…', ""Have a look at http://import.io, they provide a couple of crawlers, connectors and extractors. I'm not pretty sure how they get around bans but they do somehow (we are using their system over a year now with no problems)."", 'You could build an browser extension with artoo.']"
113,scrapy item loader return list not single value,"I am using scrapy 0.20.

I want to use item loader 

this is my code:

l = XPathItemLoader(item=MyItemClass(), response=response)
        l.add_value('url', response.url)
        l.add_xpath('title',""...",https://stackoverflow.com/questions/23894139/scrapy-item-loader-return-list-not-single-value,"['I am using scrapy 0.20.', 'You need to set an Input or Output processor. TakeFirst would work perfectly in your case.']"
114,What is the best way to block specific URL for testing?,"I am observing a website with Google Chrome and Fiddler version 4.4.

The page is using AJAX to update its data. I want to block a specific URL to test what will happen if it doesn't work.

What is ...",https://stackoverflow.com/questions/14398241/what-is-the-best-way-to-block-specific-url-for-testing,"['I am observing a website with Google Chrome and Fiddler version 4.4.', 'What would you like to have happen?']"
115,Scrapy: Extract links and text,"I am new to scrapy and I am trying to scrape the Ikea website webpage. The basic page with the list of locations as given here.

My items.py file is given below:

import scrapy
class IkeaItem(scrapy....",https://stackoverflow.com/questions/27753232/scrapy-extract-links-and-text,"['I am new to scrapy and I am trying to scrape the Ikea website webpage. The basic page with the list of locations as given here.', ""There is a simple mistake inside the xpath expressions for the item fields. The loop is already going over the a tags, you don't need to specify a in the inner xpath expressions. In other words, currently you are searching for a tags inside the a tags inside the td inside tr. Which obviously results into nothing."", 'use this....']"
116,"Create link previews on the client side, like in Facebook/LinkedIn","I am creating a web app with an input box where the user can write anything, including URLs. I want to create a link preview like Facebook and LinkedIn does:
Scrape the given URL and display its main ...",https://stackoverflow.com/questions/24054691/create-link-previews-on-the-client-side-like-in-facebook-linkedin,"['I am creating a web app with an input box where the user can write anything, including URLs. I want to create a link preview like Facebook and LinkedIn does:', ""After hours of googling I found the answer myself..\nthere is already a question in SO Is there open-source code for making 'link preview' text and icons, like in facebook? . So we can use this link http://api.embed.ly/1/oembed?url=YOUR-URL via http GET where we get the meta tags in json format..\nI wrote my own code using JSdom and here goes the code..."", 'If anybody is still looking for an answer, I recommend you to see http://ogp.me.\nIt works on Telegram messenger, Facebook, Discord, etc.', 'Yes, you can generate link previews completely on the client, which is how link previews should be generated anyway, for efficiency and to avoid DOS-ing your server.']"
117,Web scraping SEC Edgar 10-K and 10-Q filings,"Are there anyone experienced with scraping SEC 10-K and 10-Q filings? I got stuck while trying to scrape monthly realised share repurchases from these filings. In specific, I would like to get the ...",https://stackoverflow.com/questions/31527633/web-scraping-sec-edgar-10-k-and-10-q-filings,"[""Are there anyone experienced with scraping SEC 10-K and 10-Q filings? I got stuck while trying to scrape monthly realised share repurchases from these filings. In specific, I would like to get the following information: 1. Period; 2. Total Number of Shares Purchased; 3. Average Price Paid per Share; 4. Total Number of Shares Purchased as Part of Publicly Announced Plans or Programs; 5. Maximum Number (or Approximate Dollar Value) of Shares that May Yet Be Purchased Under the Plans or Programs for each month from 2004 to 2014. I have in total 90,000+ forms to parse, so it won't be feasible to do it manually."", ""I'm not sure about python, but in R there is an beautiful solution using 'finstr' package (https://github.com/bergant/finstr).\n'finstr' automatically extracts the financial statements (income statement, balance sheet, cash flow and etc.) from EDGAR using XBRL format.""]"
118,How to retrieve/calculate citation counts and/or citation indices from a list of authors?,"I have a list of authors.
I wish to automatically retrieve/calculate the (ideally yearly) citation index (h-index, m-quotient,g-index, HCP indicator or ...) for each author.

Author Year Index
first  ...",https://stackoverflow.com/questions/10536601/how-to-retrieve-calculate-citation-counts-and-or-citation-indices-from-a-list-of,"['I have a list of authors.\nI wish to automatically retrieve/calculate the (ideally yearly) citation index (h-index, m-quotient,g-index, HCP indicator or ...) for each author.', 'Effectively the main problem is to build the citation graph. Once you have that you can compute any metrics you want (e.g. h-index, g-index, PageRank).']"
119,How can I get a Wikipedia article's text using Python 3 with Beautiful Soup?,"I have this script made in Python 3:

response = simple_get(""https://en.wikipedia.org/wiki/Mathematics"")
result = {}
result[""url""] = url
if response is not None:
    html = BeautifulSoup(response, '...",https://stackoverflow.com/questions/53804643/how-can-i-get-a-wikipedia-articles-text-using-python-3-with-beautiful-soup,"['I have this script made in Python 3:', 'select the <p> tag. There are 52 elements. Not sure if you want the whole thing, but you can iterate through those tags to store it as you may. I just chose to print each of them to show the output.', 'There is a much, much more easy way to get information from wikipedia - Wikipedia API.', 'Use the library wikipedia', 'You can get the desired output using lxml library like following.', 'What you seem to want is the (HTML) page content without the surrounding navigation elements.  As I described in this earlier answer from 2013, there are (at least) two ways to get it:', 'To get a proper way using function, you can just get JSON API offered by Wikipedia :']"
120,Scrapy Shell and Scrapy Splash,"We've been using scrapy-splash middleware to pass the scraped HTML source through the Splash javascript engine running inside a docker container.

If we want to use Splash in the spider, we configure ...",https://stackoverflow.com/questions/35352423/scrapy-shell-and-scrapy-splash,"[""We've been using scrapy-splash middleware to pass the scraped HTML source through the Splash javascript engine running inside a docker container."", 'just wrap the url you want to shell to in splash http api.', 'You can run scrapy shell without arguments inside a configured Scrapy project, then create req = scrapy_splash.SplashRequest(url, ...) and call fetch(req).', 'For the windows users, who use Docker Toolbox:']"
121,Python BeautifulSoup scrape tables,"I am trying to create a table scrape with BeautifulSoup. I wrote this Python code:

import urllib2
from bs4 import BeautifulSoup

url = ""http://dofollow.netsons.org/table1.htm""  # change to whatever ...",https://stackoverflow.com/questions/18966368/python-beautifulsoup-scrape-tables,"['I am trying to create a table scrape with BeautifulSoup. I wrote this Python code:', 'Loop over table rows (tr tag) and get the text of cells (td tag) inside:', ""The original link posted by OP is dead... but here's how you might scrape table data with gazpacho:""]"
122,scrapy- how to stop Redirect (302),"I'm trying to crawl a url using Scrapy. But it redirects me to page that doesn't exist. 

Redirecting (302) to <GET http://www.shop.inonit.in/mobile/Products/Inonit-Home-Decor--Knick-Knacks-...",https://stackoverflow.com/questions/15476587/scrapy-how-to-stop-redirect-302,"[""I'm trying to crawl a url using Scrapy. But it redirects me to page that doesn't exist."", 'yes you can do this simply by adding meta values like', ""After looking at the documentation and looking through the relevant source, I was able to figure it out. If you look in the source for start_requests, you'll see that it calls make_requests_from_url for all URLs."", 'By default, Scrapy use RedirectMiddleware to handle redirection. You can set REDIRECT_ENABLED to False to disable redirection.', 'As explained here: Scrapy docs']"
123,Click the javascript popup through webdriver,"I am scraping a webpage using Selenium webdriver in Python

The webpage I am working on, has a form. I am able to fill the form and then I click on the Submit button.

It generates an popup window( ...",https://stackoverflow.com/questions/8631500/click-the-javascript-popup-through-webdriver,"['I am scraping a webpage using Selenium webdriver in Python', 'Python Webdriver Script:', 'More about excepted_conditions https://seleniumhq.github.io/selenium/docs/api/py/webdriver_support/selenium.webdriver.support.expected_conditions.html', 'I am using Ruby bindings but here what I found in Selenium Python Bindings 2 documentation:\nhttp://readthedocs.org/docs/selenium-python/en/latest/index.html', 'If you want to Accept or Click the popup, regardless of for what it is then', ""that depends on the javascript function that handles the form submission\nif there's no such function try to submit the form using post"", 'Try The Code below! Working Fine for me!']"
124,How do I merge results from target page to current page in scrapy?,"Need example in scrapy on how to get a link from one page, then follow this link, get more info from the linked page, and merge back with some data from first page.",https://stackoverflow.com/questions/8467700/how-do-i-merge-results-from-target-page-to-current-page-in-scrapy,"['Need example in scrapy on how to get a link from one page, then follow this link, get more info from the linked page, and merge back with some data from first page.', ""Partially fill your item on the first page, and the put it in your request's meta. When the callback for the next page is called, it can take the partially filled request, put more data into it, and then return it."", 'More information on passing the meta data and request objects is specifically described in this part of the documentation:', 'An example from scrapy documntation', 'A bit illustration of Scrapy documentation code']"
125,How to parse DOM (REACT),I am trying to scrape data from a website. The website uses Facebook's React. As such the source code that I can parse using Jaunt is completely different to the code I see when inspecting the ...,https://stackoverflow.com/questions/29972996/how-to-parse-dom-react,"[""I am trying to scrape data from a website. The website uses Facebook's React. As such the source code that I can parse using Jaunt is completely different to the code I see when inspecting the elements using Chrome's inspector."", ""ReactJS, like many other Javascript libraries / frameworks, uses client-side code (Javascript) to render the final HTML. This means that when you, Jaunt, or your browser fetch the HTML source code from the server, it doesn't yet contain the final code the user will see. The browser needs to run the Javascript program(s) contained in the page, in order to generate the final content you wish to scrape.""]"
126,Module request how to properly retrieve accented characters? � � �,"I'm using: Module: Request -- Simplified HTTP request method to scrape a webpage with accented characters á é ó ú ê ã etc. 

I've already tried encoding: utf-8 with no success. I'm still getting this �...",https://stackoverflow.com/questions/8332500/module-request-how-to-properly-retrieve-accented-characters,"[""I'm using: Module: Request -- Simplified HTTP request method to scrape a webpage with accented characters á é ó ú ê ã etc."", 'Since binary is deprecated it seems like a better idea to use iconv and correctly handle the decoding:', 'Specify the encoding as utf8 not utf-8. Here are a list of possible encodings for a buffer from the Node.js documentation.', 'I were tried and OK (Shift_JIS):']"
127,R - How to make a click on webpage using rvest or rcurl,"I want to download data from this webpage

The data can be easily scraped with rvest.

The code maybe like this :

library(rvest)
library(pipeR)
url <- ""http://www.tradingeconomics.com/""
css <-  ...",https://stackoverflow.com/questions/29185501/r-how-to-make-a-click-on-webpage-using-rvest-or-rcurl,"['I want to download data from this webpage', ""Sometimes it's better to attack the problem at the ajax web-request level. For this site, you can use Chrome's dev tools and watch the requests. To build the table (the whole table, too) it makes a POST to the site with various ajax-y parameters. Just replicate that, do a bit of data-munging of the response and you're good to go:""]"
128,BeautifulSoup like scraper for nodejs [closed],"I am former python developer and I have used BS4 for couple of years
Now I am developing with node and yes cheerio package is very good, but I need smth like BS4 for scraping in node

Is there some ...",https://stackoverflow.com/questions/32667219/beautifulsoup-like-scraper-for-nodejs,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", ""I suggest you check out the x-ray node package. It is essentially an abstraction layer above cheerio that makes the code you need to 'structure any website' even more terse.""]"
129,automating the login to the uk data service website in R with RCurl or httr,I am in the process of writing a collection of freely-downloadable R scripts for http://asdfree.com/ to help people analyze the complex sample survey data hosted by the UK data service. In addition to ...,https://stackoverflow.com/questions/17778543/automating-the-login-to-the-uk-data-service-website-in-r-with-rcurl-or-httr,"['I am in the process of writing a collection of freely-downloadable R scripts for http://asdfree.com/ to help people analyze the complex sample survey data hosted by the UK data service. In addition to providing lots of statistics tutorials for these data sets, I also want to automate the download and importation of this survey data. In order to do that, I need to figure out how to programmatically log into this UK data service website.', 'The relevant data variables returned by the form are action and origin, not combobox. Give action the value selection and origin the value from the relevant entry in combobox', 'I think one way to address ""enter your organization"" page goes like this:']"
130,"Twitter: How to extract tweets containing symbols (!,%,$)?","For a project, I want to be able to create a dataset of tweets containing some particular string of symbols. Since I would also like to go as far back in time as possible, I tried using the ...",https://stackoverflow.com/questions/47424751/twitter-how-to-extract-tweets-containing-symbols,"['For a project, I want to be able to create a dataset of tweets containing some particular string of symbols. Since I would also like to go as far back in time as possible, I tried using the GetOldTweets script ( https://github.com/Jefferson-Henrique/GetOldTweets-python ) mentioned here: https://stackoverflow.com/a/35077920/5858873 .', 'You can create your own regular expression on the basis of your requirement and\nthen hit the twitter data to extract the specific tweets.', 'I found this interesting ressource : https://webapps.stackexchange.com/questions/92196/search-for-tweets-with-special-characters', ""You can download and store data from Twitter API using various criteria (search for words in a dictionary, location search, popular Twitter accounts etc) It won't be the whole data for sure but you will have some part of it.""]"
131,Scraping Google Keyword Tools with CasperJS and PhantomJS,"I'm currently trying to scrape Google Keyword Tools with CasperJS and PhantomJS (both excellent tools, thanks n1k0 and Ariya), but I can't get it to work.

Here is my current process:
Log in with my ...",https://stackoverflow.com/questions/9391943/scraping-google-keyword-tools-with-casperjs-and-phantomjs,"[""I'm currently trying to scrape Google Keyword Tools with CasperJS and PhantomJS (both excellent tools, thanks n1k0 and Ariya), but I can't get it to work."", ""You can't use elt.value on a textarea. Did you try with elt.textContent?"", ""Why do you try to scrape the results. Google already creating a csv file for us. \nTry downloading that. That links selector must be like $('.gux-combo gux-dropdown-c .sJK')\nWill you use that for automating things ?"", ""I'm not sure exactly what's happening here, but the classes that you're using for targeting are different for me.  The OneBoxKeywordsInputPanel-input textarea that I assume you're attempting to target has a second class, sPFB, and no other classes.  It's possible that these cryptic classes are dynamic in some way.  I'd recommend using the more descriptive class names instead.  The following works just fine for me:""]"
132,Scraping webpage generated by javascript with C#,"I have a webBrowser, and a label in Visual Studio, and basically what I'm trying to do is grab a section from another webpage.

I tried using WebClient.DownloadString and WebClient.DownloadFile, and ...",https://stackoverflow.com/questions/24288726/scraping-webpage-generated-by-javascript-with-c-sharp,"[""I have a webBrowser, and a label in Visual Studio, and basically what I'm trying to do is grab a section from another webpage."", 'The problem is the browser usually executes the javascript and it results with an updated DOM. Unless you can analyze the javascript or intercept the data it uses, you will need to execute the code as a browser would. In the past I ran into the same issue, I utilized selenium and PhantomJS to render the page. After it renders the page, I would use the WebDriver client to navigate the DOM and retrieve the content I needed, post AJAX.', 'ok i will show you how to enable javascript using phantomjs and selenuim with c#', 'Thanks to wbennet, discovered https://phantomjscloud.com. Enough free service to scrap pages through web api calls.']"
133,Fetching multiple urls with aiohttp in Python 3.5,"Since Python 3.5 introduced async with the syntax recommended in the docs for aiohttp has changed. Now to get a single url they suggest:

import aiohttp
import asyncio

async def fetch(session, url):
 ...",https://stackoverflow.com/questions/35879769/fetching-multiple-urls-with-aiohttp-in-python-3-5,"['Since Python 3.5 introduced async with the syntax recommended in the docs for aiohttp has changed. Now to get a single url they suggest:', 'For parallel execution you need an asyncio.Task']"
134,Scrape password-protected website in R,"I'm trying to scrape data from a password-protected website in R.  Reading around, it seems that the httr and RCurl packages are the best options for scraping with password authentication (I've also ...",https://stackoverflow.com/questions/24723606/scrape-password-protected-website-in-r,"[""I'm trying to scrape data from a password-protected website in R.  Reading around, it seems that the httr and RCurl packages are the best options for scraping with password authentication (I've also looked into the XML package)."", ""I don't have an account to test with, but maybe this will work:"", 'You can use RSelenium. I have used the dev version as you can run phantomjs without a Selenium Server.']"
135,How to webscrape secured pages in R (https links) (using readHTMLTable from XML package)?,"There are good answers on SO about how to use readHTMLTable from the XML package and I did that with regular http pages, however I am not able to solve my problem with https pages.

I am trying to ...",https://stackoverflow.com/questions/10692066/how-to-webscrape-secured-pages-in-r-https-links-using-readhtmltable-from-xml,"['There are good answers on SO about how to use readHTMLTable from the XML package and I did that with regular http pages, however I am not able to solve my problem with https pages.', 'The new package httr provides a wrapper around RCurl to make it easier to scrape all kinds of pages.', ""Using Andrie's great way to get past the https"", 'This is the function I have to deal with this problem.  Detects if https in url and uses httr if it is.']"
136,How to send cookie with scrapy CrawlSpider requests?,"I am trying to create this Reddit scrapper using Python's Scrapy framework. 

I have used the CrawSpider to crawl through Reddit and its subreddits. But, when I come across pages that have adult ...",https://stackoverflow.com/questions/32623285/how-to-send-cookie-with-scrapy-crawlspider-requests,"[""I am trying to create this Reddit scrapper using Python's Scrapy framework."", 'Okay. Try doing something like this.', '1.Using a dict:', 'You can also send it via header.', 'You could use the process_request parameter in the rule, something like:']"
137,Get HTML Code from a website after it completed loading,"I am trying to get the HTML Code from a specific website async with the following code:

var response = await httpClient.GetStringAsync(""url"");
But the problem is that the website usually takes ...",https://stackoverflow.com/questions/53898592/get-html-code-from-a-website-after-it-completed-loading,"['I am trying to get the HTML Code from a specific website async with the following code:', 'You are on the wrong direction. The referenced site has playlist api which returns json. you can get information from :', 'You could use Puppeteer-Sharp:', ""If there are things that load after, it means that they are generated by javascript code after page load (an ajax request for example), so no matter how long you wait, it won't have the content you want (because they are not in the source code when it loads)."", 'The thing to understand here is that when you read the response from the URL, all you will ever get is the raw response, in this case the HTML source code the server replied with.', 'I have checked out the website, data is loaded by javascript. You only can get the html using httpClient.GetStringAsync(""url"");.\nAs far as I know, there is no luck to get the elements what is manipulate by browser.']"
138,Pretend Firefox instead of Phantom.js,"When I try to scrap this site with Phantomjs, by default, Phantomjs send the following headers to server:

""name"":""User-Agent"",
""value"":""Mozilla/5.0 (Unknown; Linux i686) AppleWebKit/534.34 (KHTML, ...",https://stackoverflow.com/questions/18324125/pretend-firefox-instead-of-phantom-js,"['When I try to scrap this site with Phantomjs, by default, Phantomjs send the following headers to server:', 'Actually, is on page.settings. Do it before the open.']"
139,Scrapy with Privoxy and Tor: how to renew IP,"I am dealing with Scrapy, Privoxy and Tor. I have all installed and properly working. But Tor connects with the same IP everytime, so I can easily be banned. Is it possible to tell Tor to reconnect ...",https://stackoverflow.com/questions/45009940/scrapy-with-privoxy-and-tor-how-to-renew-ip,"['I am dealing with Scrapy, Privoxy and Tor. I have all installed and properly working. But Tor connects with the same IP everytime, so I can easily be banned. Is it possible to tell Tor to reconnect each X seconds or connections?', 'This blog post might help you a bit as it deals with the same issue.', 'But Tor connects with the same IP everytime']"
140,How can I handle Javascript in a Perl web crawler?,"I would like to crawl a website, the problem is, that its full of JavaScript things, such as buttons and such that when they are pressed, they do not change the URL, but the data on the page is ...",https://stackoverflow.com/questions/3769015/how-can-i-handle-javascript-in-a-perl-web-crawler,"['I would like to crawl a website, the problem is, that its full of JavaScript things, such as buttons and such that when they are pressed, they do not change the URL, but the data on the page is changed.', 'Another option might be Selenium with WWW::Selenium module', ""The WWW::Scripter module has a JavaScript plugin that may be useful. Can't say I've used it myself, however."", 'WWW::Mechanize::Firefox might be of use.  that way you can have Firefox handle the complex JavaScript issues and then extract the resultant html.', 'I would suggest HtmlUnit and Perl wrapper: WWW::HtmlUnit.']"
141,Scrape tables into dataframe with BeautifulSoup,"I'm trying to scrape the data from the coins catalog. 

There is one of the pages. I need to scrape this data into Dataframe 

So far I have this code:

import bs4 as bs
import urllib.request
import ...",https://stackoverflow.com/questions/50633050/scrape-tables-into-dataframe-with-beautifulsoup,"[""I'm trying to scrape the data from the coins catalog."", 'Try this', 'Pandas already has a built-in method to convert the table on the web to a dataframe:', 'Try:', ""Just a head's up... This part of Rakesh's code means that only HTML rows containing text will be included in the dataframe, as the rows don't get appended if row is an empty list:""]"
142,How to convert raw javascript object to python dictionary?,"When screen-scraping some website, I extract data from <script> tags.
The data I get is not in standard JSON format. I cannot use json.loads().  

# from
js_obj = '{x:1, y:2, z:3}'

# to
py_obj =...",https://stackoverflow.com/questions/24027589/how-to-convert-raw-javascript-object-to-python-dictionary,"['When screen-scraping some website, I extract data from <script> tags.\nThe data I get is not in standard JSON format. I cannot use json.loads().', ""This will likely not work everywhere, but as a start, here's a simple regex that should convert the keys into quoted strings so you can pass into json.loads.  Or is this what you're already doing?"", 'Not including objects', 'If you have node available on the system, you can ask it to evaluate the javascript expression for you, and print the stringified result. The resulting JSON can then be fed to json.loads:']"
143,Speed up web scraper,"I am scraping 23770 webpages with a pretty simple web scraper using scrapy. I am quite new to scrapy and even python, but managed to write a spider that does the job. It is, however, really slow (it ...",https://stackoverflow.com/questions/17029752/speed-up-web-scraper,"['I am scraping 23770 webpages with a pretty simple web scraper using scrapy. I am quite new to scrapy and even python, but managed to write a spider that does the job. It is, however, really slow (it takes approx. 28 hours to crawl the 23770 pages).', ""Here's a collection of things to try:"", ""Looking at your code, I'd say most of that time is spent in network requests rather than processing the responses. All of the tips @alecxe provides in his answer apply, but I'd suggest the HTTPCACHE_ENABLED setting, since it caches the requests and avoids doing it a second time. It would help on following crawls and even offline development. See more info in the docs: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.contrib.downloadermiddleware.httpcache"", 'I work also on web scrapping, using optimized C#, and it ends up CPU bound, so I am switching to C.', 'One workaround to speed up your scrapy is to config your start_urls appropriately.']"
144,How do I write a web scraper in Ruby?,"I would like to crawl a popular site (say Quora) that doesn't have an API and get some specific information and dump it into a file - say either a csv, .txt, or .html formatted nicely :)

E.g. return ...",https://stackoverflow.com/questions/5947096/how-do-i-write-a-web-scraper-in-ruby,"[""I would like to crawl a popular site (say Quora) that doesn't have an API and get some specific information and dump it into a file - say either a csv, .txt, or .html formatted nicely :)"", ""Your best bet would be to use Mechanize.It can follow links, submit forms, anything you will need, web client-wise. By the way, don't use regexes to parse HTML. Use an HTML parser."", 'If you want something more high level, try wombat, which is this gem I built on top of Mechanize and Nokogiri. It is able to parse pages and follow links using a really simple and high level DSL.', 'I know the answer has been accepted, but Hpricot is also very popular for parsing HTML.', ""Mechanize is awesome.  If you're looking to learn something new though, you could take a look at Scrubyt: https://github.com/scrubber/scrubyt.  It looks like Mechanize + Hpricot.  I've never used it, but it seems interesting."", 'Nokogiri is great, but I find the output messy to work with. I wrote a ruby gem to easily create classes off HTML: https://github.com/jassa/hyper_api']"
145,How can I scrape faster,"The work here is to scrape an API a site that starts from https://xxx.xxx.xxx/xxx/1.json to https://xxx.xxx.xxx/xxx/1417749.json and write it exactly to mongodb. For that I have the following code:

...",https://stackoverflow.com/questions/59317026/how-can-i-scrape-faster,"['The work here is to scrape an API a site that starts from https://xxx.xxx.xxx/xxx/1.json to https://xxx.xxx.xxx/xxx/1417749.json and write it exactly to mongodb. For that I have the following code:', ""asyncio is also a solution if you don't want to use multi threading"", 'There are several things that you could do:', 'You can improve your code on two aspects:', 'What you are probably looking for is asynchronous scraping. I would recommend you to create some batches of urls, ie 5 urls (try not to chrash the website), and scrape them asynchronous. If you dont know much about async, google for the libary asyncio. I hope I could help you :)', 'Try to chunk the requests and use the MongoDB bulk write operation.', ""Assuming that you won't get blocked by the API and that there are no rate limits, this code should make the process 50 times faster (maybe more because all requests are now sent using the same session)."", ""I happened to have the same question many years ago. I'm never satisfied with python-based answers, which are pretty slow or too complicated. After I switch to other mature tools, the speed is fast and I never come back."", 'First create list of all links because all are same just change iterate it.']"
146,How do I make a do block return early?,"I'm trying to scrape for a webpage using Haskell and compile the results into an object. 

If, for whatever reason, I can't get all the items from the pages, I want to stop trying to process the page ...",https://stackoverflow.com/questions/15441956/how-do-i-make-a-do-block-return-early,"[""I'm trying to scrape for a webpage using Haskell and compile the results into an object."", 'return in haskell does not do the same thing as return in other languages.  Instead, what return does is to inject a value into a monad (in this case IO).  You have a couple of options', 'Use a monad transformer!', ""I have never worked with Haskell, but it seems quitte easy. Try when (isNothing date) $ exit (). If this also isn't working, then make sure your statement is correct. Also see this website for more info: Breaking From loop.""]"
147,PhantomJS hanging when called from CLI or Web,"I'm trying to use phantomJS to capture a screenshot of a URL, however when I call phantomJS (from either the command line or web app) it hangs and seesm to never execute the ""exit()"" call.  I can't ...",https://stackoverflow.com/questions/16657744/phantomjs-hanging-when-called-from-cli-or-web,"['I\'m trying to use phantomJS to capture a screenshot of a URL, however when I call phantomJS (from either the command line or web app) it hangs and seesm to never execute the ""exit()"" call.  I can\'t seem to find any error messages and it remains running until I kill it.  This is the JS file that is passed to the phantomjs command:', ""If something goes wrong in your script (such as in page.render), phantom.exit() will never be called. That's why phantomJs seems to hang."", 'Use :']"
148,What is the meaning of [:] in python [duplicate],"What does the line del taglist[:] do in the code below?

import urllib
from bs4 import BeautifulSoup
taglist=list()
url=raw_input(""Enter URL: "")
count=int(raw_input(""Enter count:""))
position=int(...",https://stackoverflow.com/questions/39241529/what-is-the-meaning-of-in-python,"['What does the line del taglist[:] do in the code below?', '[:] is the array slice syntax for every element in the array.']"
149,An array field in scrapy.Item,"I want to add a field to scrapy.Item so that it's an array:

class MyItem(scrapy.Item):
    field1 = scrapy.Field()
    field2 = scrapy.Field()
    field3_array = ???
How can I do that?",https://stackoverflow.com/questions/29227119/an-array-field-in-scrapy-item,"[""I want to add a field to scrapy.Item so that it's an array:"", 'You just create a filed']"
150,Capture AJAX response with selenium and python,"I click on a link in Firefox, the webpage sends a request using javascript, then the server sends some sort of response which includes a website address. So this new website then opens in a new Window....",https://stackoverflow.com/questions/26481212/capture-ajax-response-with-selenium-and-python,"[""I click on a link in Firefox, the webpage sends a request using javascript, then the server sends some sort of response which includes a website address. So this new website then opens in a new Window. The html code behind the link is (I've omitted initial and final <span> tag):"", 'I once intercepted some ajax calls injecting javascript to the page using selenium. The bad side of the history is that selenium could sometimes be, let\'s say ""fragile"". So for no reason I got selenium exceptions while doing this injection.', 'I was unable to capture AJAX response with selenium but here is what works, although without selenium:', ""I've come up to this page when trying to catch XHR content based on AJAX requests.\nAnd  I eventually found this package""]"
151,What would be the most ethical way to consume content from a site that is not providing an API? [closed],"I was wondering what would be the most ethical way to consume some bytes (386 precisely) of content from a given Site A, with an application (e.g. Google App Engine) in some Site B, but doing it right,...",https://stackoverflow.com/questions/6394410/what-would-be-the-most-ethical-way-to-consume-content-from-a-site-that-is-not-pr,"['Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.', 'There will be some very differents points of view, but hopefully here is some food for thought:', ""Update (4 years later): The question specifically embraces the ethical side of the problem. That's why this old answer is written in this way."", ""I would not touch it save for emailing the site admin, then getting their written permission. \nThat being said -- if you're consuming the content yet not extracting value beyond the value\na single user gets when observing the data you need from them, it's arguable that any \nTOU they have wouldn't find you in violation.  If however you get noteworthy value beyond\nwhat a single user would get from the data you need from their site -- ie., let's say you use\nthe data then your results end up providing value to 100x of your own site's users -- I'd say\nyou need express permission to do that, to sleep well at night."", '""no scraping intended"" - You are intending to scrape. =)']"
152,Scrape web page contents,"I am developing a project, for which I want to scrape the contents of a website in the background and get some limited content from that scraped website. For example, in my page I have ""userid"" and ""...",https://stackoverflow.com/questions/584826/scrape-web-page-contents,"['I am developing a project, for which I want to scrape the contents of a website in the background and get some limited content from that scraped website. For example, in my page I have ""userid"" and ""password"" fields, by using those I will access my mail and scrape my inbox contents and display it in my page.', ""Definitely go with PHP Simple HTML DOM Parser. It's fast, easy and super flexible. It basically sticks an entire HTML page in an object then you can access any element from that object."", 'First, you make an HTTP request to get the content of the page. There are several ways to do that.', 'You can use the cURL extension of PHP to do HTTP requests to another web site from within your PHP page script. See the documentation here.', ""Have you tried OutWit Hub? It's a whole scraping environment. You can let it try to guess the structure or develop your own scrapers. I really suggest you have a look at it. It made my life much simpler.\nZR""]"
153,Jsoup Cookies for HTTPS scraping,"I am experimenting with this site to gather my username on the welcome page to learn Jsoup and Android.  Using the following code

Connection.Response res = Jsoup.connect(""http://www.mikeportnoy.com/...",https://stackoverflow.com/questions/7139178/jsoup-cookies-for-https-scraping,"['I am experimenting with this site to gather my username on the welcome page to learn Jsoup and Android.  Using the following code', ""I know I'm kinda late by 10 months here. But a good option using Jsoup is to use this easy peasy piece of code:"", 'I always do this in two steps (like normal human),', 'What if you try fetching and passing all cookies without assuming anything like this: Sending POST request with username and password and save session cookie']"
154,Scrape multiple pages with BeautifulSoup and Python,"My code successfully scrapes the tr align=center tags from [ http://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=201501&subjId=ACCY ] and writes the td elements to a text file. 

However, ...",https://stackoverflow.com/questions/26497722/scrape-multiple-pages-with-beautifulsoup-and-python,"['My code successfully scrapes the tr align=center tags from [ http://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=201501&subjId=ACCY ] and writes the td elements to a text file.', ""The trick here is to check the requests that are coming in and out of the page-change action when you click on the link to view the other pages. The way to check this is to use Chrome's inspection tool (via pressing F12) or installing the Firebug extension in Firefox. I will be using Chrome's inspection tool in this answer. See below for my settings.""]"
155,Using 'rvest' to extract links,"I am trying to scrape data from Yelp. One step is to extract links from each restaurant. For example, I search restaurants in NYC and get some results. Then I want to extract the links of all the 10 ...",https://stackoverflow.com/questions/35247033/using-rvest-to-extract-links,"['I am trying to scrape data from Yelp. One step is to extract links from each restaurant. For example, I search restaurants in NYC and get some results. Then I want to extract the links of all the 10 restaurants Yelp recommends on page 1. Here is what I have tried:', 'Hope this would simplify your problem', 'I also was able to clean the results from above which for me were quite noisy']"
156,How can I get all the plain text from a website with Scrapy?,"I would like to have all the text visible from a website, after the HTML is rendered. I'm working in Python with Scrapy framework.
With xpath('//body//text()') I'm able to get it, but with the HTML ...",https://stackoverflow.com/questions/23156780/how-can-i-get-all-the-plain-text-from-a-website-with-scrapy,"[""I would like to have all the text visible from a website, after the HTML is rendered. I'm working in Python with Scrapy framework.\nWith xpath('//body//text()') I'm able to get it, but with the HTML tags, and I only want the text. Any solution for this?"", 'The easiest option would be to extract //body//text() and join everything found:', 'Have you tried?', ""The xpath('//body//text()') doesn't always drive dipper into the nodes in your last used tag(in your case body.) If you type xpath('//body/node()/text()').extract() you will see the nodes which are in you html body. You can try xpath('//body/descendant::text()').""]"
157,Python Selenium Timeout Exception Catch,"In my threads I use a simple variable either set to '1' or '0' to indicate if it is ready to go again. Trying to debug an issue where sometimes this isn't being reset and I think I might have it.

I ...",https://stackoverflow.com/questions/36026676/python-selenium-timeout-exception-catch,"[""In my threads I use a simple variable either set to '1' or '0' to indicate if it is ready to go again. Trying to debug an issue where sometimes this isn't being reset and I think I might have it."", 'Almost of your code is working fine except the Driver.Close(). It should be Driver.close(). The TimeoutException will be thrown when the page is not loaded within specific time. See my code below:']"
158,Is it possible for Scrapy to get plain text from raw HTML data?,"For example:

scrapy shell http://scrapy.org/
content = hxs.select('//*[@id=""content""]').extract()[0]
print content
Then, I get the following raw HTML code:

<div id=""content"">
  <h2>...",https://stackoverflow.com/questions/17721782/is-it-possible-for-scrapy-to-get-plain-text-from-raw-html-data,"['For example:', ""Scrapy doesn't have such functionality built-in. html2text is what you are looking for."", 'Another solution using lxml.html\'s tostring() with parameter method=""text"". lxml is used in Scrapy internally. (parameter encoding=unicode is usually what you want.)', ""At this moment, I don't think you need to install any 3rd party library. scrapy provides this functionality using selectors:\nAssume this complex selector:""]"
159,Scraping ajax pages using python,"I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. ...",https://stackoverflow.com/questions/16390257/scraping-ajax-pages-using-python,"[""I've already seen this question about scraping ajax, but python isn't mentioned there. I considered using scrapy, i believe they have some docs on that subject, but as you can see the website is down. So i don't know what to do. I want to do the following:"", 'First of all, scrapy docs are available at https://scrapy.readthedocs.org/en/latest/.', 'I found the answer very useful but I would like to make it more simple.']"
160,Inherent way to save web page source,"I have read a lot of answers regarding web scraping that talk about BeautifulSoup, Scrapy e.t.c. to perform web scraping.

Is there a way to do the equivalent of saving a page's source from a web ...",https://stackoverflow.com/questions/13332192/inherent-way-to-save-web-page-source,"['I have read a lot of answers regarding web scraping that talk about BeautifulSoup, Scrapy e.t.c. to perform web scraping.', 'You may try urllib2:', 'Updated code, for Python 3 (where urllib2 is deprecated):', ""Answer from SoHei will not work because it's missing html.read() and the file must be opened with 'wb' parameter instead of just a 'w'. The 'b' indicates that data will be written in binary mode (since .read() returns sequence of bytes). \nThe fully working code is:""]"
161,Using HTMLParser in Python 3.2,"I have been using HTML Parser to scrapping data from websites and stripping html coding whilst doing so. I'm aware of various modules such as Beautiful Soup, but decided to go down the path of not ...",https://stackoverflow.com/questions/11061058/using-htmlparser-in-python-3-2,"['I have been using HTML Parser to scrapping data from websites and stripping html coding whilst doing so. I\'m aware of various modules such as Beautiful Soup, but decided to go down the path of not depending on ""outside"" modules. There is a code code supplied by Eloff: Strip HTML from strings in Python', ""You're subclassing HTMLParser, but you aren't calling its __init__ method. You need to add one line to your __init__ method:""]"
162,Python selenium error when trying to launch firefox,I am getting an error when trying to open Firefox using Selenium in ipython notebook. I've looked around and have found similar errors but nothing that exactly matches the error I'm getting. Anybody ...,https://stackoverflow.com/questions/17580730/python-selenium-error-when-trying-to-launch-firefox,"[""I am getting an error when trying to open Firefox using Selenium in ipython notebook. I've looked around and have found similar errors but nothing that exactly matches the error I'm getting. Anybody know what the problem might be and how I fix it? I'm using Firefox 22."", 'Try specify your Firefox binary when initialize Firefox()', ""Problem is ocuring because you don't have geckodriver"", 'Requirements:', 'I got the same error when I set environment variable export PYTHONDONTWRITEBYTECODE=1 to get rid of pyc files on every test run. I was able to revert the change by updating selenium pip install --upgrade selenium. OSX (10.10)', 'This is what worked:', 'those 2 packages are needed (ubuntu)!', 'You are required to install geckodriver:']"
163,Yield Request call produce weird result in recursive method with scrapy,"I'm trying to scrap all departures and arrivals in one day from all airports in all country using Python and Scrapy.

The JSON database used by this famous site (flight radar) need to query page by ...",https://stackoverflow.com/questions/43667622/yield-request-call-produce-weird-result-in-recursive-method-with-scrapy,"[""I'm trying to scrap all departures and arrivals in one day from all airports in all country using Python and Scrapy."", 'Comment: ... not totally clear ... you call AirportData(response, 1) ... also a little typo here : self.pprint(schedule)', 'Yes, i finally found the answer here on SO ...', 'I tried cloning locally and investigate a little better, but when it gets to the departure parsing I got some ConnectionRefused error so I am not sure my proposed answer will fix it, anyhow:']"
164,How do I set up Scrapy to deal with a captcha,I'm trying to scrape a site that requires the user to enter the search value and a captcha. I've got an optical character recognition (OCR) routine for the captcha that succeeds about 33% of the time. ...,https://stackoverflow.com/questions/39137559/how-do-i-set-up-scrapy-to-deal-with-a-captcha,"['I\'m trying to scrape a site that requires the user to enter the search value and a captcha. I\'ve got an optical character recognition (OCR) routine for the captcha that succeeds about 33% of the time. Since the captchas are always alphabetic text, I want to reload the captcha if the OCR function returns non-alphabetic characters. Once I have a text ""word"", I want to submit the search form.', ""It's a really deep topic with a bunch of solutions. But if you want to apply the logic you've defined in your post you can use scrapy Downloader Middlewares.""]"
165,Submit data via web form and extract the results,My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to ...,https://stackoverflow.com/questions/8377055/submit-data-via-web-form-and-extract-the-results,"[""My python level is Novice. I have never written a web scraper or crawler. I have written a python code to connect to an api and extract the data that I want. But for some the extracted data I want to get the gender of the author. I found this web site http://bookblog.net/gender/genie.php but downside is there isn't an api available. I was wondering how to write a python to submit data to the form in the page and extract the return data. It would be a great help if I could get some guidance on this."", 'No need to use mechanize, just send the correct form data in a POST request.', 'You can use mechanize to submit and retrieve content, and the re module for getting what you want. For example, the script below does it for the text of your own question:', 'You can use mechanize, see examples for details.']"
166,Following hyperlink and “Filtered offsite request”,"I know that there are several related threads out there, and they have helped me a lot, but I still can't get all the way. I am at the point where running the code doesn't result in errors, but I get ...",https://stackoverflow.com/questions/17862474/following-hyperlink-and-filtered-offsite-request,"[""I know that there are several related threads out there, and they have helped me a lot, but I still can't get all the way. I am at the point where running the code doesn't result in errors, but I get nothing in my csv file. I have the following Scrapy spider that starts on one webpage, then follows a hyperlink, and scrapes the linked page:"", 'You need to modify your yielded Request in parse to use parse2 as its callback.', 'try make this dont_filter=true']"
167,Beautifulsoup 4: Remove comment tag and its content,"So the page that I'm scrapping contains these html codes. How do I remove the comment tag <!-- --> along with its content with bs4?

<div class=""foo"">
cat dog sheep goat
<!-- 
<p>...",https://stackoverflow.com/questions/23299557/beautifulsoup-4-remove-comment-tag-and-its-content,"[""So the page that I'm scrapping contains these html codes. How do I remove the comment tag <!-- --> along with its content with bs4?"", 'You can use extract() (solution is based on this answer):', ""Usually modifying the bs4 parse tree is unnecessary.  You can just get the div's text, if that's what you wanted:"", 'From this answer\nIf you are looking for solution in BeautifulSoup version 3 BS3 Docs - Comment']"
168,Scraping data from all asp.net pages with AJAX pagination implemented,"I want to scrap a webpage containing a list of user with addresses, email etc. webpage contain list of user with pagination i.e. page contains 10 users when I click on page 2 link it will load users ...",https://stackoverflow.com/questions/14768302/scraping-data-from-all-asp-net-pages-with-ajax-pagination-implemented,"['I want to scrap a webpage containing a list of user with addresses, email etc. webpage contain list of user with pagination i.e. page contains 10 users when I click on page 2 link it will load users list form 2nd page via AJAX and update list so on for all pagination links.', 'In general, in order to fake the ASP.NET web site to think that you actually pressed a button (in more general terms - performed a postback), you need to do the following:', 'My best advice is to use iMacros https://addons.mozilla.org/en-US/firefox/addon/imacros-for-firefox/', 'I would recommend branching out into Ruby and trying Capybara which is a sane way of using Selenium. It lets you do a visit of a page, then examine the actual DOM. You can click on everything, wait for events, etc. It uses a real browser.', 'I got some test code working using yours as a basis and the only issue I found was this line.']"
169,Scrape title by only downloading relevant part of webpage,I would like to scrape just the title of a webpage using Python. I need to do this for thousands of sites so it has to be fast. I've seen previous questions like retrieving just the title of a webpage ...,https://stackoverflow.com/questions/44123552/scrape-title-by-only-downloading-relevant-part-of-webpage,"[""I would like to scrape just the title of a webpage using Python. I need to do this for thousands of sites so it has to be fast. I've seen previous questions like retrieving just the title of a webpage in python, but all of the ones I've found download the entire page before retrieving the title, which seems highly inefficient as most often the title is contained within the first few lines of HTML."", 'You can defer downloading the entire response body by enabling stream mode of requests.', 'Question: ... the only place I can optimize is likely to not read in the entire page.', ""You're scraping webpages using standard REST requests and I'm not aware of any request that only returns the title, so I don't think it's possible."", 'the kind of thing you want i don\'t think can be done, since the way the web is set up, you get the response for a request before anything is parsed. there isn\'t usually a streaming ""if encounter <title> then stop giving me data"" flag. if there is id love to see it, but there is something that may be able to help you. keep in mind, not all sites respect this. so some sites will force you to download the entire page source before you can act on it. but a lot of them will allow you to specify a range header. so in a requests example:', 'using urllib you can set the Range header to request a certain range of bytes, but there are some consequences:', 'my code also solves cases when title tag is splitted between chunks.']"
170,web scraping google news with python,"I am creating a web scraper for different news outlets, for Nytimes and the Guardian it was easy since they have their own API.

Now, I want to scrape results from this newspaper GulfTimes.com. They ...",https://stackoverflow.com/questions/15550655/web-scraping-google-news-with-python,"['I am creating a web scraper for different news outlets, for Nytimes and the Guardian it was easy since they have their own API.', 'You can use awesome requests library:', 'Disclosure: I work at SerpApi.', 'hi you can scrap like this with easy way']"
171,Items vs item loaders in scrapy,"I'm pretty new to scrapy, I know that items are used to populate scraped data, but I cant understand the difference between items and item loaders. I tried to read some example codes, they used item ...",https://stackoverflow.com/questions/39127256/items-vs-item-loaders-in-scrapy,"[""I'm pretty new to scrapy, I know that items are used to populate scraped data, but I cant understand the difference between items and item loaders. I tried to read some example codes, they used item loaders to store instead of items and I can't understand why. Scrapy documentation wasn't clear enough for me. Can anyone give a simple explanation (better with example) about when item loaders are used and what additional facilities do they provide over items ?"", 'I really like the official explanation in the docs:']"
172,Unable to make my script work asynchronously,"I've written a script in vba to scrape different movie names and their genre from a torrent site. Although the name and genre are present in it's landing page, I created the script to parse the same ...",https://stackoverflow.com/questions/55219181/unable-to-make-my-script-work-asynchronously,"[""I've written a script in vba to scrape different movie names and their genre from a torrent site. Although the name and genre are present in it's landing page, I created the script to parse the same going one layer deep (from their main pages). To be clearer, this is one of such page what I meant by main page. My script is parsing them flawlessly. However, my intention is to do the same making asynchronous requests. Currently the script is doing it's job synchronously (in blocking manner)."", 'Here is the example showing single loop parser implementation with async requests pool. The code parses all Browse Pages from the first to last and Movies Pages, both types are parsed simultaneously. Movies URLs are parsed from Browse Pages and placed in Movies Queue, then details from each Movie Page from the queue is parsed and output to the worksheet. It handles all HTTP requests errors types and makes retry until the limit.', 'This code should do the trick. It uses a MSXML2.XMLHTTP object to handle the request.', 'The base for my answer is this post mentioned by @Louis, where only one call is performed, but you need many. I was very surprised about how quicker the GetInfoAsync method was.']"
173,Scrapy Vs Nutch [closed],I am planning to use webcrawling in an application i am currently working on. I did some research on Nutch and run some preliminary test using it. But then i came across scrapy. But when i did some ...,https://stackoverflow.com/questions/17199457/scrapy-vs-nutch,"['Want to improve this question? Update the question so it focuses on one problem only by editing this post.', 'Scrapy would work perfectly in your case.']"
174,Website does not recognize my inputs [how to fire IE dom event manually from VBA],"I woud like to buy on gdax automatically. But my inputs in the Amount window doesn´t get recognized. I can see that on the little field, that says: Total (LTC) ≈ 0.00000000

My code:

Sub test()

    ...",https://stackoverflow.com/questions/48611570/website-does-not-recognize-my-inputs-how-to-fire-ie-dom-event-manually-from-vba,"['I woud like to buy on gdax automatically. But my inputs in the Amount window doesn´t get recognized. I can see that on the little field, that says: Total (LTC) ≈ 0.00000000', 'You need to make sure the page is fully loaded before you take any initiative. I checked for the availability of one such class generated dynamically OrderBookPanel_text_33dbp. Then I did the rest what you tried to do. Lastly, you need to Focus the target before putting that amount in the placeholder. Applying all of the above, your script should more like below:', 'Not a complete answer, just a direction. Open the webpage https://www.gdax.com/trade/LTC-EUR in a browser (e. g. Chrome). Click to open context menu on the target element (step 1 on the below screenshot), click Inspect (2), from opened  developer tools on the right you can see target element (3), and that there is a bunch of event listeners attached to target object (4). One of the handlers is input on the document node level. Actually the amount is updated by this event handler, when event is bubbled from <input> node. You can easily check that by deleting all other event handlers (click on their Remove buttons). But if you delete particularly this input handler (5) then there will be no update (until you reload the webpage).', 'Sorry could not get to work but have nudged along the problem.']"
175,HtmlAgilityPack & Selenium Webdriver returns random results,"I'm trying to scrape product names from a website.  Oddly, I seem to only scrape random 12 items.  I've tried both HtmlAgilityPack and with HTTPClient and I get the same random results.  Here's my ...",https://stackoverflow.com/questions/45243042/htmlagilitypack-selenium-webdriver-returns-random-results,"[""I'm trying to scrape product names from a website.  Oddly, I seem to only scrape random 12 items.  I've tried both HtmlAgilityPack and with HTTPClient and I get the same random results.  Here's my code for HtmlAgilityPack:"", 'Since the v1.5.0-beta92,', 'So there are a couple issues that prevent the count from being correct.', 'As others said, the page from this site loads itself dynamically using some javascript, so the Html Agility Pack just gets the first items.', ""For most single page apps or pages that load content dynamically you better off using an actual browser to navigate the pages. I'd suggest looking into selenium for this type of setup.""]"
176,python clicking a button on a webpage,"I currently have a script that logs me into a website and I want to have it click a button on the website if it is currently not clicked. Here is the info for the button:

When the button is already ...",https://stackoverflow.com/questions/27869225/python-clicking-a-button-on-a-webpage,"['I currently have a script that logs me into a website and I want to have it click a button on the website if it is currently not clicked. Here is the info for the button:', ""When I compare the two tags I see that the difference is for the class tag.\nSo if you can read it then you're done""]"
177,Can't click on some dots to scrape information,"I've written a script in vba in combination with IE to click on some dots available on a map in a web page. When a dot is clicked, a small box containing relevant information pops up.
Link to that ...",https://stackoverflow.com/questions/51526683/cant-click-on-some-dots-to-scrape-information,"[""I've written a script in vba in combination with IE to click on some dots available on a map in a web page. When a dot is clicked, a small box containing relevant information pops up."", 'No need to click on each dots. Json file has all the details and you can extract as per your requirement.']"
178,Can't fetch the profile name using Selenium after logging in using requests,I've written a script in Python to get only the name visible in my profile in SO. The thing is I would like to log in that site using requests module and once I'm logged in I wish to get the profile ...,https://stackoverflow.com/questions/54555439/cant-fetch-the-profile-name-using-selenium-after-logging-in-using-requests,"[""I've written a script in Python to get only the name visible in my profile in SO. The thing is I would like to log in that site using requests module and once I'm logged in I wish to get the profile name using Selenium. The bottom line is -- when I get the profile URL, I would like that URL to be reused by Selenium to fetch the profile name."", ""It's probably more appropriate to use the Stack Exchange API rather than scrape the site, but in any case.."", 'You need to be on the domain that the cookie will be valid for.']"
179,R: extracting “clean” UTF-8 text from a web page scraped with RCurl,"Using R, I am trying to scrape a web page save the text, which is in Japanese, to a file. Ultimately this needs to be scaled to tackle hundreds of pages on a daily basis. I already have a workable ...",https://stackoverflow.com/questions/11069908/r-extracting-clean-utf-8-text-from-a-web-page-scraped-with-rcurl,"[""Using R, I am trying to scrape a web page save the text, which is in Japanese, to a file. Ultimately this needs to be scaled to tackle hundreds of pages on a daily basis. I already have a workable solution in Perl, but I am trying to migrate the script to R to reduce the cognitive load of switching between multiple languages. So far I am not succeeding. Related questions seem to be this one on saving csv files and this one on writing Hebrew to a HTML file. However, I haven't been successful in cobbling together a solution based on the answers there. Edit: this question on UTF-8 output from R is also relevant but was not resolved."", 'I seem to have found an answer and nobody else has yet posted one, so here goes.', 'Hi I have wrote a scraping engine that allows for the scraping of data on webpages that are deeply embedded within the main listing page. I wonder if it might be helpful to use it as an aggregator for your web data prior to importing in R?']"
180,Scraping data from website using vba,"Im trying to scrape data from website: http://uk.investing.com/rates-bonds/financial-futures via vba, like real-time price, i.e. German 5 YR Bobl, US 30Y T-Bond, i have tried excel web query but it ...",https://stackoverflow.com/questions/27066963/scraping-data-from-website-using-vba,"['Im trying to scrape data from website: http://uk.investing.com/rates-bonds/financial-futures via vba, like real-time price, i.e. German 5 YR Bobl, US 30Y T-Bond, i have tried excel web query but it only scrapes the whole website, but I would like to scrape the rate only, is there a way of doing this?', 'There are several ways of doing this. This is an answer that I write hoping that all the basics of Internet Explorer automation will be found when browsing for the keywords ""scraping data from website"", but remember that nothing\'s worth as your own research (if you don\'t want to stick to pre-written codes that you\'re not able to customize).', ""Other methods were mentioned so let us please acknowledge that, at the time of writing, we are in the 21st century. Let's park the local bus browser opening, and fly with an XMLHTTP GET request (XHR GET for short)."", ""you can use winhttprequest object instead of internet explorer as it's good to load data excluding pictures n advertisement instead of downloading full webpage including advertisement n pictures those make internet explorer object heavy compare to winhttpRequest object."", 'This question asked long before. But I thought following information will useful for newbies. Actually you can easily get the values from class name like this.', 'I modified some thing that were poping up error for me and end up with this which worked great to extract the data as I needed:']"
181,How to make Scrapy show user agent per download request in log?,"I am learning Scrapy, a web crawling framework.  

I know I can set USER_AGENT in settings.py file of the Scrapy project. When I run the Scrapy, I can see the USER_AGENT's value in INFO logs.
This ...",https://stackoverflow.com/questions/23152739/how-to-make-scrapy-show-user-agent-per-download-request-in-log,"['I am learning Scrapy, a web crawling framework.', 'Just FYI.', 'You can see it by using this:', ""You can add logging to solution you're using:"", 'EDIT: I came here because I was looking for the functionality to change the user agent.']"
182,Selenium Webdriver vs Mechanize,I am interested in automating repetitive data entry in some forms for a website I frequent. So far the tools I've looked up that would provide support for this in a headless fashion could be Selenium ...,https://stackoverflow.com/questions/31530335/selenium-webdriver-vs-mechanize,"[""I am interested in automating repetitive data entry in some forms for a website I frequent. So far the tools I've looked up that would provide support for this in a headless fashion could be Selenium WebDriver and Mechanize."", 'These are completely different tools that somewhat ""cross"" in the web-scraping, web automation, automated data extraction scope.']"
183,How find specific data attribute from html tag in BeautifulSoup4?,"Is there a way to find an element using only the data attribute in html, and then grab that value?

For example, with this line inside an html doc:

<ul data-bin=""Sdafdo39"">
How do I retrieve ...",https://stackoverflow.com/questions/24197922/how-find-specific-data-attribute-from-html-tag-in-beautifulsoup4,"['Is there a way to find an element using only the data attribute in html, and then grab that value?', 'You can use find_all method to get all the tags and filtering based on ""data-bin"" found in its attributes will get us the actual tag which has got it. Then we can simply extract the value corresponding to it, like this', 'A little bit more accurate', 'You could solve this with gazpacho in just a couple of lines:']"
184,Managing puppeteer for memory and performance,"I'm using puppeteer for scraping some pages, but I'm curious about how to manage this in production for a node app. I'll be scraping up to 500,000 pages in a day, but these scrape jobs will happen at ...",https://stackoverflow.com/questions/51971760/managing-puppeteer-for-memory-and-performance,"[""I'm using puppeteer for scraping some pages, but I'm curious about how to manage this in production for a node app. I'll be scraping up to 500,000 pages in a day, but these scrape jobs will happen at random intervals, so it's not a single queue that I can plow through."", 'If you are scraping 500,000 pages per day (approximately one page every 0.1728 seconds), then I would recommend opening a new page in an existing browser session rather than opening a new browser session for each page.', 'You probably want to create a pool of multiple Chromium instances with independent browsers. The advantage of that is, when one browser crashes all other jobs can keep running. The advantage of one browser (with multiple pages) is a slight memory and CPU advantage and the cookies are shared between your pages.', 'Other performance related articles are,']"
185,CasperJS loop or iterate through multiple web pages?,I have a CasperJS script that scrapes ratings and dates from one webpage. Now I want to scrape the same data from multiple pages under the same website. How can I loop through the different subpages ...,https://stackoverflow.com/questions/23384963/casperjs-loop-or-iterate-through-multiple-web-pages,"['I have a CasperJS script that scrapes ratings and dates from one webpage. Now I want to scrape the same data from multiple pages under the same website. How can I loop through the different subpages given this code:', 'Since there exists a next page button, you can use it to traverse all pages recursively:', 'This code can help you :\nyou define in an array of objects the wanted urls, selectors for each page and in a loop you do what you want to do with these properties.', ""Thanks Fanch and Artjom B. Both of your answers rendered the working solution. I used the recursive walk through the 'next' pages on the pagination as given by Artjom B. Next, I added a wait() function to make sure the next ratings page was loaded before scraping them. Without this wait() function, we scrape the same page multiple times between the instant that 'next' is clicked and the resp. next page is done loading. See the working code below:""]"
186,Running selenium behind a proxy server,"I have been using selenium for automatic browser simulations and web scraping in python and it has worked well for me. But now, I have to run it behind a proxy server. So now selenium open up the ...",https://stackoverflow.com/questions/17988821/running-selenium-behind-a-proxy-server,"['I have been using selenium for automatic browser simulations and web scraping in python and it has worked well for me. But now, I have to run it behind a proxy server. So now selenium open up the window but could not open the requested page because of proxy settings not set on the opened browser. Current code is as follows (sample):', 'You need to set desired capabilities or browser profile, like this:', 'The official Selenium documentation (http://docs.seleniumhq.org/docs/04_webdriver_advanced.jsp#using-a-proxy) provides clear and helpful guidelines about using a proxy.\nFor Firefox (which is the browser of choice in your sample code) you should do the following:', 'This will do the job:']"
187,rvest how to select a specific css node by id,"I'm trying to use the rvest package to scrape data from a web page. In a simple format, the html code looks like this:

<div class=""style"">
   <input id=""a"" value=""123"">
   <input id=""b""...",https://stackoverflow.com/questions/32127921/rvest-how-to-select-a-specific-css-node-by-id,"[""I'm trying to use the rvest package to scrape data from a web page. In a simple format, the html code looks like this:"", 'You can use xpath:', 'This will work just fine with straight CSS selectors:', ""Adding an answer bc I don't see the easy css selector shorthand for selecting by id: using #your_id_name:""]"
188,Scraping in Python - Preventing IP ban,"I am using Python to scrape pages. Until now I didn't have any complicated issues.

The site that I'm trying to scrape uses a lot of security checks and have some mechanism to prevent scraping. 

...",https://stackoverflow.com/questions/35133200/scraping-in-python-preventing-ip-ban,"[""I am using Python to scrape pages. Until now I didn't have any complicated issues."", 'If you would switch to the Scrapy web-scraping framework, you would be able to reuse a number of things that were made to prevent and tackle banning:', 'You could use proxies.', 'I had this problem too. I used urllib with tor in python3.']"
189,Beautifulsoup: parsing html – get part of href,"I'm trying to parse

<td height=""16"" class=""listtable_1""><a href=""http://steamcommunity.com/profiles/76561198134729239"" target=""_blank"">76561198134729239</a></td>
for the ...",https://stackoverflow.com/questions/41720896/beautifulsoup-parsing-html-get-part-of-href,"[""I'm trying to parse"", 'There are many such entries in that HTML. To get all of them you could use the following:', '""target"":""_blank"" is a class of anchor tag a within the td tag. It\'s not a class of td tag.', 'As others mentioned you are trying to check attributes of different elements in a single find(). Instead, you can chain find() calls as MYGz suggested, or use a single CSS selector:', '""class"":""listtable_1"" belong to td tag and target=""_blank"" belong to a tag, you should not use them together.', 'You could chain together two finds in gazpacho to solve this problem:']"
190,Running selenium browser on server (Flask/Python/Heroku),"I am scraping some websites that seem to have pretty good protection against it. The only way I can get it to work is to use Selenium to load the page and then scrape stuff from that. 

Currently this ...",https://stackoverflow.com/questions/15904035/running-selenium-browser-on-server-flask-python-heroku,"['I am scraping some websites that seem to have pretty good protection against it. The only way I can get it to work is to use Selenium to load the page and then scrape stuff from that.', 'Heroku, wonderful as it is, has a major limitation in that one cannot use custom software or in many cases, libraries. In providing an easy to use, centrally-controlled, managed stack, Heroku strips their servers down to prevent other usage.', 'There are buildpacks to make selenium work on heroku.']"
191,What is the Java equivalent to PhantomJS? [closed],I would like to know whether there is any Java library equivalent to PhantomJS. What I want to achieve is to be able to simulate form login and submit actions from a web page and also to do page ...,https://stackoverflow.com/questions/19759421/what-is-the-java-equivalent-to-phantomjs,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'There is a PhantomJS driver for Java called GhostDriver. Maybe this suits your requirements?', 'Selenium with Ghostdriver/PhantomJS (This is good but there is an issue with automating file uploads for a website. Other automation activites work like a charm!)', ""Here's a similar question and answer with a list of options for you. Not all are written in Java, but there may be some suitable options for you.""]"
192,Prevent CSS/other resource download in PhantomJS/Selenium driven by Python,"I'm trying to speed up Selenium/PhantomJS webscraper in Python by preventing download of CSS/other resources. All I need to download is img src and alt tags. I've found this code: 

page....",https://stackoverflow.com/questions/19099070/prevent-css-other-resource-download-in-phantomjs-selenium-driven-by-python,"[""I'm trying to speed up Selenium/PhantomJS webscraper in Python by preventing download of CSS/other resources. All I need to download is img src and alt tags. I've found this code:"", 'A bold young soul by the name of “watsonmw” recently added functionality to Ghostdriver (which Phantom.js uses to interface with Selenium) that allows access to Phantom.js API calls which require a page object, like the onResourceRequested one you cited.', ""Will's answer got me on track. (Thanks Will!)"", ""Proposed solutions didn't work for me, but this one works (it uses driver.execute_script):""]"
193,VBA - XMLHTTP and WinHttp request speed,"Below are declared variables for 3 requests which I implement in my macros. I listed libraries they use and their late bindings in comments:

Dim XMLHTTP As New MSXML2.XMLHTTP 'Microsoft XML, v6.0 '...",https://stackoverflow.com/questions/41523223/vba-xmlhttp-and-winhttp-request-speed,"['Below are declared variables for 3 requests which I implement in my macros. I listed libraries they use and their late bindings in comments:', ""In addition to the methods you've mentioned:"", 'Most of the time is spent waiting for a response from the server. So if you want improve the execution time, then send the requests in parallel.']"
194,Click an item in autocomplete list with VBA and HTML,"I've created an automation where it will allow me to enter details on a website (though I cannot share it as it is internal). My code below is working only until it enters a text on ""received from"". ...",https://stackoverflow.com/questions/60183571/click-an-item-in-autocomplete-list-with-vba-and-html,"['I\'ve created an automation where it will allow me to enter details on a website (though I cannot share it as it is internal). My code below is working only until it enters a text on ""received from"". However, this ""received from"" field has an autocomplete list and I need to select it in order to populate other fields such as TIN and Address.', 'Good question!', 'I hope the example page is real quite similar to your page. I used it to show how you can deal with those things. What you need to have in your head is thinking about the dynamik of a page. I have commented on the macro in detail. Please read everything carefully to understand it. The solution consists of the following 3 parts. Copy all into one module.', 'See how the LI item contains a  tag. You should try clicking the first child.']"
195,CSV Exports - Ordering of columns using scrapy crawl -o output.csv,"Is there a way to specify the order of the columns in a CSV output using the -o parameter?

It seems to follow a random order and does not follow the order in the items.py file or when the item fields ...",https://stackoverflow.com/questions/28368912/csv-exports-ordering-of-columns-using-scrapy-crawl-o-output-csv,"['Is there a way to specify the order of the columns in a CSV output using the -o parameter?', 'There is a relevant field_to_export attribute in CsvItemExporter, but, as far as I understand, there is no way to set it from the command-line. You need to do it through the pipeline, see:', 'You can solve it from adding a line in settings.py']"
196,Remove all backslashes in Javascript,"How to remove all backslash in a JavaScript string ?

var str = ""one thing\\\'s for certain: power blackouts and surges can damage your equipment."";
I want output like 

one thing's for certain: ...",https://stackoverflow.com/questions/16171320/remove-all-backslashes-in-javascript,"['How to remove all backslash in a JavaScript string ?', 'Use a simple regex to solve it', 'This is the answer according to the title as the title is ""Remove all slashes in Javascript"" not backslashes. So here is the code to remove all the slashes from a string in JavaScript.', ""Regexs are great str.replace(/\\\\/g,'')"", 'How do I do string replace in JavaScript to convert ‘9.61’ to ‘9:61’?']"
197,selenium webdriver to find the anchor tag and click that,"<div id=""ContentPrimary"">
<ul class=""selectors modeSelectors"">
    <li><a href=""/content/l411846326l1213g/references/"" title="""">
        <span class=""selector"">References ...",https://stackoverflow.com/questions/10773944/selenium-webdriver-to-find-the-anchor-tag-and-click-that,"['In this I need to find and click About link using Selenium api but I was unable to do it.', 'In my experience the Selenium API has many flaws in that way. They can mostly only be overcome by reformulating your selectors. For example you could try using a XPath selector to get your element:', 'You can use ExpectedConditions:']"
198,Puppeteer - Protocol error (Page.navigate): Target closed,"As you can see with the sample code below, I'm using Puppeteer with a cluster of workers in Node to run multiple requests of websites screenshots by a given URL:

const cluster = require('cluster');
...",https://stackoverflow.com/questions/51629151/puppeteer-protocol-error-page-navigate-target-closed,"[""As you can see with the sample code below, I'm using Puppeteer with a cluster of workers in Node to run multiple requests of websites screenshots by a given URL:"", 'When you launch a browser via puppeteer.launch it will start a browser and connect to it. From there on any function you execute on your opened browser (like page.goto) will be send via the Chrome DevTools Protocol to the browser. A target means a tab in this context.', 'I was just experiencing the same issue every time I tried running my puppeteer script*. The above did not resolve this issue for me.', 'Check your jest-puppeteer.config.js file.\nI made the below mistake']"
199,Scraping with Meteor.js,"Can I scrape with meteor.js? Just discovered cheerio which works excellent combined with request. Can I use these with meteor, or is there something similar?

Do you have an working example?",https://stackoverflow.com/questions/14787552/scraping-with-meteor-js,"['Can I scrape with meteor.js? Just discovered cheerio which works excellent combined with request. Can I use these with meteor, or is there something similar?', ""Of course! Its hard to imagine what meteor can't do! First you need something to handle the remote http requests. In your meteor directory in the terminal run meteor add http to add the Meteor.http package, also npm install cheerio (have a look at another SO question on how to install npm modules to see exactly where to install external npm modules."", 'The following code is used in this project to scrape a tweetstorm:', 'You can have a look to http://casperjs.org/ which is very useful. You can also do screenshots, automated test, etc...', 'now you should use meteorhacks npm package https://github.com/meteorhacks/npm\nand require this with :']"
200,Facebook meta tags scraped with locale not working,"My website is multi-language and I have a FB like button. I'd like to have the like posts in different languages.

According to Facebook documentation, if I use the meta tag og:locale and og:locale:...",https://stackoverflow.com/questions/7614603/facebook-meta-tags-scraped-with-locale-not-working,"[""My website is multi-language and I have a FB like button. I'd like to have the like posts in different languages."", 'I got this working. The documentation is not very detailed; here are details.', ""Facebook's locale handling is completely inconsistent"", 'I had the same issues until finally getting it to work by setting all locale values in the meta tags (og:locale and og:locale:alternate) in lowercase.', ""What language did you specify when loading the Javascript SDK?  It's easy to overlook that one.'"", 'I have the same problem.']"
201,Check if a large file exists without downloading it,"Not sure if this is possible, but I would like to check the status code of an HTTP request to a large file without downloading it; I just want to check if it's present on the server.

Is it possible ...",https://stackoverflow.com/questions/41546386/check-if-a-large-file-exists-without-downloading-it,"[""Not sure if this is possible, but I would like to check the status code of an HTTP request to a large file without downloading it; I just want to check if it's present on the server."", 'use requests.head(), this only return the header of requests, not all content, in other word, it will not renturn the body of message, but you can get all the information from header.', 'Normally, you use HEAD method instead of GET for such sort of things. If you query some random server on the web, then be prepared that it may be configured to return inconsistent results (this is typical for servers requiring registration). In such cases you may want to use GET request with Range header to download only small number of bytes.', 'Use HEAD method.\nFor example urllib']"
202,How to use Beautiful Soup to extract string in <script> tag?,"In a given .html page, I have a script tag like so:

     <script>jQuery(window).load(function () {
  setTimeout(function(){
    jQuery(""input[name=Email]"").val(""name@email.com"");
  }, 1000);
});...",https://stackoverflow.com/questions/38547569/how-to-use-beautiful-soup-to-extract-string-in-script-tag,"['In a given .html page, I have a script tag like so:', ""To add a bit more to the @Bob's answer and assuming you need to also locate the script tag in the HTML which may have other script tags."", 'not possible using only BeautifulSoup, but you can do it for example with BS + regular expressions', 'I ran into a similar problem and the issue seems to be that calling script_tag.text returns an empty string. Instead, you have to call script_tag.string. Maybe this changed in some version of BeautifulSoup?', 'You could solve this with just a couple of lines of gazpacho and .split, no regex required!']"
203,How to scrape tables in thousands of PDF files?,"I have about 1'500 PDFs consisting of only 1 page each, and exhibiting the same structure (see http://files.newsnetz.ch/extern/interactive/downloads/BAG_15m_kzh_2012_de.pdf for an example). 

What I ...",https://stackoverflow.com/questions/25125178/how-to-scrape-tables-in-thousands-of-pdf-files,"[""I have about 1'500 PDFs consisting of only 1 page each, and exhibiting the same structure (see http://files.newsnetz.ch/extern/interactive/downloads/BAG_15m_kzh_2012_de.pdf for an example)."", ""I didn't know this before, but less has this magical ability to read pdf files. I was able to extract the table data from your example pdf with this script:""]"
204,Use getElementById on HTMLElement instead of HTMLDocument,"I've been playing around with scraping data from web pages using VBS/VBA.

If it were Javascript I'd be away as its easy, but it doesn't seem to be quite as straight forward in VBS/VBA.

This is an ...",https://stackoverflow.com/questions/15191847/use-getelementbyid-on-htmlelement-instead-of-htmldocument,"[""I've been playing around with scraping data from web pages using VBS/VBA."", ""I don't like it either."", 'Thanks to dee for the answer above with the Scrape() subroutine. The code worked perfectly as written, and I was able to then convert the code to work with the specific website I am trying to scrape.', 'I would use XMLHTTP request to retrieve page content as much faster. Then it is easy enough to use querySelectorAll to apply a CSS class selector to grab by class name. Then you access the child elements by tag name and index.']"
205,How to work with the scrapy contracts?,"Scrapy Contracts Problem

I started working on the scrapy framework. Implemented some spiders too for
 extraction, but I am not able to write a unit test case for the spider because the contracts
 ...",https://stackoverflow.com/questions/25764201/how-to-work-with-the-scrapy-contracts,"['Scrapy Contracts Problem', 'Yes, Spiders Contracts is far from being clear and detailed.', 'The two most basic questions in testing the spider might be:']"
206,How to get text from span tag in BeautifulSoup,"I have links looks like this

<div class=""systemRequirementsMainBox"">
<div class=""systemRequirementsRamContent"">
<span title=""000 Plus Minimum RAM Requirement"">1 GB</span> </...",https://stackoverflow.com/questions/38133759/how-to-get-text-from-span-tag-in-beautifulsoup,"['I have links looks like this', 'You can use a css selector, pulling the span you want using the title text :', 'You can simply use span tag in BeautifulSoup or you can include other attributes like class, title along with the span tag.', ""contents[0]' after iterating over all the tags in the folder."", 'You could solve this with just a couple lines of gazpacho:']"
207,Unable to click on signs on a map,"I've written a script in Python in association with selenium to click on each of the signs available in a map. However, when I execute my script, it throws timeout exception error upon reaching this ...",https://stackoverflow.com/questions/52134302/unable-to-click-on-signs-on-a-map,"[""I've written a script in Python in association with selenium to click on each of the signs available in a map. However, when I execute my script, it throws timeout exception error upon reaching this line wait.until(EC.staleness_of(item))."", 'You can click one by one using Selenium if, for some reasons, you cannot use API. Also it is possible to extract information for each sign without clicking on them with Selenium.', ""I know you wrote you don't want to use the API but using Selenium to get the locations from the map markers seems a bit overkill for this, instead, why not making a call to their Web service using requests and parse the returned json?""]"
208,Using R to add field to online form and scrape resulting javascript created table,"I am trying to get R to complete the 'Search by postcode' field on this webpage http://cti.voa.gov.uk/cti/ with predefined text (e.g. BN1 1NA), advance to the next page and scrape the resulting 4 ...",https://stackoverflow.com/questions/31296376/using-r-to-add-field-to-online-form-and-scrape-resulting-javascript-created-tabl,"[""I am trying to get R to complete the 'Search by postcode' field on this webpage http://cti.voa.gov.uk/cti/ with predefined text (e.g. BN1 1NA), advance to the next page and scrape the resulting 4 column table, which, depending on the postcode, can be over multiple pages. To make it more complex the 'Improvement indicator' is not a text field, rather an image file (as seen if you search with postcode BN1 3HP). I would prefer this column to either contain a 0 or 1 depending on if the image is present."", ""I'm not sure what the T&C of the VOA website have to say about scraping, but this code will do the job:"", 'I use RSelenium to scrap a council tax list of an Exeter postcode:']"
209,Why 'Error: length(url) == 1 is not TRUE' with rvest web scraping,"I'm trying to scrap web data but first step requires a login. I've successfully been able to log into other websites but I a weird error with this website.

library(""rvest"")
library(""magrittr"")    

...",https://stackoverflow.com/questions/29090676/why-error-lengthurl-1-is-not-true-with-rvest-web-scraping,"[""I'm trying to scrap web data but first step requires a login. I've successfully been able to log into other websites but I a weird error with this website."", ""I was having the same issue.  I jumped through a few hoops to get the dev version of rvest running, and it's working smoothly now.  Here's how I went about it:""]"
210,Easiest way to “browse” to a page and submit form in Java,"What I need to do is browse to a webpage, login, then browse to another webpage on that site that requires you to be logged in, so it needs to save cookies. After that, I need to click an element on ...",https://stackoverflow.com/questions/14740801/easiest-way-to-browse-to-a-page-and-submit-form-in-java,"['What I need to do is browse to a webpage, login, then browse to another webpage on that site that requires you to be logged in, so it needs to save cookies. After that, I need to click an element on that page, in which I would fill out the form and get the message that the webpage returns to me. The reason I need to actually go to the page and click the button as suppose to just navigating directly to the link is because the you are assigned a session ID every time you log in and click the link, and its always different. The button looks like this, its not a normal href link:', 'This should be possible in Selenium as others have noted.', 'Have a look at the 5 Minute Getting Started Guide for Selenium: http://code.google.com/p/selenium/wiki/GettingStarted', ""On the login page, look at the form's HTML to see the url it posts to and the url parameters. Then request that url with the same parameters filled in with correct info, and make sure to save all the cookie headers to send to the second page. Then use an html parser to find your link. There are several html parsers available on sourceforge, and you could even try java's built in xml parsers, though if the site has even a tiny html mistake they will glitch."", 'Instead of trying to browse around programmatically, try executing the login request and save the cookies then set those in the next request to the form post.', 'HTMLUnit is pretty bad at processing JavaScript, the Rhino JS library produces often errors (actually no errors is much the exception). I would advise to use Selenium, which is basically a framework to control headless browsers (chrome, firefox based).', ""I would recommend htmlunit any day. It's a great library.""]"
211,Block Website Scraping by Google Docs,I run a website that provides various pieces of data in chart/tabular format for people to read. Recently I've noticed an increase in the requests to the website that originate from Google Docs. ...,https://stackoverflow.com/questions/41830988/block-website-scraping-by-google-docs,"[""I run a website that provides various pieces of data in chart/tabular format for people to read. Recently I've noticed an increase in the requests to the website that originate from Google Docs. Looking at the IPs and User Agent, it does appear to be originating from Google servers - example IP lookup here."", ""Blocking on User-Agent is great solution because there doesn't appear to be a way to set a different User-Agent and still use INPUTHTML function -- and since you're happy to ban 'all' usage from doc-sheets, that's perfect."", 'You can force the issue by setting a cookie on the first attempt and serve a response only if the cookie is present. This way any ""simple"" imports will not work as in the first request the cookie is not there so it will be nothing to be read by a third party.']"
212,Scraping Youtube comments in R,"I'm extracting user comments from a range of websites (like reddit.com) and Youtube is also another juicy source of information for me. My existing scraper is written in R:

# x is the url
html = ...",https://stackoverflow.com/questions/25224613/scraping-youtube-comments-in-r,"[""I'm extracting user comments from a range of websites (like reddit.com) and Youtube is also another juicy source of information for me. My existing scraper is written in R:"", 'Following this Answer: R: rvest: scraping a dynamic ecommerce page', 'you mentioned that the youtube comments do not appear in the html source code of a youtube page. However, I used to the developer tools build into Chrome, and I was able to see the html markup of that makes up the comments. I also tried loading the page with scripting blocked and the comments were still there.']"
213,Webscraping with Julia? [closed],"We are having a difficult time to find a good (actually, any) web scraping library or modules  for the Julia language. 

What we need is to have some kind of facility to make it easier to parse or ...",https://stackoverflow.com/questions/21728907/webscraping-with-julia,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", ""Something like that probably doesn't exist yet.  Julia is young that way.  Either write you own, based on other packages like Requests.jl, or bind to the Pthyon stuff you mention with PyCall.jl""]"
214,casperjs doesn't work as expected on windows machine,"I have a casperjs script which gives the desired result when I run on a linux server, but when I run the same from my laptop, it doesn't work.

How should I debug? Logs of the working one:

[info] [...",https://stackoverflow.com/questions/41413716/casperjs-doesnt-work-as-expected-on-windows-machine,"[""I have a casperjs script which gives the desired result when I run on a linux server, but when I run the same from my laptop, it doesn't work."", 'Add a resource.error event handler:']"
215,Selendroid as a web scraper,"I intend to create an Android application that performs a headless login to a website and then scrape some content from the subsequent page while maintaining the logged-in session.

I first used ...",https://stackoverflow.com/questions/30058692/selendroid-as-a-web-scraper,"['I intend to create an Android application that performs a headless login to a website and then scrape some content from the subsequent page while maintaining the logged-in session.', ""Unfortunately I didn't get Selendroid to work. But I find a workaround to scrape dynamic content by using just Android's built in WebView with JavaScript enabled."", ""I never had used Selendroid so I'm not really sure about that but searching by the net I found this example and, according to it, I suppose that your code translation from Selenium to Selendroid would be:"", 'I would suggest you use WebdriverIO since you want to use Javascript.\nIt uses NodeJs so it will be easy to require other plugins to scrape the HTML.']"
216,regex pattern in python for parsing HTML title tags,"I am learning to use both the re module and the urllib module in python and attempting to write a simple web scraper. Here's the code I've written to scrape just the title of websites:

#!/usr/bin/...",https://stackoverflow.com/questions/20045955/regex-pattern-in-python-for-parsing-html-title-tags,"[""I am learning to use both the re module and the urllib module in python and attempting to write a simple web scraper. Here's the code I've written to scrape just the title of websites:"", 'You are using a regular expression, and matching HTML with such expressions get too complicated, too fast.', 'It is recommended that you use Beautiful Soup or any other parser to parse HTML, but if you badly want regex the following piece of code would do the job.', 'If you wish to identify all the htlm tags, you can use this', 'You could scrape a bunch of titles with a couple lines of gazpacho:']"
217,Whats the best screen scraping language? [closed],"Hi I want to create a desktop app (c# prob) that scrapes or manipulates a form on a 3rd party web page. Basically I enter my data in the form in the desktop app, it goes away to the 3rd party website ...",https://stackoverflow.com/questions/760142/whats-the-best-screen-scraping-language,"['Hi I want to create a desktop app (c# prob) that scrapes or manipulates a form on a 3rd party web page. Basically I enter my data in the form in the desktop app, it goes away to the 3rd party website and, using the script or whatever in the background, enters my data there (incl my login) and clicks the submit button for me.I just want to avoid loading up the browser!', 'Do not forget to look at BeautifulSoup, comes highly recommended.', 'I use C# for scraping. See the helpful HtmlAgilityPack package. \nFor parsing pages, I either use XPATH or regular expressions. .NET can also easily handle cookies if you need that.', 'You could try using the .NET HTML Agility Pack:', ""C# is more than suitable for your screen scraping needs. .NET's Regex functionality is really nice. However, with such a simple task, you'll be hard to find a language that doesn't do what you want relatively easily. Considering you're already programming in C#, I'd say stick with that."", 'We use Groovy with NekoHTML. (Also note that you can now run Groovy on Google App Engine.)', ""IMO Perl's built in regular expression functionality and ability to manipulate text would make it a pretty good contender for screen scraping."", 'Ruby is pretty great !...\ntry its hpricot/mechanize', 'Groovy is very good.', 'PHP is a good contender due to its good Perl-Compatible Regex support and cURL library.', 'HTML Agility Pack (c#)', ""Take a look at HP's Web Language (formerly WEBL)."", 'Or stick with WebClient in C# and some string manipulations.', ""I second the recommendation for python (or Beautiful Soup). I'm currently in the middle of a small screen-scraping project using python, and python 3's automatic handling of things like cookie authentication (through CookieJar and urllib) are greatly simplifying things. Python supports all of the more advanced features you might need (like regexes), as well as having the benefit of being able to handle projects like this quickly (not too much overhead in dealing with low level stuff). It's also relatively cross-platform.""]"
218,How can I download images on a page using puppeteer?,"I'm new to web scraping and want to download all images on a webpage using puppeteer:

const puppeteer = require('puppeteer');

let scrape = async () => {
  // Actual Scraping goes Here...

  const ...",https://stackoverflow.com/questions/52542149/how-can-i-download-images-on-a-page-using-puppeteer,"[""I'm new to web scraping and want to download all images on a webpage using puppeteer:"", 'Here is another example. It goes to a generic search in google and downloads the google image at the top left.', 'You can use the following to scrape an array of all the src attributes of all images on the page:', 'The logic is simple i think. You just need to make a function which will take url of image and save it to your directory. The puppeteer will just scrape the image url and pass it to downloader function. Here is an example:', 'If you want to skip the manual dom traversal you can write the images to disk directly from the page response.', 'For image download by its selector I did the following:', 'This code saves all images found on the page into images folder', 'It is possible to get all the images without visiting each url independently. You need to listen to all the requests to the server:']"
219,Python + BeautifulSoup: How to get ‘href’ attribute of ‘a’ element?,"I have the following:

  html =
  '''<div class=“file-one”>
    <a href=“/file-one/additional” class=“file-link"">
      <h3 class=“file-name”>File One</h3>
    </a>
    &...",https://stackoverflow.com/questions/43814754/python-beautifulsoup-how-to-get-href-attribute-of-a-element,"['I have the following:', ""The 'a' tag in your html does not have any text directly, but it contains a 'h3' tag that has text. This means that text is None, and .find_all() fails to select the tag. Generally do not use the text parameter if a tag contains any other html elements except text content."", ""First of all, use a different text editor that doesn't use curly quotes."", 'You can also use attrs to get the href tag with regex search', 'You could solve this with just a couple lines of gazpacho:']"
220,How do you Screen Scrape? [closed],"When there is no webservice API available, your only option might be to Screen Scrape, but how do you do it in c#?

how do you think of doing it?",https://stackoverflow.com/questions/2425043/how-do-you-screen-scrape,"['Want to improve this question? Update the question so it focuses on one problem only by editing this post.', 'Matt and Paul\'s answers are correct. ""Screen scraping"" by parsing the HTML from a website is usually a bad idea because:', ""Use Html Agility Pack. It handles poorly and malformed HTML. It lets you query with XPath, making it very easy to find the data you're looking for. DON'T write a parser by hand and DON'T use regular expressions, it's just too clumsy."", ""The term you're looking for is actually called Screen Scraping."", 'Here are sample C# code which will help you', ""Just one thing to note, a few people have mentioned pulling down the website as XML and then using XPath to iterate through the nodes. It's probably important to make sure you are working with a site that has been developed in XHTML to make sure that the HTML represents a well formed XML document."", 'From a practical perspective (I have written dozens of ""web-interactive"" apps over the years), I finally settled on Watin combined with CSQuery.']"
221,How can I catch and process the data from the XHR responses using casperjs?,The data on the webpage is displayed dynamically and it seems that checking for every change in the html and extracting the data is a very daunting task and also needs me to use very unreliable XPaths....,https://stackoverflow.com/questions/24555370/how-can-i-catch-and-process-the-data-from-the-xhr-responses-using-casperjs,"['The data on the webpage is displayed dynamically and it seems that checking for every change in the html and extracting the data is a very daunting task and also needs me to use very unreliable XPaths. So I would want to be able to extract the data from the XHR packets.', 'This is not easily possible, because the resource.received event handler only provides meta data like url, headers or status, but not the actual data. The underlying phantomjs event handler acts the same way.', 'I may be late into the party, but the answer may help someone like me who would fall into this problem later in future.', ""@Artjom's answer's doesn't work for me in the recent Chrome and CasperJS versions."", 'Additionally, you can also directly download the content and manipulate it later. \nHere is the example of the script I am using to retrieve a JSON and save it locally :']"
222,Python BeautifulSoup findAll by “class” attribute,"I want to do the following code, which is what BS documentation says to do, the only problem is that the word ""class"" isn't just a word. It can be found inside HTML, but it's also a python keyword ...",https://stackoverflow.com/questions/19969056/python-beautifulsoup-findall-by-class-attribute,"['I want to do the following code, which is what BS documentation says to do, the only problem is that the word ""class"" isn\'t just a word. It can be found inside HTML, but it\'s also a python keyword which causes this code to throw an error.', 'Your problem seems to be that you expect find_all in the soup to find an exact match for your string. In fact:', 'Here is how to do it:', 'If OP is interested in getting the finalScore by going through ul you could solve this with a couple of lines of gazpacho:']"
223,Getting scrapy project settings when script is outside of root directory,I have made a Scrapy spider that can be successfully run from a script located in the root directory of the project. As I need to run multiple spiders from different projects from the same script (...,https://stackoverflow.com/questions/31662797/getting-scrapy-project-settings-when-script-is-outside-of-root-directory,"[""I have made a Scrapy spider that can be successfully run from a script located in the root directory of the project. As I need to run multiple spiders from different projects from the same script (this will be a django app calling the script upon the user's request), I moved the script from the root of one of the projects to the parent directory. For some reason, the script is no longer able to get the project's custom settings in order to pipeline the scraped results into the database tables. Here is the code from the scrapy docs I'm using to run the spider from a script:"", ""Thanks to some of the answers already provided here, I realised scrapy wasn't actually importing the settings.py file. This is how I fixed it."", 'It should work , can you share your scrapy log file', 'I have used this code to solve the problem:', 'this could happen because you are no longer ""inside"" a scrapy project, so it doesn\'t know how to get the settings with get_project_settings().']"
224,How can I protect my site from HTTrack or other software's ripping?,I recently got a Site Template approved on Themeforest. I am getting just too much traffic on my site and noticed that my demo on Themeforest is getting ripped by some softwares such as HTTrack. If it ...,https://stackoverflow.com/questions/10453306/how-can-i-protect-my-site-from-httrack-or-other-softwares-ripping,"['I recently got a Site Template approved on Themeforest. I am getting just too much traffic on my site and noticed that my demo on Themeforest is getting ripped by some softwares such as HTTrack. If it continues sales on the item may decrease eventually.', 'No, there is nothing you can do about this.  If you have a demo you need to protect, watermark it.  However, watermarks can be removed, and someone may think the watermark is in the actual product.', 'At least, you can try any of these though user agent can be tricked.', ""since everyone is able to read your html source through browser there's nothing you can do."", ""There isn't much you can do sadly. However, you can stop script kitties half way by importing your source code for your style through the style.css using @import url('here'). They can still view your .css styling but you they'll have to type in the full url to get it. Which can deter some, but not all."", 'Put this code into your .htaccess file. it protects your website from all clone software.']"
225,Make Scrapy follow links and collect data,"I am trying to write program in Scrapy to open links and collect data from this tag: <p class=""attrgroup""></p>.

I've managed to make Scrapy collect all the links from given URL but not to ...",https://stackoverflow.com/questions/30152261/make-scrapy-follow-links-and-collect-data,"['I am trying to write program in Scrapy to open links and collect data from this tag: <p class=""attrgroup""></p>.', 'You need to yield Request instances for the links to follow, assign a callback and extract the text of the desired p element in the callback:']"
226,How to read an html table using Rselenium?,"I'm using Rselenium to navigate to a webpage. The following code is doing so. I haven't provided the url because I'm using the url in a company which needs vpn to connect:

RSelenium::startServer()
...",https://stackoverflow.com/questions/29932542/how-to-read-an-html-table-using-rselenium,"[""I'm using Rselenium to navigate to a webpage. The following code is doing so. I haven't provided the url because I'm using the url in a company which needs vpn to connect:"", 'Something like:', 'I prefer using rvest, so what I did was:']"
227,Retrieving JavaScript Rendered HTML with Puppeteer,I am attempting to scrape the html from this NCBI.gov page. I need to include the #see-all URL fragment so that I am guaranteed to get the searchpage instead of retrieving the HTML from an incorrect ...,https://stackoverflow.com/questions/45871187/retrieving-javascript-rendered-html-with-puppeteer,"['I am attempting to scrape the html from this NCBI.gov page. I need to include the #see-all URL fragment so that I am guaranteed to get the searchpage instead of retrieving the HTML from an incorrect gene page https://www.ncbi.nlm.nih.gov/gene/119016.', 'You can try to change this:', 'Maybe try to wait', 'I had success using the following to get html content that was generated after the page has been loaded.', 'If you want to actually await a custom event, you can do it this way.']"
228,How to scrape all contents from infinite scroll website? scrapy,"I'm using scrapy.

The website i'm using has infinite scroll.

the website has loads of posts but i only scraped 13.

How to scrape the rest of the posts?

here's my code:

class exampleSpider(scrapy....",https://stackoverflow.com/questions/37207959/how-to-scrape-all-contents-from-infinite-scroll-website-scrapy,"[""I'm using scrapy."", 'Check the website code.', 'I use Selenium rather than scrapy but you must be able to do the equivalent and what I do is run some JavaScript on loading the file, namely:', 'i think what you are looking for is a pagination logic along side your normal logic', 'I think you are looking for something like DEPTH-LIMIT', 'Obviously, that target site upload its content dynamically. Hence there are two appropriate solutions there:', 'In some cases, you can find in the source code the element called to run the ""next"" pagination, even in infinite scroll. So you just have to click on this element and it will show the rest of the posts. With scrapy/selenium :']"
229,Fetch data of variables inside script tag in Python or Content added from js,"I want to fetch data from another url for which I am using urllib and Beautiful Soup , My data is inside table tag (which I have figure out using Firefox console). But when I tried to fetch table ...",https://stackoverflow.com/questions/24118337/fetch-data-of-variables-inside-script-tag-in-python-or-content-added-from-js,"['I want to fetch data from another url for which I am using urllib and Beautiful Soup , My data is inside table tag (which I have figure out using Firefox console). But when I tried to fetch table using his id the result is None , Then I guess this table must be dynamically added via some js code.', 'EDIT', ""Just to add to @mhawke 's answer, rather than hardcoding the offset of the script tag, you loop through all the script tags and match the one that matches your pattern;""]"
230,How to automate multiple requests to a web search form using R,I'm trying to learn how to use RCurl (or some other suitable R package if I'm wrong about RCurl being the right tool) to automate the process of submitting search terms to a web form and placing the ...,https://stackoverflow.com/questions/5396461/how-to-automate-multiple-requests-to-a-web-search-form-using-r,"[""I'm trying to learn how to use RCurl (or some other suitable R package if I'm wrong about RCurl being the right tool) to automate the process of submitting search terms to a web form and placing the search results in a data file. The specific problem I'm working on is as follows:"", 'Adding to the suggestion by daroczig and Rguy, here is a short piece of code to automate the entire process of extracting the data into a data frame.', 'Just use http instead of https and that should solve your problem. Here is the output you get if you try this', 'The details of my comment above:', 'I meant to post this as a comment after the original post, but do not have enough reputation.']"
231,How to extract meaningful and useful content from web pages? [closed],"I would like to parse a webpage and extract meaningful content from it. By meaningful, I mean the content (text only) that the user wants to see in that particular page (data excluding ads, banners, ...",https://stackoverflow.com/questions/13791316/how-to-extract-meaningful-and-useful-content-from-web-pages,"['I would like to parse a webpage and extract meaningful content from it. By meaningful, I mean the content (text only) that the user wants to see in that particular page (data excluding ads, banners, comments etc.) I want to ensure that when a user saves a page, the data that he wanted to read is saved, and nothing else.', ""did you type 'python readability' into google? there is a pretty popular (200+ followers) library on github."", 'You may use htql.']"
232,How can I parse Javascript variables using python?,"The problem: A website I am trying to gather data from uses Javascript to produce a graph. I'd like to be able to pull the data that is being used in the graph, but I am not sure where to start. For ...",https://stackoverflow.com/questions/18368058/how-can-i-parse-javascript-variables-using-python,"[""The problem: A website I am trying to gather data from uses Javascript to produce a graph. I'd like to be able to pull the data that is being used in the graph, but I am not sure where to start. For example, the data might be as follows:"", 'If your format really is just one or more var foo = [JSON array or object literal];, you can just write a dotall regex to extract them, then parse each one as JSON. For example:', 'Okay, so there are a few ways to do it, but I ended up simply using a regular expression to find everything between line1= and ;', 'The following makes a few assumptions such as knowing how the page is formatted, but a way of getting your example into memory on Python is like this', 'Assuming you have a python variable with a javascript line/block as a string like""var line1 = [[a,b,c], [d,e,f]];"", you could use the following few lines of code.']"
233,Getting Final HTML with Javascript rendered Java as String,I want to fetch data from an HTML page(scrape it). But it contains reviews in javascript. In normal java url fetch I am only getting the HTML(actual one) without Javascript executed. I want the final ...,https://stackoverflow.com/questions/10872382/getting-final-html-with-javascript-rendered-java-as-string,"['I want to fetch data from an HTML page(scrape it). But it contains reviews in javascript. In normal java url fetch I am only getting the HTML(actual one) without Javascript executed. I want the final page with Javascript executed.', 'Use phantomjs: http://phantomjs.org', 'You can use HTML Unit, A java based ""GUI LESS Browser"". You can easily get the final rendered output of any page because this loads the page as a web browser do so and returns the final rendered output. You can disable this behaviour though.', 'The simple way to solve that problem.\nHello, you can use HtmlUnit is java API, i think it can help you to access the executed js content, as a simple html.']"
234,How to scrape HTTPS javascript web pages,"I am trying to monitor day-to-day prices from an online catalogue.
The site uses HTTPS and generates the catalogue pages with javascript.  How can i interface with the site and make it generate the ...",https://stackoverflow.com/questions/5561950/how-to-scrape-https-javascript-web-pages,"['I am trying to monitor day-to-day prices from an online catalogue.\nThe site uses HTTPS and generates the catalogue pages with javascript.  How can i interface with the site and make it generate the pages I need?', 'Take a look at HTMLUnit - a headless Java browser that can be fully controlled by your code. A simple example can be seen here: http://htmlunit.sourceforge.net/gettingStarted.html', ""If they've created a Web API that their JavaScript interfaces with, you might be able to scrape that directly, rather than trying to go the HTML route."", ""I use webkit through it's python bindings for scraping javascript content. See here for example.""]"
235,Scrapy CrawlSpider for AJAX content,"I am attempting to crawl a site for news articles. My start_url contains:

(1) links to each article: http://example.com/symbol/TSLA

and 

(2) a ""More"" button that makes an AJAX call that dynamically ...",https://stackoverflow.com/questions/23706111/scrapy-crawlspider-for-ajax-content,"['I am attempting to crawl a site for news articles. My start_url contains:', 'Crawl spider may be too limited for your purposes here. If you need a lot of logic you are usually better off inheriting from Spider.']"
236,"Puppeteer Execution context was destroyed, most likely because of a navigation","I am facing this problem in puppeteer in a for loop when i go on another page to get data, then when i go back it comes me this error line:

Error ""We have an error Error: the execution context was ...",https://stackoverflow.com/questions/55877263/puppeteer-execution-context-was-destroyed-most-likely-because-of-a-navigation,"['I am facing this problem in puppeteer in a for loop when i go on another page to get data, then when i go back it comes me this error line:', 'The error means that you are accessing data which has become obsolete/invalid because of navigation. In your script the error references the variable listeCompanies:']"
237,How to submit login form in Rvest package w/o button argument,"I am trying to scrape a web page that requires authentication using html_session() & html_form() from the rvest package.
I found this e.g. provided by Hadley Wickham, but am not able to customize ...",https://stackoverflow.com/questions/34830882/how-to-submit-login-form-in-rvest-package-w-o-button-argument,"['I am trying to scrape a web page that requires authentication using html_session() & html_form() from the rvest package.\nI found this e.g. provided by Hadley Wickham, but am not able to customize it to my case.', 'Currently, this issue is the same as the open issue #159 in the rvest package, which causes issues where not all fields in a form have a type value. This buy may be fixed in a future release.']"
238,How to prevent a twisted.internet.error.ConnectionLost error when using Scrapy?,"I'm scraping some pages with scrapy and get the following error: 

twisted.internet.error.ConnectionLost

My command line output:

2015-05-04 18:40:32+0800 [cnproxy] INFO: Spider opened
2015-05-04 18:...",https://stackoverflow.com/questions/30028585/how-to-prevent-a-twisted-internet-error-connectionlost-error-when-using-scrapy,"[""I'm scraping some pages with scrapy and get the following error:"", ""You need to set a user-agent string. It seems some websites don't like it and block when your user agent is not a browser.\nYou can find examples of user agent strings.""]"
239,Scraping Data from a website which uses Power BI - retrieving data from Power BI on a website,"I want to scrap data from this page (and pages similar to it): https://cereals.ahdb.org.uk/market-data-centre/historical-data/feed-ingredients.aspx

This page uses Power BI. Unfortunately, finding a ...",https://stackoverflow.com/questions/55063265/scraping-data-from-a-website-which-uses-power-bi-retrieving-data-from-power-bi,"['I want to scrap data from this page (and pages similar to it): https://cereals.ahdb.org.uk/market-data-centre/historical-data/feed-ingredients.aspx', 'Putting the scroll part and the JSON aside, I managed to read the data. The key is to read all of the elements inside the parent (which is done in the question):', 'A few more details about exactly which data  you are trying to scrap would have helped to construct a canonical answer. However, to scrape the data within the Commodity and Basis using Selenium, as the the desired element is within an <iframe> so you have to:']"
240,Is it possible to scrape a React website (Instagram) with Cheerio?,"I'm trying to scrape Instagram (built with React) with Node.js / Cheerio. Debugging the document shows an object returned, but it doesn't look like the typical response.

I'm guessing this has to do ...",https://stackoverflow.com/questions/29702615/is-it-possible-to-scrape-a-react-website-instagram-with-cheerio,"[""I'm trying to scrape Instagram (built with React) with Node.js / Cheerio. Debugging the document shows an object returned, but it doesn't look like the typical response."", 'In the general case -- if the website is SEO friendly, you can do it by spoofing the user agent string of a web crawler. This returns a rendered DOM that can be parsed by Cheerio.']"
241,Scraping NBA data in R with rjson,"I have been spending a long time using R to try to scrape NBA data, so far I was doing it a little by trial and error, but finally I found this documentation. Some time ago I had some problems ...",https://stackoverflow.com/questions/47745164/scraping-nba-data-in-r-with-rjson,"['I have been spending a long time using R to try to scrape NBA data, so far I was doing it a little by trial and error, but finally I found this documentation. Some time ago I had some problems scraping the shotchartdetail, and I figured out the problem when I found this', 'So I believe the issue here is that you need to pass a PlayerID and TeamID to the API. Using PlayerID = 2544 and TeamID = 1610612739 below as an example seems to work:']"
242,Javascript: REGEX to change all relative Urls to Absolute,"I'm currently creating a Node.js webscraper/proxy, but I'm having trouble parsing relative Urls found in the scripting part of the source, I figured REGEX would do the trick. 
Although it is unknown ...",https://stackoverflow.com/questions/7544550/javascript-regex-to-change-all-relative-urls-to-absolute,"[""I'm currently creating a Node.js webscraper/proxy, but I'm having trouble parsing relative Urls found in the scripting part of the source, I figured REGEX would do the trick. \nAlthough it is unknown how I would achieve this."", ""Note for OP, because he requested such a function: Change base_url to your proxy's basE URL in order to achieve the desired results."", 'A reliable way to convert urls from relative to absolute is to use the built-in url module.', 'This is Rob W answer ""Advanced HTML string replacement functions"" in current thread plus some code re-factoring from me to make JSLint happy.', 'From a comment by Rob W above about the base tag I wrote an injection function:', 'If you use a regex to find all non-absolute URLs, you can then just prefix them with the current URL and that should be it.']"
243,Beautiful Soup and Table Scraping - lxml vs html parser,"I'm trying to extract the HTML code of a table from a webpage using BeautifulSoup.

<table class=""facts_label"" id=""facts_table"">...</table>
I would like to know why the code bellow works ...",https://stackoverflow.com/questions/25714417/beautiful-soup-and-table-scraping-lxml-vs-html-parser,"[""I'm trying to extract the HTML code of a table from a webpage using BeautifulSoup."", 'There is a special paragraph in BeautifulSoup documentation called Differences between parsers, it states that:', 'Short answer.']"
244,Getting all Links from a page Beautiful Soup,"I am using beautifulsoup to get all the links from a page. My code is:

import requests
from bs4 import BeautifulSoup
url = 'http://www.acontecaeventos.com.br/marketing-promocional-sao-paulo'
r = ...",https://stackoverflow.com/questions/46490626/getting-all-links-from-a-page-beautiful-soup,"['I am using beautifulsoup to get all the links from a page. My code is:', 'You are telling the find_all method to find href tags, not attributes.', 'Replace your last line:', 'To get a list of everyhref regardless of tag use:']"
245,How to get user agent information in Selenium WebDriver with Python,"I am trying to get the actual user agent that I am using in Selenium, at the moment with the chromedriver.

I found a Java version of this problem:
How to get userAgent information in Selenium Web ...",https://stackoverflow.com/questions/51482697/how-to-get-user-agent-information-in-selenium-webdriver-with-python,"['I am trying to get the actual user agent that I am using in Selenium, at the moment with the chromedriver.', 'The same manner as inside your link:']"
246,R: Using rvest package instead of XML package to get links from URL,"I use XML package to get the links from this url.

# Parse HTML URL
v1WebParse <- htmlParse(v1URL)
# Read links and and get the quotes of the companies from the href
t1Links <- data.frame(...",https://stackoverflow.com/questions/27297484/r-using-rvest-package-instead-of-xml-package-to-get-links-from-url,"['I use XML package to get the links from this url.', ""Despite my comment, here's how you can do it with rvest. Note that we need to read in the page with htmlParse first since the site has the content-type set to text/plain for that file and that tosses rvest into a tizzy."", ""I know you're looking for an rvest answer, but here's another way using the  XML package that might be more efficient than what you're doing."", ""Richard's answer works for HTTP pages but not the HTTPS page I needed (Wikipedia). I substituted RCurl's getURL function as below:""]"
247,Speeding up beautifulsoup,"I'm running a scraper of this course website and I'm wondering whether there's a faster way to scrape the page once I have it put into beautifulsoup.  It takes way longer than I would have expected.

...",https://stackoverflow.com/questions/25539330/speeding-up-beautifulsoup,"[""I'm running a scraper of this course website and I'm wondering whether there's a faster way to scrape the page once I have it put into beautifulsoup.  It takes way longer than I would have expected."", 'Okay, you can really speed this up by:', ""I'm gonna post this hidden gem in hopes that it might help someone as it helped me a lot:"", 'According to beautifulsoup docs:']"
248,How to scrape all the content of each link with scrapy?,"I am new with scrapy I would like to extract all the content of each advertise from this website. So I tried the following:

from scrapy.spiders import Spider
from craigslist_sample.items import ...",https://stackoverflow.com/questions/40479789/how-to-scrape-all-the-content-of-each-link-with-scrapy,"['I am new with scrapy I would like to extract all the content of each advertise from this website. So I tried the following:', 'To scaffold a basic scrapy project you can use the command:', 'I am trying to answer your question.']"
249,BeautifulSoup extract top-level tags only [duplicate],"I'm doing some web-scraping with BeautifulSoup in Python 3.4.

Now I have encountered a problem during my learning: 
I'm trying to get a table rows from a webpage and I'm using find_all() to get them, ...",https://stackoverflow.com/questions/37911009/beautifulsoup-extract-top-level-tags-only,"[""I'm doing some web-scraping with BeautifulSoup in Python 3.4."", 'Apparently there is an argument called recursive in the method find_all() and it is set by default to True.']"
250,graph.facebook.com/username does not work,"I tried to get user detail using https://graph.facebook.com/username . But it throws an error 

    ""error"": {
      ""message"": ""(#803) Cannot query users by their username (ramesh.randika.56)"",
      ...",https://stackoverflow.com/questions/30868201/graph-facebook-com-username-does-not-work,"['I tried to get user detail using https://graph.facebook.com/username . But it throws an error', 'graph.facebook.com/{Username} is not avaiable now but lookup-id.com still can find your facebook ID, facebook group ID and facebook page ID', '""View page source"" Option in every web browser will help you.\nFollow this-', 'The username field was removed with the introduction of the Graph API v2.0.']"
251,Scrapy grab div with multiple classes?,"I am trying to grab div's with the class: 'product'. The problem is, some of the div's with class 'product' also have the class 'product-small'. So when I use xpath('//div[@class='product']'), it only ...",https://stackoverflow.com/questions/28140421/scrapy-grab-div-with-multiple-classes,"[""I am trying to grab div's with the class: 'product'. The problem is, some of the div's with class 'product' also have the class 'product-small'. So when I use xpath('//div[@class='product']'), it only captures the divs with one class and not multiple. How can I do this with scrapy?"", 'You should consider using a CSS selector for this part of your query.', 'This could be also solved with xpath. You just needed to use contains():']"
252,Puppeteer: get localStorage from a website,I need to take with Puppeteer all the data that a website saves: cookies and localStorage (for example after Login). I have read all Puppeteer documentation but I can not find anything about ...,https://stackoverflow.com/questions/55725295/puppeteer-get-localstorage-from-a-website,"['I need to take with Puppeteer all the data that a website saves: cookies and localStorage (for example after Login). I have read all Puppeteer documentation but I can not find anything about localStorage.', 'I found the way:', 'easier way that worked for me']"
253,Html Agility Pack: Find Comment Node,"I am scraping a website that uses Javascript to dynamically populate the content of a website with the Html Agility pack.

Basically, I was searching for the XPATH ""\\div[@class='PricingInfo']"", but ...",https://stackoverflow.com/questions/3844208/html-agility-pack-find-comment-node,['I am scraping a website that uses Javascript to dynamically populate the content of a website with the Html Agility pack.']
254,How to isolate a single element from a scraped web page in R,"I want to use R to scrape this page: (http://www.fifa.com/worldcup/archive/germany2006/results/matches/match=97410001/report.html ) and others, to get the goal scorers and times.

So far, this is what ...",https://stackoverflow.com/questions/2998655/how-to-isolate-a-single-element-from-a-scraped-web-page-in-r,"['I want to use R to scrape this page: (http://www.fifa.com/worldcup/archive/germany2006/results/matches/match=97410001/report.html ) and others, to get the goal scorers and times.', 'These questions are very helpful when dealing with web scraping and XML in R:']"
255,Exclude unwanted tag on Beautifulsoup Python,"<span>
  I Like
  <span class='unwanted'> to punch </span>
   your face
 </span>
How to print ""I Like your face"" instead of ""I Like to punch your face""

I tried this

lala = ...",https://stackoverflow.com/questions/40760441/exclude-unwanted-tag-on-beautifulsoup-python,"['How to print ""I Like your face"" instead of ""I Like to punch your face""', 'You can use extract() to remove unwanted tag before you get text.', 'You can easily find the (un)desired text like this:']"
256,What is the best practice for writing maintainable web scrappers?,"I need to implement a few scrapers to crawl some web pages (because the site doesn't have open API), extracting information and save to database. I am currently using beautiful soup to write code like ...",https://stackoverflow.com/questions/21252847/what-is-the-best-practice-for-writing-maintainable-web-scrappers,"[""I need to implement a few scrapers to crawl some web pages (because the site doesn't have open API), extracting information and save to database. I am currently using beautiful soup to write code like this:"", 'Pages have the potential to change so drastically that building a very ""smart"" scraper might be pretty difficult; and if possible, the scraper would be somewhat unpredictable, even with fancy techniques like machine-learning etcetera. It\'s hard to make a scraper that has both trustworthiness and automated flexibility.', 'Completely unrelated to Python and not auto-flexible, but I think the templates of my Xidel scraper have the best maintability.', ""EDIT: Oops, I now see you're already using CSS selectors. I think they provide the best answer to your question. So no, I don't think there is a better way.""]"
257,Starting Scrapy from a Django view,"My experience with Scrapy is limited, and each time I use it, it's always through the terminal's commands. How can I get my form data (a url to be scraped) from my django template to communicate with ...",https://stackoverflow.com/questions/26921879/starting-scrapy-from-a-django-view,"['My experience with Scrapy is limited, and each time I use it, it\'s always through the terminal\'s commands. How can I get my form data (a url to be scraped) from my django template to communicate with scrapy to start doing scraping? So far, I\'ve only thought of is to get the form\'s returned data from django\'s views and then try to reach into the spider.py in scrapy\'s directory to add the form data\'s url to the spider\'s start_urls. From there, I don\'t really know how to trigger the actual crawling since I\'m used to doing it strictly through my terminal with commands like ""scrapy crawl dmoz"". Thanks.', ""You've actually answered it with an edit. The best option would be to setup scrapyd service and make an API call to schedule.json to trigger a scraping job to run.""]"
258,How to scrape Instagram with BeautifulSoup,"I want to scrape pictures from a public Instagram account. I'm pretty familiar with bs4 so I started with that. Using the element inspector on Chrome, I noted the pictures are in an unordered list and ...",https://stackoverflow.com/questions/18130499/how-to-scrape-instagram-with-beautifulsoup,"[""I want to scrape pictures from a public Instagram account. I'm pretty familiar with bs4 so I started with that. Using the element inspector on Chrome, I noted the pictures are in an unordered list and li has class 'photo', so I figure, what the hell -- can't be that hard to scrape with findAll, right?"", ""If you look at the source code for the page, you'll see that some javascript generates the webpage.  What you see in the element browser is the webpage after the script has been run, and beautifulsoup just gets the html file.  In order to parse the rendered webpage you'll need to use something like Selenium to render the webpage for you.""]"
259,How can I do web scraping in Julia?,"I want to extract the names of universities and their websites from this site into lists.

In Python I did it with BeautifulSoup v4:

import requests
from bs4 import BeautifulSoup
import pandas as pd

...",https://stackoverflow.com/questions/59825336/how-can-i-do-web-scraping-in-julia,"['I want to extract the names of universities and their websites from this site into lists.', ""Your python code doesn't quite work.\nI guess the website has been updated recently.\nSince they have removed the links as far as i can tell,.\nHere is a similar example using Gumbo.jl and Cascadia.jl."", 'Yes.']"
260,How to scrape a table with rvest and xpath?,"using the following documentation i have been trying to scrape a series of tables from marketwatch.com 

here is the one represented by the code bellow: 
The link and xpath are already included in ...",https://stackoverflow.com/questions/35707534/how-to-scrape-a-table-with-rvest-and-xpath,"['using the following documentation i have been trying to scrape a series of tables from marketwatch.com', ""That website doesn't use an html table, so html_table() can't find anything. It actaully uses div classes column and data lastcolumn.""]"
261,Trouble parsing product names out of some links with different depth,I've written a script in python to reach the target page where each category has their avaiable item names in a website. My below script can get the product names from most of the links (generated ...,https://stackoverflow.com/questions/52065009/trouble-parsing-product-names-out-of-some-links-with-different-depth,"[""I've written a script in python to reach the target page where each category has their avaiable item names in a website. My below script can get the product names from most of the links (generated through roving category links and then subcategory links)."", ""The site has six main product categories. Products that belong to a subcategory can also be found in a main category (for example the products in /furniture/furniture/tables can also be found in /furniture), so you only have to collect products from the main categories. You could get the categories links from the main page, but it'd be easier to use the sitemap."", 'I would recommend starting your scrape from the pages sitemap', 'Since your main issue is finding the links, here is a generator that will find all of the category and sub-category links using the sitemap krflol pointed out in his solution:', 'I saw the website for parsing and found that all the products are available at the bottom left side of the main page https://www.courts.com.sg/ .After clicking one of these we goes to advertisement front page of a particular category. Where we have to go in click All Products for getting it.']"
262,How does one scrape all the products from a random website?,I tried to get all the products from this website but somehow I don't think I chose the best method because some of them are missing and I can't figure out why. It's not the first time when I get ...,https://stackoverflow.com/questions/48015149/how-does-one-scrape-all-the-products-from-a-random-website,"[""I tried to get all the products from this website but somehow I don't think I chose the best method because some of them are missing and I can't figure out why. It's not the first time when I get stuck when it comes to this."", 'If your ultimate goal is to scrape the entire product listing for each category, it may make sense to target the full product listings for each category on the index page. This program uses BeautifulSoup to find each category on the index page and then iterates over each product page under each category. The final output is a list of namedtuples stories each category name with the current page link and the full product titles for each link:', 'First of all, there is no definite answer to your generic question of how would one know if the data one has already scraped is all the available data. This is at least web-site specific and is rarely actually revealed. Plus, the data itself might be highly dynamic. On this web-site though you may more or less use the product counters to verify the amount of results found:', 'As pointed out by @mzjn and @alecxe, some websites employ anti-scraping measures. To hide their intentions, scrapers should try to mimic a human visitor.']"
263,Python - Manipulate and read browser from current browser,"I am struggling to find a method in python which allows you to read data in a currently used web browser. Effectively, I am trying to download a massive dataframe of data on a locally controlled ...",https://stackoverflow.com/questions/46673644/python-manipulate-and-read-browser-from-current-browser,"['I am struggling to find a method in python which allows you to read data in a currently used web browser. Effectively, I am trying to download a massive dataframe of data on a locally controlled company webpage and implement it into a dataframe. The issue is that the website has a fairly complex authentication token process which I have not been able to bypass using Selenium using a slew of webdrivers, Requests, urllib, and cookielib using a variety of user parameters. I have given up on this front entirely as I am almost positive that there is more to the authentication process than can be achieved easily with these libraries.', '1) Start browser with Selenium.']"
264,Excessive depth in document: XML_PARSE_HUGE option for xml2::read_html() in R,First I would like to apologize for a new question as my profile does not yet allow me to comment on other people's comments especially on two SO posts I've seen. So please bear with this older guy :-)...,https://stackoverflow.com/questions/39673775/excessive-depth-in-document-xml-parse-huge-option-for-xml2read-html-in-r,"[""First I would like to apologize for a new question as my profile does not yet allow me to comment on other people's comments especially on two SO posts I've seen. So please bear with this older guy :-)"", ""I don't know if this is the right thing to do, but my question was answered by @hrbrmstr in one of his comments. I decided to post an answer so that people stumbling upon this question see that it has at least one answer.""]"
265,Scrapy doesn't seem to be doing DFO,"I have a website for which my crawler needs to follow a sequence. So for example, it needs to go a1, b1, c1 before it starts going a2 etc. each of a, b and c are handled by different parse functions ...",https://stackoverflow.com/questions/9548126/scrapy-doesnt-seem-to-be-doing-dfo,"[""I have a website for which my crawler needs to follow a sequence. So for example, it needs to go a1, b1, c1 before it starts going a2 etc. each of a, b and c are handled by different parse functions and the corresponding urls are created in a Request object and yielded. The following roughly illustrates the code I'm using:"", 'Depth first searching is exactly what you are describing:', 'I believe that you are noticing the difference between depth-first and breadth-first searching algorithms (see Wikipedia for info on both.)', 'Scrapy use DFO by default. The reason of the sequence of crawls is that scrapy crawls pages asynchronously. Even though it use DFO, the sequence seems in unreasonable order because of network delay or something else.']"
266,Where is the memory leak? How to timeout threads during multiprocessing in python?,"It is unclear how to properly timeout workers of joblib's Parallel in python. Others have had similar questions here, here, here and here. 

In my example I am utilizing a pool of 50 joblib workers ...",https://stackoverflow.com/questions/48540668/where-is-the-memory-leak-how-to-timeout-threads-during-multiprocessing-in-pytho,"[""It is unclear how to properly timeout workers of joblib's Parallel in python. Others have had similar questions here, here, here and here."", 'It is not possible to kill a Thread in Python without a hack.']"
267,How to solve a reCaptcha in advance using a web scraper?,"I'm currently in the process of trying to solve a reCaptcha. One of the suggestions received was a method called token farming.

For example, it's possible to farm for reCaptcha tokens from another ...",https://stackoverflow.com/questions/43557112/how-to-solve-a-recaptcha-in-advance-using-a-web-scraper,"[""I'm currently in the process of trying to solve a reCaptcha. One of the suggestions received was a method called token farming."", 'Token farming / token harvesting has been described here in detail: https://www.blackhat.com/docs/asia-16/materials/asia-16-Sivakorn-Im-Not-a-Human-Breaking-the-Google-reCAPTCHA-wp.pdf']"
268,Scrapy or Selenium or Mechanize to scrape web data?,"I want to scrape some data from a website.

Basically, the website has some tabular display and shows around 50 records. For more records, the user has to click some button which makes an ajax call ...",https://stackoverflow.com/questions/20939401/scrapy-or-selenium-or-mechanize-to-scrape-web-data,"['I want to scrape some data from a website.', 'I would recommend you to go with a combination of Mechanize and ExecJS (https://github.com/sstephenson/execjs) to execute any javascript requests you might come across. I have used those two gems in combination for quite some time now and they do a great job.', ""Definitely I'd choose Scrapy. If you can't handle javascript you can try with Scrapy + splash.\nScrapy is by far the fastest tool for web scraping that I'm aware of.\nGood luck!""]"
269,How to crawl an entire website with Scrapy?,"I'm unable to crawl a whole website, Scrapy just crawls at the surface, I want to crawl deeper. Been googling for the last 5-6 hours and no help. My code below:

from scrapy.contrib.spiders import ...",https://stackoverflow.com/questions/15500281/how-to-crawl-an-entire-website-with-scrapy,"[""I'm unable to crawl a whole website, Scrapy just crawls at the surface, I want to crawl deeper. Been googling for the last 5-6 hours and no help. My code below:"", 'Rules short-circuit, meaning that the first rule a link satisfies will be the rule that gets applied, your second Rule (with callback) will not be called.', 'When parsing the start_urls, deeper urls can be parsed by the tag href. Then, deeper request can be yielded in the function parse(). Here is a simple example. The most important source code is shown below:']"
270,Scraper throws errors instead of quitting the browser when everything is done,"I've written a scraper to parse movie information from a torrent site. I used IE and queryselector.

My code does parse everything. It throws errors instead of quitting the browser when everything is ...",https://stackoverflow.com/questions/47993064/scraper-throws-errors-instead-of-quitting-the-browser-when-everything-is-done,"[""I've written a scraper to parse movie information from a torrent site. I used IE and queryselector."", 'Ok, so there is something seriously unfriendly about that webpage.  It kept crashing for me.   So I have resorted to running a javascript program within scripting engine/scripting control and it works.', 'The website has an API. Check e. g. result from the URL https://yts.am/api/v2/list_movies.json?page=1&limit=50, which actually represents 50 movies from first page of latest movies category, in JSON format.']"
271,html scraping and css queries,"what are the advantages and disadvantages of the following libraries?
PHP Simple HTML DOM Parser
QP
phpQuery
From the above i've used QP and it failed to parse invalid HTML, and simpleDomParser, ...",https://stackoverflow.com/questions/3603511/html-scraping-and-css-queries,"['what are the advantages and disadvantages of the following libraries?', ""I used to use simple html dom exclusively until some bright SO'ers showed me the light hallelujah.""]"
272,scraping asp javascript paginated tables behind search with R,i'm trying to pull the content on https://www.askebsa.dol.gov/epds/default.asp with either rvest or RSelenium but not finding guidance when the javascript page begins with a search box?  it'd be great ...,https://stackoverflow.com/questions/51794400/scraping-asp-javascript-paginated-tables-behind-search-with-r,"[""i'm trying to pull the content on https://www.askebsa.dol.gov/epds/default.asp with either rvest or RSelenium but not finding guidance when the javascript page begins with a search box?  it'd be great to just get all of this content into a simple CSV file."", ""In order to get the results you'll have to fill in the form and submit it. You can find the url and field names by inspecting the html."", 'For the first part of the problem, this approach using rvest should work. I am receiving an error in the last step where it cannot find the required name-tag.', 'Here is an example use of RSelenium to get links to individual filings. The rest should be straightforward once you retrieve links. You can navigate to these URLs using rvest (as you already did before) and parse the content with the help of string manipulation tools such as stringr. For the second part, it would be optimistic to expect a systematic structure across all forms. Please try spend some time to construct specific regular expressions to pull what you need from the text retrieved.']"
273,Unable to exhaust the content of all the identical urls used within my scraper,I've written a scraper in python using BeautifulSoup library to parse all the names traversing different pages of a website. I could manage it if it were not for more than one urls with different ...,https://stackoverflow.com/questions/50611496/unable-to-exhaust-the-content-of-all-the-identical-urls-used-within-my-scraper,"[""I've written a scraper in python using BeautifulSoup library to parse all the names traversing different pages of a website. I could manage it if it were not for more than one urls with different pagination, meaning some urls have pagination some does not as the content are few."", 'This solution attempts to find pagination a tags. If any pagination is found, all the pages are scraped when the user iterates over the instance of the class PageScraper. If not, only the first result (the single page) will be crawled:', ""It seems I've found out a very robust solutioon to this problem. The approach is iterative. It will first check if there is any next page url available in that page. If it finds one then it will track that url and repeat the process. However, if any link doesn't have any pagination, the scraper will break and try for another link.""]"
274,"x-ray-phantom authentication, unable to effectively login","I'm really can't find any example of using x-ray and .driver(phantom()) for authentication.. 
I've trawled through the documentation for x-ray and x-ray-phantom yet can't find any help.",https://stackoverflow.com/questions/33289143/x-ray-phantom-authentication-unable-to-effectively-login,"[""I'm really can't find any example of using x-ray and .driver(phantom()) for authentication.. \nI've trawled through the documentation for x-ray and x-ray-phantom yet can't find any help."", 'Refer below github links if it helps.']"
275,requests response.iter_content() gets incomplete file ( 1024MB instead of 1.5GB )?,"hi i have been using this code snippet to download files from a website, so far files smaller than 1GB are all good. but i noticed a 1.5GB file is incomplete

# s is requests session object
r = s.get(...",https://stackoverflow.com/questions/23645212/requests-response-iter-content-gets-incomplete-file-1024mb-instead-of-1-5gb,"['hi i have been using this code snippet to download files from a website, so far files smaller than 1GB are all good. but i noticed a 1.5GB file is incomplete', 'Please double check that you can download the file via wget and/or any regular browser. It could be restriction on the server. As I see your code can download big files (bigger then 1.5Gb)', 'If you are using Nginx as file system, you may check Nginx config file to see if you have set', 'I think you forgot to close req.']"
276,Extremely strange Web-Scraping issue: Post request not behaving as expected,"I'm attempting to programmatically submit some data to a form on our company's admin page rather than doing it by hand.   

I've written numerous other tools which scrape this website and manipulate ...",https://stackoverflow.com/questions/15733078/extremely-strange-web-scraping-issue-post-request-not-behaving-as-expected,"[""I'm attempting to programmatically submit some data to a form on our company's admin page rather than doing it by hand."", 'Read and re-read your post and the other folks answers a few times. My thoughts:', 'I think that the PHP script is erroring out and not displaying anything because your form data is not exactly identical. Try replicating a post request to be completely identical including all the values. I see that the line-based text data on your Wireshark screenshot for the browser includes parameters such as SegmentPosition which is 0, but in your Python screenshot does not have a value for SegmentPosition. The format for some of the parameters such as Segment seem different between the Browser and the Python request which may be causing an error as it tries to parse it.']"
277,Websites that are particularly challenging to crawl and scrape? [closed],"I'm interested in public facing sites (nothing behind a login / authentication) that have things like:
High use of internal 301 and 302 redirects
Anti-scraping measures (but not banning crawlers via ...",https://stackoverflow.com/questions/18762334/websites-that-are-particularly-challenging-to-crawl-and-scrape,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'Here are some:']"
278,Crawling LinkedIn while authenticated with Scrapy,"So I've read through the Crawling with an authenticated session in Scrapy and I am getting hung up, I am 99% sure that my parse code is correct, I just don't believe the login is redirecting and being ...",https://stackoverflow.com/questions/10953991/crawling-linkedin-while-authenticated-with-scrapy,"[""So I've read through the Crawling with an authenticated session in Scrapy and I am getting hung up, I am 99% sure that my parse code is correct, I just don't believe the login is redirecting and being successful."", 'should be:']"
279,"POST request works in Postman, but not in Python Requests (200 response with robot detection)","I have a POST request that works perfectly with both Postman an cURL (it returns a JSON blob of data). However, when I perform the exact same request with Python's Requests library, I get a 200 ...",https://stackoverflow.com/questions/54878769/post-request-works-in-postman-but-not-in-python-requests-200-response-with-rob,"[""I have a POST request that works perfectly with both Postman an cURL (it returns a JSON blob of data). However, when I perform the exact same request with Python's Requests library, I get a 200 success response, but instead of my JSON blob, I get this:"", 'You are getting a 200 success response but not JSON data in the response.\nThis means that is just a response object. It contains only response code\nto extract blob information from the response, convert response object to json\nsimply json_resp = response_raw.json() \nThis json_resp contains your actual response details.', 'I had a similar issue that I was able to resolve by sending a cookie in the request. Try this:']"
280,How to Scrape Google Map? [closed],"Sorry for this question, I am new to this.

I have a project where I need to scrape Google Maps to find all the companies in a region, I just heard about the term when we decided on the project, I ...",https://stackoverflow.com/questions/36539442/how-to-scrape-google-map,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", 'You are not legally allowed to scrape data from Google Maps API. A better practice would be to store the place_id of any place and retrieve it for later use.', 'Disclosure: I work at SerpApi.']"
281,Downloading file with Python mechanize,"I am trying to download a file from a website using python and mechanize. My current code successfully logs on to the website and opens the page that contains the download link.

The download link is: ...",https://stackoverflow.com/questions/11002014/downloading-file-with-python-mechanize,"['I am trying to download a file from a website using python and mechanize. My current code successfully logs on to the website and opens the page that contains the download link.', ""For anyone who's interested, this was the solution:""]"
282,Pass the user-agent through webdriver in Selenium,"I am working on a website scraping project using Selenium in Python. When I open the homepage through a browser, it opens properly.

But, when I try to open the webpage through webdriver() in Selenium,...",https://stackoverflow.com/questions/8286127/pass-the-user-agent-through-webdriver-in-selenium,"['I am working on a website scraping project using Selenium in Python. When I open the homepage through a browser, it opens properly.', ""Changing the user agent in the python version of webdriver is done by altering your browser's profile. I have only done this for webdriver.Firefox() by passing a profile parameter. You need to do the following:"", 'Assuming the user-agent is the problem, in Java you can modify it like this:']"
283,CSS Selector to get the element attribute value,"The HTML structure is like this:

<td class='hey'> 
<a href=""https://example.com"">First one</a>
</td>
This is my selector:

m_URL = sel.css(""td.hey a:nth-child(1)[href] "")....",https://stackoverflow.com/questions/24987480/css-selector-to-get-the-element-attribute-value,"['The HTML structure is like this:', 'Get the ::attr(value) from the a tag.', 'you may try this:']"
284,HTML encoding and lxml parsing,"I'm trying to finally solve some encoding issues that pop up from trying to scrape HTML with lxml. Here are three sample HTML documents that I've encountered:

1.

<!DOCTYPE html>
<html lang='...",https://stackoverflow.com/questions/15302125/html-encoding-and-lxml-parsing,"[""I'm trying to finally solve some encoding issues that pop up from trying to scrape HTML with lxml. Here are three sample HTML documents that I've encountered:"", 'lxml has several issues related to handling Unicode. It might be best to use bytes (for now) while specifying the character encoding explicitly:', ""The problem probably stems from the fact that <meta charset> is a relatively new standard (HTML5 if I'm not mistaken, or it wasn't really used before it.)""]"
285,Using urllib and BeautifulSoup to retrieve info from web with Python,"I can get the html page using urllib, and use BeautifulSoup to parse the html page, and it looks like that I have to generate file to be read from BeautifulSoup.

import urllib                         ...",https://stackoverflow.com/questions/2647179/using-urllib-and-beautifulsoup-to-retrieve-info-from-web-with-python,"['I can get the html page using urllib, and use BeautifulSoup to parse the html page, and it looks like that I have to generate file to be read from BeautifulSoup.', 'No file writing needed: Just pass in the HTML string. You can also pass the object returned from urlopen directly:', 'You could open the url, download the html, and make it parse-able in one shot with gazpacho:']"
286,Scrapy concurrency strategy,"What is better method of scaling Scrapy? 
By running one scrapy process and increasing CONCURRENT_REQUESTS internal Scrapy's setting
By running multiple scrapy processes but still focusing on ...",https://stackoverflow.com/questions/24691309/scrapy-concurrency-strategy,"['What is better method of scaling Scrapy?', 'Scrapyd is a great tool for managing Scrapy processes. But the best answer I can give is that it depends. First you need to figure out where your bottleneck is.', 'Scrapyd was made exactly for deploying and running scrapy spiders. Basically it is a daemon that listens to requests for spiders to run. Scrapyd runs spiders in multiple processes, you can control the behavior with max_proc and max-proc-per-cpu settings:', 'This might not be exactly in your predefined choices, but for concurrency and delays management, you can improve your overall configuration by cutting off every hard limits in your internal settings and letting the Autothrottle extension work on that for you.']"
287,scrape multiple linked HTML tables in R and rvest,"This article http://www.ajnr.org/content/30/7/1402.full contains four links to html-tables which I would like to scrape with rvest.

With help of the css selector:

""#T1 a"" 
it's possible to get to ...",https://stackoverflow.com/questions/28729507/scrape-multiple-linked-html-tables-in-r-and-rvest,"['This article http://www.ajnr.org/content/30/7/1402.full contains four links to html-tables which I would like to scrape with rvest.', ""Here's one approach:"", 'You might want to use as follows:']"
288,College/University data API [closed],"I'm trying to build an application that allows users to look up a specific university and see data about it (admission rate, SAT scores, size, etc.). However, I can't find an API/database that I can ...",https://stackoverflow.com/questions/15775893/college-university-data-api,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", ""Just figured it out. Turns out you can use the IPEDS database to get this info, but it doesn't offer an API to do it. Go to IPEDS and create a group of institutions you want (ex. all 4-year, degree granting universities in the United States), select what variables you want (address, admission rate, etc.), then finally export the data in CSV. If you want the data in a less terrible format, just convert it to JSON or whatever you'd like."", 'An extensive collection of data in json, csv, pdf, and other formats is at:']"
289,How can I use R (Rcurl/XML packages ?!) to scrape this webpage?,"I have a (somewhat complex) web scraping challenge that I wish to accomplish and would love for some direction (to whatever level you feel like sharing) here goes:

I would like to go through all the ""...",https://stackoverflow.com/questions/2443127/how-can-i-use-r-rcurl-xml-packages-to-scrape-this-webpage,"['I have a (somewhat complex) web scraping challenge that I wish to accomplish and would love for some direction (to whatever level you feel like sharing) here goes:', 'Tal,', 'Just tried it using Mozenda (http://www.mozenda.com).  After roughly 10 minutes and I had an agent that could scrape the data as you describe.  You may be able to get all of this data just using their free trial.  Coding is fun, if you have time, but it looks like you may already have a solution coded for you.  Nice job Drew.', 'Interesting problem and agree that R is cool, but somehow i find R to be a bit cumbersome in this respect. I seem to prefer to get the data in intermediate plain text form first in order to be able to verify that the data is correct in every step...  If the data is ready in its final form or for uploading your data somewhere RCurl is very useful.']"
290,How to extract all the current video files and its address in the web page with js?,"var imgs=document.images.length;
It can extract all the images on the web page.
How extract all the flv files whose suffix is flv such as sample.flv in the web page with js?Not all the flv files on ...",https://stackoverflow.com/questions/40379347/how-to-extract-all-the-current-video-files-and-its-address-in-the-web-page-with,"[""It can extract all the images on the web page.\nHow extract all the flv files whose suffix is flv such as sample.flv in the web page with js?Not all the flv files on my local directory but web page.\nThe plugin Video DownloadHelper in firefox can get the current mp4 file.\n\nWhy my js code can't do the same task?"", 'Yahoo Movies use blob stream to transfer video data. There are no direct mp4/flv links anywhere, nor can you get such links directly. The src of the <video> tag refers to a blob stream link:', ""If I'm understanding correctly you want to look for all the links which have an associated .flv."", 'to get all flv files of a directory, please try the following code -', 'You can inspect all video requests on Chrome -> Networks tab in Media sub tab.']"
291,Difference between BaseSpider and CrawlSpider,I have been trying to understand the concept of using BaseSpider and CrawlSpider in web scrapping. I have read the docs. But there is no mention on BaseSpider. It would be really helpful to me if ...,https://stackoverflow.com/questions/32632001/difference-between-basespider-and-crawlspider,"['I have been trying to understand the concept of using BaseSpider and CrawlSpider in web scrapping. I have read the docs. But there is no mention on BaseSpider. It would be really helpful to me if someone explain the differences between BaseSpider and CrawlSpider.', 'BaseSpider is something existed before and now is deprecated (since 0.22) - use scrapy.Spider instead:']"
292,Can't get a certain item from a webpage using requests,I've created a script to scrape name and email address from a webpage. When I run my script I get the name accordingly but in case of email this is what I get aeccdcd7cfc0eedadcc783cdc1dc80cdc1c3. The ...,https://stackoverflow.com/questions/58148170/cant-get-a-certain-item-from-a-webpage-using-requests,"[""I've created a script to scrape name and email address from a webpage. When I run my script I get the name accordingly but in case of email this is what I get aeccdcd7cfc0eedadcc783cdc1dc80cdc1c3. The string that I get instead of email changes every time I run the script."", 'You have to decode the email.', 'The short answer is that you have to decode the email string, because it is being obfuscated.']"
293,Using R to scrape the link address of a downloadable file from a web page?,"I'm trying to automate a process that involves downloading .zip files from a couple of web pages and extracting the .csvs they contain. The challenge is that the .zip file names, and thus the link ...",https://stackoverflow.com/questions/31517121/using-r-to-scrape-the-link-address-of-a-downloadable-file-from-a-web-page,"[""I'm trying to automate a process that involves downloading .zip files from a couple of web pages and extracting the .csvs they contain. The challenge is that the .zip file names, and thus the link addresses, change weekly or annually, depending on the page. Is there a way to scrape the current link addresses from those pages so I can then feed those addresses to a function that downloads the files?"", ""I think you're trying to do too much in a single xpath expression - I'd attack the problem in a sequence of smaller steps:""]"
294,Selenium pdf automatic download not working,"I am new to selenium and I am writing a scraper to download pdf files automatically from a given site. 

Below is my code:

from selenium import webdriver

fp = webdriver.FirefoxProfile()

fp....",https://stackoverflow.com/questions/30452395/selenium-pdf-automatic-download-not-working,"['I am new to selenium and I am writing a scraper to download pdf files automatically from a given site.', 'Disable the built-in pdfjs plugin and navigate to the URL - the PDF file would be downloaded automatically, the code:', 'I tested the following code and I succesfully downloaded your pdf on Windows 7:', 'Since there is not HTML code available, my guess is that this line']"
295,Multithreading in Python/BeautifulSoup scraping doesn't speed up at all,"I have a csv file (""SomeSiteValidURLs.csv"") which listed all the links I need to scrape. The code is working and will go through the urls in the csv, scrape the information and record/save in another ...",https://stackoverflow.com/questions/25373167/multithreading-in-python-beautifulsoup-scraping-doesnt-speed-up-at-all,"['I have a csv file (""SomeSiteValidURLs.csv"") which listed all the links I need to scrape. The code is working and will go through the urls in the csv, scrape the information and record/save in another csv file (""Output.csv""). However, since I am planning to do it for a large portion of the site (for >10,000,000 pages), speed is important. For each link, it takes about 1s to crawl and save the info into the csv, which is too slow for the magnitude of the project. So I have incorporated the multithreading module and to my surprise it doesn\'t speed up at all, it still takes 1s person link. Did I do something wrong? Is there other way to speed up the processing speed?', ""You're not parallelizing this properly. What you actually want to do is have the work being done inside your for loop happen concurrently across many workers. Right now you're moving all the work into one background thread, which does the whole thing synchronously. That's not going to improve performance at all (it will just slightly hurt it, actually).""]"
296,httrack wget curl scrape & fetch,"There are a number of tools on the internet for downloading a static copy of a website, such as HTTrack. There are also many tools, some commercial, for “scraping” content from a website, such as ...",https://stackoverflow.com/questions/19098315/httrack-wget-curl-scrape-fetch,"['There are a number of tools on the internet for downloading a static copy of a website, such as HTTrack. There are also many tools, some commercial, for “scraping” content from a website, such as Mozenda. Then there are tools which are apparently built in to programs like PHP and *nix where you can “file_get_contents” or “wget” or “cURL” or just “file()”.', 'First, let me clarify the difference between ""mirroring"" and ""scraping"".', 'There is one more thing, rather technical, that would be said:\nduring downloading (mirroring), HTTrack/wget/curl not only downloads many HTML files, but also changes internal links in these files - so, that these links allow proper ""running"" of the downloaded web pages on the new location (or on new domain or on your local). These internal links include: links from one downloaded page to another downloaded page, links to embeded images and other media and links to ""auxiliary"" files, like javascripts, ccs and so on.']"
297,Trouble fetching some title from a webpage,"I've written a script in php to scrape a title visible as hair fall shamboo from a webpage. When I execute my below script, I get the following error:
  Notice: Trying to get property 'nodeValue' of ...",https://stackoverflow.com/questions/52682173/trouble-fetching-some-title-from-a-webpage,"[""I've written a script in php to scrape a title visible as hair fall shamboo from a webpage. When I execute my below script, I get the following error:"", 'It could very well be that there are more issues with your code than I have covered in this answer, but the most prominent issue that I see is the following:', ""What's wrong with php then?""]"
298,AttributeError: module 'sys' has no attribute 'setdefaultencoding',"My original code is this.

#py3.6, windows10   
import time
from selenium import webdriver
import codecs
import sys

reload(sys)
sys.setdefaultencoding('utf-8')
Reload is not supported. It was fixed.
...",https://stackoverflow.com/questions/45252305/attributeerror-module-sys-has-no-attribute-setdefaultencoding,"['My original code is this.', 'You should remove the sys.setdefaultencoding. Notice that this has been an abuse of sys.setdefaultencoding all along in Python 2 too. From Python 2 documentation:']"
299,Scraping password protected forum in r,"I have a problem with logging in in my script. Despite all other good answers that I found on stackoverflow, none of the solutions worked for me. 

I am scraping a web forum for my PhD research, its ...",https://stackoverflow.com/questions/32434478/scraping-password-protected-forum-in-r,"['I have a problem with logging in in my script. Despite all other good answers that I found on stackoverflow, none of the solutions worked for me.', 'Thanks to Simon I found the answer here: Using rvest or httr to log in to non-standard forms on a webpage']"
300,stumped on how to scrape the data from this site (using R),"I am trying to scrape the data, using R, from this site:
http://www.soccer24.com/kosovo/superliga/results/#

I can do the following:

library(rvest)
doc <- html(""http://www.soccer24.com/kosovo/...",https://stackoverflow.com/questions/29431787/stumped-on-how-to-scrape-the-data-from-this-site-using-r,"['I am trying to scrape the data, using R, from this site:\nhttp://www.soccer24.com/kosovo/superliga/results/#', 'Using Selenium with phantomjs']"
