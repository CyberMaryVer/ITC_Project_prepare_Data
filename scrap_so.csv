,question,summary,link,answers
0,,,,
1,Options for HTML scraping? [closed],"I'm thinking of trying Beautiful Soup, a Python package for HTML scraping. Are there any other HTML scraping packages I should be looking at? Python is not a requirement, I'm actually interested in ...",https://stackoverflow.com/questions/2861/options-for-html-scraping,". 
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                . The Ruby world's equivalent to Beautiful Soup is why_the_lucky_stiff's Hpricot.. In the .NET world, I recommend the HTML Agility Pack. Not near as simple as some of the above options (like HTMLSQL), but it's very flexible. It lets you maniuplate poorly formed HTML as if it were well formed XML, so you can use XPATH or just itereate over nodes.. BeautifulSoup is a great way to go for HTML scraping. My previous job had me doing a lot of scraping and I wish I knew about BeautifulSoup when I started. It's like the DOM with a lot more useful options and is a lot more pythonic. If you want to try Ruby they ported BeautifulSoup calling it RubyfulSoup but it hasn't been updated in a while.. I found HTMLSQL to be a ridiculously simple way to screenscrape. It takes literally minutes to get results with it.. The Python lxml library acts as a Pythonic binding for the libxml2 and libxslt libraries. I like particularly its XPath support and pretty-printing of the in-memory XML structure. It also supports parsing broken HTML. And I don't think you can find other Python libraries/bindings that parse XML faster than lxml.. For Perl, there's WWW::Mechanize.. Python has several options for HTML scraping in addition to Beatiful Soup. Here are some others:. 'Simple HTML DOM Parser' is a good option for PHP, if your familiar with jQuery or JavaScript selectors then you will find yourself at home.. Why has no one mentioned JSOUP yet for Java? http://jsoup.org/. The templatemaker utility from Adrian Holovaty (of Django fame) uses a very interesting approach: You feed it variations of the same page and it ""learns"" where the ""holes"" for variable data are. It's not HTML specific, so it would be good for scraping any other plaintext content as well. I've used it also for PDFs and HTML converted to plaintext (with pdftotext and lynx, respectively).. I know and love Screen-Scraper.. I would first find out if the site(s) in question provide an API server or RSS Feeds for access the data you require.. Scraping Stack Overflow is especially easy with Shoes and Hpricot.. Another option for Perl would be Web::Scraper which is based on Ruby's Scrapi. In a nutshell, with nice and concise syntax, you can get a robust scraper directly into data structures.. I've had some success with HtmlUnit, in Java. It's a simple framework for writing unit tests on web UI's, but equally useful for HTML scraping.. Yahoo! Query Language or YQL can be used alongwith jQuery, AJAX, JSONP to screen scrape web pages. Another tool for .NET is MhtBuilder. There is this solution too: netty HttpClient. I use Hpricot on Ruby. As an example this is a snippet of code that I use to retrieve all book titles from the six pages of my HireThings account (as they don't seem to provide a single page with this information):. I've used Beautiful Soup a lot with Python. It is much better than regular expression checking, because it works like using the DOM, even if the HTML is poorly formatted. You can quickly find HTML tags and text with simpler syntax than regular expressions. Once you find an element, you can iterate over it and its children, which is more useful for understanding the contents in code than it is with regular expressions. I wish Beautiful Soup existed years ago when I had to do a lot of screenscraping -- it would have saved me a lot of time and headache since HTML structure was so poor before people started validating it.. Although it was designed for .NET web-testing, I've been using the WatiN framework for this purpose. Since it is DOM-based, it is pretty easy to capture HTML, text, or images. Recentely, I used it to dump a list of links from a MediaWiki All Pages namespace query into an Excel spreadsheet. The following VB.NET code fragement is pretty crude, but it works.. Implementations of the HTML5 parsing algorithm: html5lib (Python, Ruby), Validator.nu HTML Parser (Java, JavaScript; C++ in development), Hubbub (C), Twintsam (C#; upcoming).. You would be a fool not to use Perl.. Here come the flames.. . I have used LWP and HTML::TreeBuilder with Perl and have found them very useful.. In Java, you can use TagSoup.. Well, if you want it done from the client side using only a browser you have jcrawl.com. After having designed your scrapping service from the web application (http://www.jcrawl.com/app.html), you only need to add the generated script to an HTML page to start using/presenting your data.. You probably have as much already, but I think this is what you are trying to do:. I've had mixed results in .NET using SgmlReader which was originally started by Chris Lovett and appears to have been updated by MindTouch.. I like Google Spreadsheets' ImportXML(URL, XPath) function.. I've also had great success using Aptana's Jaxer + jQuery to parse pages. It's not as fast or 'script-like' in nature, but jQuery selectors + real JavaScript/DOM is a lifesaver on more complicated (or malformed) pages."
2,Headless Browser and scraping - solutions [closed],"I'm trying to put list of possible solutions for browser automatic tests suits and headless browser platforms capable of scraping.
BROWSER TESTING / SCRAPING:
Selenium - polyglot flagship in browser ...",https://stackoverflow.com/questions/18539491/headless-browser-and-scraping-solutions,". 
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                . If Ruby is your thing, you may also try:. http://triflejs.org/ is like phantomjs but based on IE. A kind of JS-based Selenium is Dalek.js. It not only aims for automated frontend-tests, you can also do screenshots with it. It has webdrivers for all important browsers. Unfortunately those webdrivers seem to be worth improving (just not to say ""buggy"" to Firefox)."
3,How can I get the Google cache age of any URL or web page? [closed],"In my project I need the Google cache age to be added as important information. I tried to search sources for the Google cache age, that is, the number of days since Google last re-indexed the page ...",https://stackoverflow.com/questions/4560400/how-can-i-get-the-google-cache-age-of-any-url-or-web-page,". 
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                . Use the URL. You'll need to scrape the resulting page, but you can view the most recent cache page using this URL:. you can Use CachedPages website   . its too simple, you can just type ""cache:"" before the URL of the page. for example
if you want to check the last webcache of this page simply type on URL bar cache:http://stackoverflow.com/questions/4560400/how-can-i-get-the-google-cache-age-of-any-url-or-web-page. You can use this site: https://cachedviews.com/ . Cache View or Cached Pages of Any Website - Google Cached Pages of Any Website. This one good also to view cachepage http://www.cachepage.net"
4,Which HTML Parser is the best? [closed],"I code a lot of parsers. Up until now, I was using HtmlUnit headless browser for parsing and browser automation.

Now, I want to separate both the tasks.

As 80% of my work involves just parsing, I ...",https://stackoverflow.com/questions/2168610/which-html-parser-is-the-best,". I code a lot of parsers. Up until now, I was using HtmlUnit headless browser for parsing and browser automation.. Self plug: I have just released a new Java HTML parser: jsoup. I mention it here because I think it will do what you are after.. The best I've seen so far is HtmlCleaner:. I suggest Validator.nu's parser, based on the HTML5 parsing algorithm. It is the parser used in Mozilla from 2010-05-03"
5,Web-scraping JavaScript page with Python,"I'm trying to develop a simple web scraper. I want to extract text without the HTML code. In fact, I achieve this goal, but I have seen that in some pages where JavaScript is loaded I didn't obtain ...",https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python,". I'm trying to develop a simple web scraper. I want to extract text without the HTML code. In fact, I achieve this goal, but I have seen that in some pages where JavaScript is loaded I didn't obtain good results.. EDIT 30/Dec/2017: This answer appears in top results of Google searches, so I decided to update it. The old answer is still at the end.. We are not getting the correct results because any javascript generated content needs to be rendered on the DOM. When we fetch an HTML page, we fetch the initial, unmodified by javascript, DOM.. Maybe selenium can do it.. If you have ever used the Requests module for python before, I recently found out that the developer created a new module called Requests-HTML which now also has the ability to render JavaScript.. This seems to be a good solution also, taken from a great blog post. It sounds like the data you're really looking for can be accessed via secondary URL called by some javascript on the primary page.. Selenium is the best for scraping JS and Ajax content.. You can also execute javascript using webdriver.. I personally prefer using scrapy and selenium and dockerizing both in separate containers. This way you can install both with minimal hassle and crawl modern websites that almost all contain javascript in one form or another. Here's an example:. A mix of BeautifulSoup and Selenium works very well for me.. You'll want to use urllib, requests, beautifulSoup and selenium web driver in your script for different parts of the page, (to name a few).
Sometimes you'll get what you need with just one of these modules.
Sometimes you'll need two, three, or all of these modules.
Sometimes you'll need to switch off the js on your browser.
Sometimes you'll need header info in your script.
No websites can be scraped the same way and no website can be scraped in the same way forever without having to modify your crawler, usually after a few months. But they can all be scraped! Where there's a will there's a way for sure.
If you need scraped data continuously into the future just scrape everything you need and store it in .dat files with pickle.
Just keep searching how to try what with these modules and copying and pasting your errors into the Google.. Using PyQt5. I've been trying to find answer to this questions for two days. Many answers direct you to different issues. But serpentr's answer above is really to the point. It is the shortest, simplest solution. Just a reminder the last word ""var"" represents the variable name, so should be used as:. As mentioned, Selenium is a good choice for rendering the results of the JavaScript:"
6,Web scraping with Python [closed],I'd like to grab daily sunrise/sunset times from a web site. Is it possible to scrape web content with Python? what are the modules used? Is there any tutorial available?,https://stackoverflow.com/questions/2081586/web-scraping-with-python,". 
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                . Use urllib2 in combination with the brilliant BeautifulSoup library:. I'd really recommend Scrapy.. I collected together scripts from my web scraping work into this bit-bucket library.. I would strongly suggest checking out pyquery. It uses jquery-like (aka css-like) syntax which makes things really easy for those coming from that background.. You can use urllib2 to make the HTTP requests, and then you'll have web content.. I use a combination of Scrapemark (finding urls - py2) and httlib2 (downloading images - py2+3). The scrapemark.py has 500 lines of code, but uses regular expressions, so it may be not so fast, did not test.. Make your life easier by using CSS Selectors. If we think of getting name of items from any specific category then we can do that by specifying the class name of that category using css selector:. Here is a simple web crawler, i used BeautifulSoup and we will search for all the links(anchors) who's class name is _3NFO0d. I used Flipkar.com, it is an online retailing store.. Python has good options to scrape the web. The best one with a framework is scrapy. It can be a little tricky for beginners, so here is a little help. 
1. Install python above 3.5 (lower ones till 2.7 will work). 
2. Create a environment in conda ( I did this). 
3. Install scrapy at a location and run in from there. 
4. Scrapy shell will give you an interactive interface to test you code. 
5. Scrapy startproject projectname will create a framework.
6. Scrapy genspider spidername will create a spider. You can create as many spiders as you want. While doing this make sure you are inside the project directory. 

"
7,Scraping html tables into R data frames using the XML package,"How do I scrape html tables using the XML package?

Take, for example, this wikipedia page on the Brazilian soccer team. I would like to read it in R and get the ""list of all matches Brazil have ...",https://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package,. How do I scrape html tables using the XML package?. …or a shorter try:. Edited to add:. Another option using Xpath.. The rvest along with xml2 is another popular package for parsing html web pages.
8,Puppeteer: pass variable in .evaluate(),"I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined.

I'm new to Puppeteer and can't ...",https://stackoverflow.com/questions/46088351/puppeteer-pass-variable-in-evaluate,". I'm trying to pass a variable into a page.evaluate() function in Puppeteer, but when I use the following very simplified example, the variable evalVar is undefined.. You have to pass the variable as an argument to the pageFunction like this:. I encourage you to stick on this style, because it's more convenient and readable.. You can pass one variable to page.evaluate() using the following syntax:. It took me quite a while to figure out that console.log() in evaluate() can't show in node console. . For pass a function, there are two ways you can do it.. I have a typescript example that could help someone new in typescript.. With page.$$eval"
9,How to use Python requests to fake a browser visit?,"I want to get the content from the below website. If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get ...",https://stackoverflow.com/questions/27652543/how-to-use-python-requests-to-fake-a-browser-visit,". I want to get the content from the below website. If I use a browser like Firefox or Chrome I could get the real website page I want, but if I use the Python requests package (or wget command) to get it, it returns a totally different HTML page. I thought the developer of the website had made some blocks for this, so the question is:. Provide a User-Agent header:. if this question is still valid. Try doing this, using firefox as fake user agent (moreover, it's a good startup script for web scraping with the use of cookies):. The root of the answer is that the person asking the question needs to have a JavaScript interpreter to get what they are after. What I have found is I am able to get all of the information I wanted on a website in json before it was interpreted by JavaScript. This has saved me a ton of time in what would be parsing html hoping each webpage is in the same format."
10,Scraping: SSL: CERTIFICATE_VERIFY_FAILED error for http://en.wikipedia.org,"I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:

from urllib.request import urlopen 
from bs4 import BeautifulSoup 
import re

pages = set()
def ...",https://stackoverflow.com/questions/50236117/scraping-ssl-certificate-verify-failed-error-for-http-en-wikipedia-org,". I'm practicing the code from 'Web Scraping with Python', and I keep having this certificate problem:. Once upon a time I stumbled  with this issue. If you're using macOS go to Macintosh HD > Applications > Python3.6 folder (or whatever version of python you're using) > double click on ""Install Certificates.command"" file. :D. To solve this: . to use unverified ssl you can add this to your code:. This terminal command:. For novice users, you can go in the Applications folder and expand the Python 3.7 folder. Now first run (or double click) the Install Certificates.command and then Update Shell Profile.command. Two steps worked for me :
- going Macintosh HD > Applications > Python3.7 folder 
- click on ""Install Certificates.command"". Take a look at this post, it seems like for later versions of Python, certificates are not pre installed which seems to cause this error. You should be able to run the following command to install the certifi package: /Applications/Python\ 3.6/Install\ Certificates.command. For anyone who is using anaconda, you would install the certifi package, see more at: . I could find this solution and is working fine:. i didn't solve the problem, sadly.
but managed to make to codes work (almost all of my codes have this probelm btw)
the local issuer certificate problem happens under python3.7
so i changed back to python2.7 QAQ
and all that needed to change including ""from urllib2 import urlopen"" instead of ""from urllib.request import urlopen""
so sad.... If you're running on a Mac you could just search for Install Certificates.command on the spotlight and hit enter.. For me the problem was that I was setting REQUESTS_CA_BUNDLE in my .bash_profile. Use requests library.
Try this solution, or just add https:// before the URL:. I'm a relative novice compared to all the experts on Stack Overflow.. This will work. Set the environment variable PYTHONHTTPSVERIFY to 0.. Install Certificates.command on your mac.. The only solution that worked for me:"
11,What's the best way of scraping data from a website? [closed],"I need to extract contents from a website, but the application doesn’t provide any application programming interface or another mechanism to access that data programmatically.

I found a useful third-...",https://stackoverflow.com/questions/22168883/whats-the-best-way-of-scraping-data-from-a-website,". 
Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.
                . You will definitely want to start with a good web scraping framework. Later on you may decide that they are too limiting and you can put together your own stack of libraries but without a lot of scraping experience your design will be much worse than pjscrape or scrapy.. Yes you can do it yourself. It is just a matter of grabbing the sources of the page and parsing them the way you want. "
12,What is the difference between web-crawling and web-scraping? [duplicate],"Is there a difference between Crawling and Web-scraping?

If there's a difference, what's the best method to use in order to collect some web data to supply a database for later use in a customised ...",https://stackoverflow.com/questions/4327392/what-is-the-difference-between-web-crawling-and-web-scraping,". Is there a difference between Crawling and Web-scraping?. Crawling would be essentially what Google, Yahoo, MSN, etc. do, looking for ANY information.  Scraping is generally targeted at certain websites, for specfic data, e.g. for price comparison, so are coded quite differently.. Yes, they are different. In practice, you may need to use both.. AFAIK Web Crawling is what Google does - it goes around a website looking at links and building a database of the layout of that site and sites it links to. There's a fundamental difference between these two. 
For those looking to dig deeper, I suggest you read this - 
Web scraper, Web Crawler. There's definitely a difference between these two. One refers to visiting a site, the other to extracting.. We crawl sites to have broad perspective how the site is structured, what are connections between pages, to estimate how much time we need to visit all pages we are interested in. Scraping is often harder to implement, but it’s an essence of data extraction. Let’s think of scraping as of covering website with sheet of paper with some rectangles cut out. We can now see only things we need, completely ignoring parts of website that are common for all pages (like navigation, footer, ads), or extraneous informations as comments or breadcrumbs.
More about differences between crawling and scrapping you find here: https://tarantoola.io/web-scraping-vs-web-crawling/"
13,XPath:: Get following Sibling,"I have following HTML Structure: I am trying to build a robust method to extract second color digest element since there will be many of these tag within the DOM.

<table>
  <tbody>
    &...",https://stackoverflow.com/questions/11657223/xpath-get-following-sibling,". I have following HTML Structure: I am trying to build a robust method to extract second color digest element since there will be many of these tag within the DOM.. You should be looking for the second tr that has the td that equals ' Color Digest ', then you need to look at either the following sibling of the first td in the tr, or the second td.. You can go for identifying a list of elements with xPath:. /html/body/table/tbody/tr[9]/td[1]"
14,selenium with scrapy for dynamic page,"I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:
starts with a product_list page with 10 products
a click on ""next""  button loads the ...",https://stackoverflow.com/questions/17975471/selenium-with-scrapy-for-dynamic-page,". I'm trying to scrape product information from a webpage, using scrapy. My to-be-scraped webpage looks like this:. It really depends on how do you need to scrape the site and how and what data do you want to get. . If (url doesn't change between the two pages) then you should add dont_filter=True with your scrapy.Request() or scrapy will find this url as a duplicate after processing first page. "
15,Web scraping with Java,I'm not able to find any good web scraping Java based API. The site which I need to scrape does not provide any API as well; I want to iterate over all web pages using some pageID and extract the HTML ...,https://stackoverflow.com/questions/3202305/web-scraping-with-java,". I'm not able to find any good web scraping Java based API. The site which I need to scrape does not provide any API as well; I want to iterate over all web pages using some pageID and extract the HTML titles / other stuff in their DOM trees.. Extracting the title is not difficult, and you have many options, search here on Stack Overflow for ""Java HTML parsers"". One of them is Jsoup.. Your best bet is to use Selenium Web Driver since it. HTMLUnit can be used to do web scraping, it supports invoking pages, filling & submitting forms. I have used this in my project. It is good java library for web scraping.
read here for more. mechanize for Java would be a good fit for this, and as Wadjy Essam mentioned it uses JSoup for the HMLT. mechanize is a stageful HTTP/HTML client that supports navigation, form submissions, and page scraping.. There is also Jaunt Java Web Scraping & JSON Querying - http://jaunt-api.com. You might look into jwht-scrapper!. Look at an HTML parser such as TagSoup, HTMLCleaner or NekoHTML.. If you wish to automate scraping of large amount pages or data, then you could try Gotz ETL. . For tasks of this type I usually use Crawller4j + Jsoup.. Normally I use selenium, which is software for testing automation.
You can control a browser through a webdriver, so you will not have problems with javascripts and it is usually not very detected if you use the full version. Headless browsers can be more identified."
16,Web Scraping in a Google Chrome Extension (JavaScript + Chrome APIs),What are the best options for performing Web Scraping of a not currently open tab from within a Google Chrome Extension with JavaScript and whatever more technologies are available. Other JavaScript-...,https://stackoverflow.com/questions/6508393/web-scraping-in-a-google-chrome-extension-javascript-chrome-apis,". What are the best options for performing Web Scraping of a not currently open tab from within a Google Chrome Extension with JavaScript and whatever more technologies are available. Other JavaScript-libraries are also accepted.. Attempt to use XHR2 responseType = ""document"" and fall back on (new DOMParser).parseFromString(responseText, getResponseHeader(""Content-Type"")) with my text/html patch. See https://gist.github.com/1138724 for an example of how I detect responseType = ""document support (synchronously checking response === null on an object URL created from a text/html blob).. If you are fine looking at something beyond a Google Chrome Plugin, look at phantomjs which uses Qt-Webkit in background and runs just like a browser incuding making ajax requests. You can call it a headless browser as it doesn't display the output on a screen and can quitely work in background while you are doing other stuff. If you want, you can export out images, pdf out of the pages it fetches. It provides JS interface to load pages, clicking on buttons etc much like you have in a browser. You can also inject custom JS for example jQuery on any of the pages you want to scrape and use it to access the dom and export out desired data. As its using Webkit its rendering behaviour is exactly like Google Chrome.. A lot of tools have been released since this question was asked.. Web scraping is kind of convoluted in a Chrome Extension. Some points:. I'm not sure it's entirely possible with just JavaScript, but if you can set up a dedicated PHP script for your extension that uses cURL to fetch the HTML for a page, the PHP script could scrape the page for you and your extension could read it in through an AJAX request.. I think you can start from this example.. couldn't you just do some iframe trickery? if you load the url into a dedicated frame, you have the dom in a document object and can do your jquery selections, no?"
17,How to run Scrapy from within a Python script,"I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:

http://tryolabs.com/Blog/2011/09/27/calling-scrapy-python-script/

http://snipplr.com/...",https://stackoverflow.com/questions/13437402/how-to-run-scrapy-from-within-a-python-script,". I'm new to Scrapy and I'm looking for a way to run it from a Python script. I found 2 sources that explain this:. All other answers reference Scrapy v0.x. According to the updated docs, Scrapy 1.0 demands:. Though I haven't tried it I think the answer can be found within the scrapy documentation. To quote directly from it:. In scrapy 0.19.x you should do this:. Simply we can use. When there are multiple crawlers need to be run inside one python script, the reactor stop needs to be handled with caution as the reactor can only be stopped once and cannot be restarted. . it  is an improvement of
Scrapy throws an error when run using crawlerprocess. Put this code to the path you can run scrapy crawl abc_spider from command line. (Tested with Scrapy==0.24.6). If you want to run a simple crawling, It's easy by just running command: "
18,Is it ok to scrape data from Google results? [closed],"I'd like to fetch results from Google using curl to detect potential duplicate content.
Is there a high risk of being banned by Google?",https://stackoverflow.com/questions/22657548/is-it-ok-to-scrape-data-from-google-results,". 
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                . Google will eventually block your IP when you exceed a certain amount of requests. . Google disallows automated access in their TOS, so if you accept their terms you would break them.. Google thrives on scraping websites of the world...so if it was ""so illegal"" then even Google won't survive ..of course other answers mention ways of mitigating IP blocks by Google. One more way to explore avoiding captcha could be scraping at random times (dint try) ..Moreover, I have a feeling, that if we provide novelty or some significant processing of data then it sounds fine at least to me...if we are simply copying a website.. or hampering its business/brand in some way...then it is bad and should be avoided..on top of it all...if you are a startup then no one will fight you as there is no benefit.. but if your entire premise is on scraping even when you are funded then you should think of more sophisticated ways...alternative APIs..eventually..Also Google keeps releasing (or depricating)  fields for its API so what you want to scrap now may be in roadmap of new Google API releases.."
19,How to manage a 'pool' of PhantomJS instances,"I'm planning a webservice for my own use internally that takes one argument, a URL, and returns html representing the resolved DOM from that URL. By resolved I mean that the webservice will firstly ...",https://stackoverflow.com/questions/9961254/how-to-manage-a-pool-of-phantomjs-instances,". I'm planning a webservice for my own use internally that takes one argument, a URL, and returns html representing the resolved DOM from that URL. By resolved I mean that the webservice will firstly get the page at that URL, then use PhantomJS to 'render' the page, and then return the resulting source after all DHTML, AJAX calls etc are executed. However launching phantom on a per-request basis (which I'm doing now) is way too sluggish. I would rather have a pool of PhantomJS instances with one always available to serve the latest call to my webservice.. I setup a PhantomJs Cloud Service, and it pretty much does what you are asking.  It took me about 5 weeks of work implement.. The async JavaScript library works in Node and has a queue function that is quite handy for this kind of thing:. For my master thesis, I developed the library phantomjs-pool which does exactly this. It allows to provide jobs which are then mapped to PhantomJS workers. The library handles the job distribution, communication, error handling, logging, restarting and some more stuff. The library was successfully used to crawl more than one million pages.. As an alternative to @JasonS great answer you can try PhearJS, which I built. PhearJS is a supervisor written in NodeJS for PhantomJS instances and provides an API via HTTP. It is available open-source from Github.. if you are using nodejs why not use selenium-webdriver  . If you are using nodejs, you can use https://github.com/sgentle/phantomjs-node, which will allow you to connect an arbitrary number of phantomjs process to your main NodeJS process, hence, the ability to use async.js and lots of node goodies."
20,Scrape web pages in real time with Node.js,"What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several ...",https://stackoverflow.com/questions/5211486/scrape-web-pages-in-real-time-with-node-js,". What's a good was to scrape website content using Node.js. I'd like to build something very, very fast that can execute searches in the style of kayak.com, where one query is dispatched to several different sites, the results scraped, and returned to the client as they become available.. Node.io seems to take the cake :-). All aforementioned solutions presume running the scraper locally. This means you will be severely limited in performance (due to running them in sequence or in a limited set of threads). A better approach, imho, is to rely on an existing, albeit commercial, scraping grid.. I've been doing research myself, and https://npmjs.org/package/wscraper boasts itself as a. You don't always need to jQuery. If you play with the DOM returned from jsdom for example you can easily take what you need yourself (also considering you dont have to worry about xbrowser issues.) See: https://gist.github.com/1335009 that's not taking away from node.io at all, just saying you might be able to do it yourself depending.... Usually when you're scraping you want to use some method to. It is my easy to use general purpose scrapper https://github.com/harish2704/html-scrapper written for Node.JS
It can extract information based on predefined schemas.
A schema defnition includes a css selector and a data extraction function.
It currently using cheerio for dom parsing... check out https://github.com/rc0x03/node-promise-parser. I see most answers the right path with cheerio and so forth, however once you get to the point where you need to parse and execute JavaScript (ala SPA's and more), then I'd check out https://github.com/joelgriffith/navalia (I'm the author). Navalia is built to support scraping in a headless-browser context, and it's pretty quick. Thanks!"
21,Extracting an information from web page by machine learning,"I would like to extract a specific type of information from web pages in Python. Let's say postal address. It has thousands of forms, but still, it is somehow recognizable. As there is a large number ...",https://stackoverflow.com/questions/13336576/extracting-an-information-from-web-page-by-machine-learning,". I would like to extract a specific type of information from web pages in Python. Let's say postal address. It has thousands of forms, but still, it is somehow recognizable. As there is a large number of forms, it would be probably very difficult to write regular expression or even something like a grammar and to use a parser generator for parsing it out.. First, your task fits into the information extraction area of research. There are mainly 2 levels of complexity for this task:. tl;dr: The problem might solvable using ML, but it's not straightforward if you're new to the topic. As per i know there are two ways to do this task using machine learning approach.. Firstly, Machine Learning is not magic. These algorithms perform specific tasks, even if these can be a bit complex sometimes.. I would suggest you look at the field of information extraction. A lot of people have been researching how to do exactly what you're asking. There are some techniques for information extraction that are machine learning based, some techniques that are not machine learning based.. The approach needs to be a supervised learning algorithm (typically, they yield much better results than unsupervised or semi-supervised methods). Also, notice that you need to basically extract chunks of text. Intuitively, your algorithm needs to say something like, ""from this character onward, for the next three lines, is a postal address"".. I had built a solution exactly for this. My goal was to extract all the information related to competitions available on the internet. I used a tweak.
What I did is that I detected the pattern in which the information are listed on the websites. In my case, they were listed one by one below the order,
I detected that using the html table tags and got the information related to 
the competitions."
22,crawler vs scraper,Can somebody distinguish between a crawler and scraper in terms of scope and functionality.,https://stackoverflow.com/questions/3207418/crawler-vs-scraper,". Can somebody distinguish between a crawler and scraper in terms of scope and functionality.. A crawler gets web pages -- i.e., given a starting address (or set of starting addresses) and some conditions (e.g., how many links deep to go, types of files to ignore) it downloads whatever is linked to from the starting point(s).. Crawlers surf the web, following links.  An example would be the Google robot that gets pages to index.  Scrapers extract values from forms, but don't necessarily have anything to do with the web.. Web crawler gets links (Urls - Pages) in a logic and scrapper get values (extracting) from HTML. . Generally, crawlers would follow the links to reach numerous pages while scrapers is, in some sense, just pulling the contents displayed online and would not reach the deeper links. . Scrapers and crawlers do not always distinguish, I mean - you can find crawlers which scrape, in fact, Scraper Crawler is doing both and is named accordingly:"
23,What should I use to open a url instead of urlopen in urllib3,"I wanted to write a piece of code like the following:

from bs4 import BeautifulSoup
import urllib2

url = 'http://www.thefamouspeople.com/singers.php'
html = urllib2.urlopen(url)
soup = BeautifulSoup(...",https://stackoverflow.com/questions/36516183/what-should-i-use-to-open-a-url-instead-of-urlopen-in-urllib3,". I wanted to write a piece of code like the following:. urllib3 is a different library from urllib and urllib2. It has lots of additional features to the urllibs in the standard library, if you need them, things like re-using connections. The documentation is here: https://urllib3.readthedocs.org/. You do not have to install urllib3. You can choose any HTTP-request-making library that fits your needs and feed the response to BeautifulSoup. The choice is though usually requests because of the rich feature set and convenient API. You can install requests by entering pip install requests in the command line. Here is a basic example:. The new urllib3 library has a nice documentation here
In order to get your desired result you shuld follow that:. With gazpacho you could pipeline the page straight into a parse-able soup object:"
24,Save and render a webpage with PhantomJS and node.js,"I'm looking for an example of requesting a webpage, waiting for the JavaScript to render (JavaScript modifies the DOM), and then grabbing the HTML of the page.

This should be a simple example with an ...",https://stackoverflow.com/questions/9966826/save-and-render-a-webpage-with-phantomjs-and-node-js,". I'm looking for an example of requesting a webpage, waiting for the JavaScript to render (JavaScript modifies the DOM), and then grabbing the HTML of the page.. From your comments, I'd guess you have 2 options. With v2 of phantomjs-node it's pretty easy to print the HTML after it has been processed. . I've used two different ways in the past, including the page.evaluate() method that queries the DOM that Declan mentioned. The other way I've passed info from the web page is to spit it out to console.log() from there, and in the phantomjs script use:. Why not just use this ? . Late update in case anyone stumbles on this question:. Here's an old version that I use running node, express and phantomjs which saves out the page as a .png. You could tweak it fairly quickly to get the html."
25,"How to “scan” a website (or page) for info, and bring it into my program?","Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java). 

For example, if I know the exact page I want info from, for the sake of ...",https://stackoverflow.com/questions/2835505/how-to-scan-a-website-or-page-for-info-and-bring-it-into-my-program,". Well, I'm pretty much trying to figure out how to pull information from a webpage, and bring it into my program (in Java). . Use a HTML parser like Jsoup. This has my preference above the other HTML parsers available in Java since it supports jQuery like CSS selectors. Also, its class representing a list of nodes, Elements, implements Iterable so that you can iterate over it in an enhanced for loop (so there's no need to hassle with verbose Node and NodeList like classes in the average Java DOM parser).. This is referred to as screen scraping, wikipedia has this article on the more specific web scraping. It can be a major challenge because there's some ugly, mess-up, broken-if-not-for-browser-cleverness HTML out there, so good luck. . I would use JTidy - it is simlar to JSoup, but I don't know JSoup well. JTidy handles broken HTML and returns a w3c Document, so you can use this as a source to XSLT to extract the content you are really interested in. If you don't know XSLT, then you might as well go with JSoup, as the Document model is nicer to work with than w3c.. You may use an html parser (many useful links here: java html parser).. jsoup supports java 1.5. You'd probably want to look at the HTML to see if you can find strings that are unique and near your text, then you can use line/char-offsets to get to the data.. You could also try jARVEST.. My answer won't probably be useful to the writer of this question (I am 8 months late so not the right timing I guess) but I think it will probably be useful for many other developers that might come across this answer.. JSoup solution is great, but if you need to extract just something really simple it may be easier to use regex or String.indexOf. Look into the cURL library.  I've never used it in Java, but I'm sure there must be bindings for it.  Basically, what you'll do is send a cURL request to whatever page you want to 'scrape'.  The request will return a string with the source code to the page.  From there, you will use regex to parse whatever data you want from the source code.  That's generally how you are going to do it."
26,How to scrape a website which requires login using python and beautifulsoup?,"If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login. 
...",https://stackoverflow.com/questions/23102833/how-to-scrape-a-website-which-requires-login-using-python-and-beautifulsoup,". If I want to scrape a website that requires login with password first, how can I start scraping it with python using beautifulsoup4 library? Below is what I do for websites that do not require login. . You can use mechanize:. You can use selenium to log in and retrieve the page source, which you can then pass to Beautiful Soup to extract the data you want.. There is a simpler way, from my pov, that gets you there without selenium or mechanize, or other 3rd party tools, albeit it is semi-automated.. If you go for selenium, then you can do something like below:. Since Python version wasn't specified, here is my take on it for Python 3, done without any external libraries (StackOverflow). After login use BeautifulSoup as usual, or any other kind of scraping."
27,Simple jQuery selector only selects first element in Chrome..?,"I'm a bit new to jQuery so forgive me for being dense. I want to select all <td> elements on a particular page via Chrome's JS console:

$('td')
Yet when I do this, I get the following output:

...",https://stackoverflow.com/questions/14308588/simple-jquery-selector-only-selects-first-element-in-chrome,". I'm a bit new to jQuery so forgive me for being dense. I want to select all <td> elements on a particular page via Chrome's JS console:. If jQuery isn't present on the webpage, and of course no other code assigns something to $, Chrome's JS console assigns $ a shortcut to document.querySelector().. It seems jQuery is not properly included to run on your target page. I had a similar problem and solved as follows  for Google Chrome.. If jQuery is installed and if the $ symbol is shorthand for jQuery, then $('td') returns a jQuery object.  But, in the w3schools page you linked, I don't see that jQuery is even present.. Also if you are trying to do something with each of the td's you would need to use a .each() to loop through them. For example:"
28,How do you scrape AJAX pages?,Please advise how to scrape AJAX pages.,https://stackoverflow.com/questions/260540/how-do-you-scrape-ajax-pages,". Please advise how to scrape AJAX pages.. Overview:. In my opinion the simpliest solution is to use Casperjs, a framework based on the WebKit headless browser phantomjs.. If you can get at it, try examining the DOM tree. Selenium does this as a part of testing a page. It also has functions to click buttons and follow links, which may be useful.. The best way to scrape web pages using Ajax or in general pages using Javascript is with a browser itself or a headless browser (a browser without GUI). Currently phantomjs is a well promoted headless browser using WebKit. An alternative that I used with success is HtmlUnit (in Java or .NET via IKVM, which is a simulated browser. Another known alternative is using a web automation tool like Selenium.. Depends on the ajax page.  The first part of screen scraping is determining how the page works.  Is there some sort of variable you can iterate through to request all the data from the page?  Personally I've used Web Scraper Plus for a lot of screen scraping related tasks because it is cheap, not difficult to get started, non-programmers can get it working relatively quickly.. I like PhearJS, but that might be partially because I built it.. I think Brian R. Bondy's answer is useful when the source code is easy to read. I prefer an easy way using tools like Wireshark or HttpAnalyzer to capture the packet and get the url from  the ""Host"" field and the ""GET"" field.. As a low cost solution you can also try SWExplorerAutomation (SWEA).  The program creates an automation API for any Web application developed with HTML, DHTML or AJAX. . Selenium WebDriver is a good solution: you program a browser and you automate what needs to be done in the browser. Browsers (Chrome, Firefox, etc) provide their own drivers that work with Selenium. Since it works as an automated REAL browser, the pages (including javascript and Ajax) get loaded as they do with a human using that browser.. I have previously linked to MIT's solvent and EnvJS as my answers to scrape off Ajax pages. These projects seem no longer accessible."
29,How to print an exception in Python 3?,"Right now, I catch the exception in the except Exception: clause, and do print(exception). The result provides no information since it always prints <class 'Exception'>. I knew this used to work ...",https://stackoverflow.com/questions/41596810/how-to-print-an-exception-in-python-3,". Right now, I catch the exception in the except Exception: clause, and do print(exception). The result provides no information since it always prints <class 'Exception'>. I knew this used to work in python 2, but how do I do it in python3?. I'm guessing that you need to assign the Exception to a variable. As shown in the Python 3 tutorial:. These are the changes since python 2:. Try. Here is the way I like that prints out all of the error stack.. I've use this :. Although if you want a code that is compatible with both python2 and python3 you can use this:. [In Python3]"
30,Java HTML Parsing [closed],I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS ...,https://stackoverflow.com/questions/238036/java-html-parsing,". I'm working on an app which scrapes data from a website and I was wondering how I should go about getting the data.  Specifically I need data contained in a number of div tags which use a specific CSS class - Currently (for testing purposes) I'm just checking for . Several years ago I used JTidy for the same purpose:. Another library that might be useful for HTML processing is jsoup.
Jsoup tries to clean malformed HTML and allows html parsing in Java using jQuery like tag selector syntax.. The main problem as stated by preceding coments is malformed HTML, so an html cleaner or HTML-XML converter is a must. Once you get the XML code (XHTML) there are plenty of tools to handle it. You could get it with a simple SAX handler that extracts only the data you need or any tree-based method (DOM, JDOM, etc.) that let you even modify original code.. You might be interested by TagSoup, a Java HTML parser able to handle malformed HTML. XML parsers would work only on well formed XHTML.. The HTMLParser project (http://htmlparser.sourceforge.net/) might be a possibility.  It seems to be pretty decent at handling malformed HTML.  The following snippet should do what you need:. Jericho: http://jericho.htmlparser.net/docs/index.html. Let's not forget Jerry, its jQuery in java: a fast and concise Java Library that simplifies HTML document parsing, traversing and manipulating; includes usage of css3 selectors.. HTMLUnit might be of help. It does a lot more stuff too.. The nu.validator project is an excellent, high performance HTML parser that doesn't cut corners correctness-wise.. You can also use XWiki HTML Cleaner:. If your HTML is well-formed, you can easily employ an XML parser to do the job for you... If you're only reading, SAX would be ideal."
31,Web Scraping With Haskell,"What is the current state of libraries for scraping websites with Haskell?

I'm trying to make myself do more of my quick oneoff tasks in Haskell, in order to help increase my comfort level with the ...",https://stackoverflow.com/questions/4838138/web-scraping-with-haskell,". What is the current state of libraries for scraping websites with Haskell?. From my searching on the Haskell mailing lists, it appears that TagSoup is the dominant choice for parsing pages. For example:
http://www.haskell.org/pipermail/haskell-cafe/2008-August/045721.html. http://hackage.haskell.org/package/shpider. Although I'm still for now a beginner in Haskell, I have the strong opinion that HTML parsing in 2012 must be done using CSS selectors, and it seems the libraries recommended so far don't use that principle.. I wrote another answer to this question already, suggesting CSS selectors-based parsing, however that answer is now a year and a half old, and nowadays I think lenses might be a better approach in haskell. In effect you get something like type-safe compiled selectors."
32,"Selenium-Debugging: Element is not clickable at point (X,Y)","I try to scrape this site by Selenium.

I want to click in ""Next Page"" buttom, for this I do:

 driver.find_element_by_class_name('pagination-r').click()
it works for many pages but not for all, I ...",https://stackoverflow.com/questions/37879010/selenium-debugging-element-is-not-clickable-at-point-x-y,". I try to scrape this site by Selenium.. Another element is covering the element you are trying to click. You could use execute_script() to click on this.. I had a similar issue where using ActionChains was not solving my error:
WebDriverException: Message: unknown error: Element is not clickable at point (5
74, 892). I have written logic to handle these type of exception . . Use explicit wait instead of implicit.. If you are receiving an element not clickable error, even after using wait on the element, try one of these workarounds:"
33,Using BeautifulSoup to extract text without tags,"My webpage looks like this:

<p>
  <strong class=""offender"">YOB:</strong> 1987<br/>
  <strong class=""offender"">RACE:</strong> WHITE<br/>
  <strong class=""...",https://stackoverflow.com/questions/23380171/using-beautifulsoup-to-extract-text-without-tags,. My webpage looks like this:. Just loop through all the <strong> tags and use next_sibling to get what you want. Like this:. I think you can get it using subc1.text.. you can try this indside findall for loop:. I think you could solve this with .strip() in gazpacho:
34,Converting html to text with Python,"I am trying to convert an html block to text using Python. 

Input:

<div class=""body""><p><strong></strong></p>
<p><strong></strong>Lorem ipsum dolor ...",https://stackoverflow.com/questions/14694482/converting-html-to-text-with-python,". I am trying to convert an html block to text using Python. . What am I missing? soup.get_text() gives exactly the same output you wanted.... It's possible using python standard html.parser:. You can use regular expression... but not recommended.... The '\n' places a newline between the paragraphs.. I liked @FrBrGeorge's no dependency answer so much that I expanded it to only extract the body tag and added a convenience method so that HTML to text is a single line:. I was in need of a way of doing this on a client's system without having to download additional libraries. I never found a good solution, so I created my own. Feel free to use this if you like.. It's possible to use BeautifulSoup to remove unwanted scripts and similar, though you may need to experiment with a few different sites to make sure you've covered the different types of things you wish to exclude.  Try this:. There are some nice things here, and i might as well throw in my solution:. gazpacho might be a good choice for this!"
35,Web scraping - how to identify main content on a webpage,"Given a news article webpage (from any major news source such as times or bloomberg), I want to  identify the main article content on that page and throw out the other misc elements such as ads, menus,...",https://stackoverflow.com/questions/4672060/web-scraping-how-to-identify-main-content-on-a-webpage,". Given a news article webpage (from any major news source such as times or bloomberg), I want to  identify the main article content on that page and throw out the other misc elements such as ads, menus, sidebars, user comments. . There's no way to do this that's guaranteed to work, but one strategy you might use is to try to find the element with the most visible text inside of it.. There are a number of ways to do it, but, none will always work.  Here are the two easiest:. A while ago I wrote a simple Python script for just this task. It uses a heuristic to group text blocks together based on their depth in the DOM. The group with the most text is then assumed to be the main content. It's not perfect, but works generally well for news sites, where the article is generally the biggest grouping of text, even if broken up into multiple div/p tags.. Diffbot offers a free(10.000 urls) API to do that, don't know if that approach is what you are looking for, but it might help someone http://www.diffbot.com/. For a solution in Java have a look at https://code.google.com/p/boilerpipe/ :. It might be more useful to extract the RSS feeds (<link type=""application/rss+xml"" href=""...""/>) on that page and parse the data in the feed to get the main content.. Check the following script. It is really amazing:. Another possibility of separating ""real"" content from noise is by measuring HTML density of the parts of a HTML page.. There is a recent (early 2020) comparison of various methods of extracting article body, without and ads, menus, sidebars, user comments, etc. - see https://github.com/scrapinghub/article-extraction-benchmark. A report, data and evaluation scripts are available. It compares many options mentioned in the answers here, as well as some options which were not mentioned:. I wouldn't try to scrape it from the web page - too many things could mess it up - but instead see which web sites publish RSS feeds. For example, the Guardian's RSS feed has most of the text from their leading articles:"
36,How to manage log in session through headless chrome?,"I want to create a scraper that:

opens a headless browser,
goes to a url,
logs in (there is steam oauth),
fills some inputs,
and clicks 2 buttons.

My problem is that every new instance of headless ...",https://stackoverflow.com/questions/48608971/how-to-manage-log-in-session-through-headless-chrome,. I want to create a scraper that:. In puppeter you have access to the session cookies through page.cookies().. There is an option to save user data using the userDataDir option when launching puppeteer. This stores the session and other things related to launching chrome. . For a version of the above solution that actually works and doesn't rely on jsonfile (instead using the more standard fs) check this out:. For writing Cookies
37,csv.writer writing each character of word in separate column/cell,"Objective: To extract the text from the anchor tag inside all lines in models and put it in a csv.

I'm trying this code: 

with open('Sprint_data.csv', 'ab') as csvfile:
  spamwriter = csv.writer(...",https://stackoverflow.com/questions/15129567/csv-writer-writing-each-character-of-word-in-separate-column-cell,". Objective: To extract the text from the anchor tag inside all lines in models and put it in a csv.. writerow accepts a sequence.  You're giving it a single string, so it's treating that as a sequence, and strings act like sequences of characters.. .writerow() requires a sequence ('', (), []) and places each index in it's own column of the row, sequentially.  If your desired string is not an item in a sequence, writerow() will iterate over each letter in your string and each will be written to your CSV in a separate cell.. This is usually the solution I use:    . Just surround it with a list sign (i.e [])"
38,Using python Requests with javascript pages,"I am trying to use the Requests framework with python (http://docs.python-requests.org/en/latest/) but the page I am trying to get to uses javascript to fetch the info that I want. 

I have tried to ...",https://stackoverflow.com/questions/26393231/using-python-requests-with-javascript-pages,". I am trying to use the Requests framework with python (http://docs.python-requests.org/en/latest/) but the page I am trying to get to uses javascript to fetch the info that I want. . You are going to have to make the same request (using the Requests library) that the javascript is making.  You can use any number of tools (including those built into Chrome and Firefox) to inspect the http request that is coming from javascript and simply make this request yourself from Python.. Good news: there is now a requests module that supports javascript:  https://pypi.org/project/requests-html/. While Selenium might seem tempting and useful, it has one main problem that can't be fixed: performance. By calculating every single thing a browser does, you will need a lot more power. Even PhantomJS does not compete with a simple request. I recommend that you will only use Selenium when you really need to click buttons. If you only need javascript, I recommend PyQt (check https://www.youtube.com/watch?v=FSH77vnOGqU to learn it).. its a wrapper around pyppeteer or smth? :( i thought its something different"
39,Click a Button in Scrapy,"I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).

I found out that Scrapy ...",https://stackoverflow.com/questions/6682503/click-a-button-in-scrapy,. I'm using Scrapy to crawl a webpage. Some of the information I need only pops up when you click on a certain button (of course also appears in the HTML code after clicking).. Scrapy cannot interpret javascript.. Selenium browser provide very nice solution. Here is an example (pip install -U selenium):. To properly and fully use JavaScript you need a full browser engine and this is possible only with Watir/WatiN/Selenium etc.
40,Scrape An Entire Website [closed],"I'm looking for recommendations for a program to scrape and download an entire corporate website.

The site is powered by a CMS that has stopped working and getting it fixed is expensive and we are ...",https://stackoverflow.com/questions/9265172/scrape-an-entire-website,". 
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                . Consider HTTrack. It's a free and easy-to-use offline browser utility. . Read more about it here. . None of the above got exactly what I needed (the whole site and all assets). This worked though. . I know this is super old and I just wanted to put my 2 cents in. . The best way is to scrape it with wget as suggested in @Abhijeet Rastogi's answer.  If you aren't familiar with is then Blackwidow is a decent scraper.  I've used it in the past.  http://www.sbl.net/"
41,How to scroll down with Phantomjs to load dynamic content,I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able ...,https://stackoverflow.com/questions/16561582/how-to-scroll-down-with-phantomjs-to-load-dynamic-content,". I am trying to scrape links from a page that generates content dynamically as the user scroll down to the bottom (infinite scrolling). I have tried doing different things with Phantomjs but not able to gather links beyond first page. Let say the element at the bottom which loads content has class .has-more-items. It is available until final content is loaded while scrolling and then becomes unavailable in DOM (display:none). Here are the things I have tried-. Found a way to do it and tried to adapt to your situation. I didn't test the best way of finding the bottom of the page because I had a different context, but check the solution below. The thing here is that you have to wait a little for the page to load and javascript works asynchronously so you have to use setInterval or setTimeout (see) to achieve this.. I know that it has been answered a long time ago, but I also found a solution to my specific scenario. The result is a piece of javascript that scrolls to the bottom of the page. It is optimized to reduce waiting time.. The ""correct"" solution didn't work for me. And, from what I've read CasperJS doesn't use window (but I may be wrong on that), which makes me doubt that window works.. The code snippet below work just fine for pinterest. I researched a lot to scrape pinterest without phantomjs but it is impossible to find the infinite scroll trigger link. I think the code below will help other infinite scroll web page to scrape."
42,How to get the scrapy failure URLs?,"I'm a newbie of scrapy and it's amazing crawler framework i have known! 

In my project, I sent more than 90, 000 requests, but there are some of them failed. 
I set the log level to be INFO, and i ...",https://stackoverflow.com/questions/13724730/how-to-get-the-scrapy-failure-urls,". I'm a newbie of scrapy and it's amazing crawler framework i have known! . Yes, this is possible. . Here's another example how to handle and collect 404 errors (checking github help pages):. Scrapy ignores 404 by default and does not parse it. If you are getting an error code 404 in response, you can handle this with a very easy way.. The answers from @Talvalin and @alecxe helped me a great deal, but they do not seem to capture downloader events that do not generate a response object (for instance, twisted.internet.error.TimeoutError and twisted.web.http.PotentialDataLoss). These errors show up in the stats dump at the end of the run, but without any meta info. . As of scrapy 0.24.6, the method suggested by alecxe won't catch errors with the start URLs. To record errors with the start URLs you need to override parse_start_urls. Adapting alexce's answer for this purpose, you'd get:. This is an update on this question. I ran in to a similar problem and needed to use the scrapy signals to call a function in my pipeline. I have edited @Talvalin's code, but wanted to make an answer just for some more clarity. . In addition to some of these answers, if you want to track Twisted errors, I would take a look at using the Request object's errback parameter, on which you can set a callback function to be called with the Twisted Failure on a request failure.
In addition to the url, this method can allow you to track the type of failure.. You can capture failed urls in two ways.. Basically Scrapy Ignores 404 Error by Default, It was defined in httperror middleware."
43,Python: Disable images in Selenium Google ChromeDriver,"I spend a lot of time searching about this.
At the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier ...",https://stackoverflow.com/questions/28070315/python-disable-images-in-selenium-google-chromedriver,". I spend a lot of time searching about this.
At the end of the day I combined a number of answers and it works. I share my answer and I'll appreciate it if anyone edits it or provides us with an easier way to do this.. Here is another way to disable images:. Java:
With this Chrome nor Firefox would load images. The syntax is different but the strings on the parameters are the same.. There is another way that comes probably to mind to everyone to access chrome://settings and then go through the settings with selenium I started this way just for didactic curiosity, but then I hit a forest of shadow-roots elements now when you encounter more than 3 shadow root element combined with dynamic content is clearly a way to obfuscate and make it impossible to automate, although might sound at least theoretically possible this approach looks more like a dead end, I will leave this answer with the example code, just for purely learning purposes to advert the people tempted to go to the challenge..  Not only was hard to find just the content settings due to the shadowroots and dynamic change when you find the button is not clickable at this point.  "
44,Get meta tag content property with BeautifulSoup and Python,"I am trying to use python and beautiful soup to extract the content part of the tags below:

<meta property=""og:title"" content=""Super Fun Event 1"" />
<meta property=""og:url"" content=""http://...",https://stackoverflow.com/questions/36768068/get-meta-tag-content-property-with-beautifulsoup-and-python,". I am trying to use python and beautiful soup to extract the content part of the tags below:. Provide the meta tag name as the first argument to find(). Then, use keyword arguments to check the specific attributes:. try this :. A way I like to solve this is as follows:
(Is neater when using with lists of properties to look up...). You could grab the content inside the meta tag with gazpacho:"
45,How can I download a file on a click event using selenium?,"I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.  

from selenium import webdriver
from selenium.common.exceptions import ...",https://stackoverflow.com/questions/18439851/how-can-i-download-a-file-on-a-click-event-using-selenium,". I am working on python and selenium. I want to download file from clicking event using selenium. I wrote following code.  . Find the link using find_element(s)_by_*, then call click method.. I'll admit this solution is a little more ""hacky"" than the Firefox Profile saveToDisk alternative, but it works across both Chrome and Firefox, and doesn't rely on a browser-specific feature which could change at any time. And if nothing else, maybe this will give someone a little different perspective on how to solve future challenges.. In chrome what I do is downloading the files by clicking on the links, then I open chrome://downloads page and then retrieve the downloaded files list from shadow DOM like this:. Here is the full working code. You can use web scrapping to enter the username password and other field. For getting the field names appearing on the webpage, use inspect element. Element name(Username,Password or Click Button) can be entered through class or name."
46,Scraping a JSON response with Scrapy,"How do you use Scrapy to scrape web requests that return JSON? For example, the JSON would look like this:

{
    ""firstName"": ""John"",
    ""lastName"": ""Smith"",
    ""age"": 25,
    ""address"": {
        ""...",https://stackoverflow.com/questions/18171835/scraping-a-json-response-with-scrapy,". How do you use Scrapy to scrape web requests that return JSON? For example, the JSON would look like this:. It's the same as using Scrapy's HtmlXPathSelector for html responses. The only difference is that you should use json module to parse the response:. The possible reason JSON is not loading is that it has single-quotes before and after. Try this:"
47,Options for HTML scraping? [closed],"I'm thinking of trying Beautiful Soup, a Python package for HTML scraping. Are there any other HTML scraping packages I should be looking at? Python is not a requirement, I'm actually interested in ...",https://stackoverflow.com/questions/2861/options-for-html-scraping,". 
Want to improve this question? Update the question so it focuses on one problem only by editing this post.
                . The Ruby world's equivalent to Beautiful Soup is why_the_lucky_stiff's Hpricot.. In the .NET world, I recommend the HTML Agility Pack. Not near as simple as some of the above options (like HTMLSQL), but it's very flexible. It lets you maniuplate poorly formed HTML as if it were well formed XML, so you can use XPATH or just itereate over nodes.. BeautifulSoup is a great way to go for HTML scraping. My previous job had me doing a lot of scraping and I wish I knew about BeautifulSoup when I started. It's like the DOM with a lot more useful options and is a lot more pythonic. If you want to try Ruby they ported BeautifulSoup calling it RubyfulSoup but it hasn't been updated in a while.. I found HTMLSQL to be a ridiculously simple way to screenscrape. It takes literally minutes to get results with it.. The Python lxml library acts as a Pythonic binding for the libxml2 and libxslt libraries. I like particularly its XPath support and pretty-printing of the in-memory XML structure. It also supports parsing broken HTML. And I don't think you can find other Python libraries/bindings that parse XML faster than lxml.. For Perl, there's WWW::Mechanize.. Python has several options for HTML scraping in addition to Beatiful Soup. Here are some others:. 'Simple HTML DOM Parser' is a good option for PHP, if your familiar with jQuery or JavaScript selectors then you will find yourself at home.. Why has no one mentioned JSOUP yet for Java? http://jsoup.org/. The templatemaker utility from Adrian Holovaty (of Django fame) uses a very interesting approach: You feed it variations of the same page and it ""learns"" where the ""holes"" for variable data are. It's not HTML specific, so it would be good for scraping any other plaintext content as well. I've used it also for PDFs and HTML converted to plaintext (with pdftotext and lynx, respectively).. I know and love Screen-Scraper.. I would first find out if the site(s) in question provide an API server or RSS Feeds for access the data you require.. Scraping Stack Overflow is especially easy with Shoes and Hpricot.. Another option for Perl would be Web::Scraper which is based on Ruby's Scrapi. In a nutshell, with nice and concise syntax, you can get a robust scraper directly into data structures.. I've had some success with HtmlUnit, in Java. It's a simple framework for writing unit tests on web UI's, but equally useful for HTML scraping.. Yahoo! Query Language or YQL can be used alongwith jQuery, AJAX, JSONP to screen scrape web pages. Another tool for .NET is MhtBuilder. There is this solution too: netty HttpClient. I use Hpricot on Ruby. As an example this is a snippet of code that I use to retrieve all book titles from the six pages of my HireThings account (as they don't seem to provide a single page with this information):. I've used Beautiful Soup a lot with Python. It is much better than regular expression checking, because it works like using the DOM, even if the HTML is poorly formatted. You can quickly find HTML tags and text with simpler syntax than regular expressions. Once you find an element, you can iterate over it and its children, which is more useful for understanding the contents in code than it is with regular expressions. I wish Beautiful Soup existed years ago when I had to do a lot of screenscraping -- it would have saved me a lot of time and headache since HTML structure was so poor before people started validating it.. Although it was designed for .NET web-testing, I've been using the WatiN framework for this purpose. Since it is DOM-based, it is pretty easy to capture HTML, text, or images. Recentely, I used it to dump a list of links from a MediaWiki All Pages namespace query into an Excel spreadsheet. The following VB.NET code fragement is pretty crude, but it works.. Implementations of the HTML5 parsing algorithm: html5lib (Python, Ruby), Validator.nu HTML Parser (Java, JavaScript; C++ in development), Hubbub (C), Twintsam (C#; upcoming).. You would be a fool not to use Perl.. Here come the flames.. . I have used LWP and HTML::TreeBuilder with Perl and have found them very useful.. In Java, you can use TagSoup.. Well, if you want it done from the client side using only a browser you have jcrawl.com. After having designed your scrapping service from the web application (http://www.jcrawl.com/app.html), you only need to add the generated script to an HTML page to start using/presenting your data.. You probably have as much already, but I think this is what you are trying to do:. I've had mixed results in .NET using SgmlReader which was originally started by Chris Lovett and appears to have been updated by MindTouch.. I like Google Spreadsheets' ImportXML(URL, XPath) function.. I've also had great success using Aptana's Jaxer + jQuery to parse pages. It's not as fast or 'script-like' in nature, but jQuery selectors + real JavaScript/DOM is a lifesaver on more complicated (or malformed) pages."
48,Headless Browser and scraping - solutions [closed],"I'm trying to put list of possible solutions for browser automatic tests suits and headless browser platforms capable of scraping.
BROWSER TESTING / SCRAPING:
Selenium - polyglot flagship in browser ...",https://stackoverflow.com/questions/18539491/headless-browser-and-scraping-solutions,". 
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                . If Ruby is your thing, you may also try:. http://triflejs.org/ is like phantomjs but based on IE. A kind of JS-based Selenium is Dalek.js. It not only aims for automated frontend-tests, you can also do screenshots with it. It has webdrivers for all important browsers. Unfortunately those webdrivers seem to be worth improving (just not to say ""buggy"" to Firefox)."
49,How can I get the Google cache age of any URL or web page? [closed],"In my project I need the Google cache age to be added as important information. I tried to search sources for the Google cache age, that is, the number of days since Google last re-indexed the page ...",https://stackoverflow.com/questions/4560400/how-can-i-get-the-google-cache-age-of-any-url-or-web-page,". 
Want to improve this question? Update the question so it's on-topic for Stack Overflow.
                . Use the URL. You'll need to scrape the resulting page, but you can view the most recent cache page using this URL:. you can Use CachedPages website   . its too simple, you can just type ""cache:"" before the URL of the page. for example
if you want to check the last webcache of this page simply type on URL bar cache:http://stackoverflow.com/questions/4560400/how-can-i-get-the-google-cache-age-of-any-url-or-web-page. You can use this site: https://cachedviews.com/ . Cache View or Cached Pages of Any Website - Google Cached Pages of Any Website. This one good also to view cachepage http://www.cachepage.net"
50,Which HTML Parser is the best? [closed],"I code a lot of parsers. Up until now, I was using HtmlUnit headless browser for parsing and browser automation.

Now, I want to separate both the tasks.

As 80% of my work involves just parsing, I ...",https://stackoverflow.com/questions/2168610/which-html-parser-is-the-best,". I code a lot of parsers. Up until now, I was using HtmlUnit headless browser for parsing and browser automation.. Self plug: I have just released a new Java HTML parser: jsoup. I mention it here because I think it will do what you are after.. The best I've seen so far is HtmlCleaner:. I suggest Validator.nu's parser, based on the HTML5 parsing algorithm. It is the parser used in Mozilla from 2010-05-03"
