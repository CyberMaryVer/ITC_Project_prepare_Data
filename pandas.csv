,question,summary,link,answers
0,,,,
1,How to select rows from a DataFrame based on column values,"How can I select rows from a DataFrame based on values in some column in Pandas?
In SQL, I would use:
SELECT *
FROM table
WHERE colume_name = some_value

I tried to look at Pandas' documentation, but ...",https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values,"['How can I select rows from a DataFrame based on values in some column in Pandas?', 'To select rows whose column value equals a scalar, some_value, use ==:', 'There are several ways to select rows from a Pandas data frame:', 'The Pandas equivalent to', ""I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the query() method in v0.13 and I much prefer it. For your question, you could do df.query('col == val')"", 'August 2019 updated answer', 'Faster results can be achieved using numpy.where.', 'Here is a simple example', 'For selecting only specific columns out of multiple columns for a given value in Pandas:', ""To append to this famous question (though a bit too late): You can also do df.groupby('column_name').get_group('column_desired_value').reset_index() to make a new data frame with specified column having a particular value. E.g."", 'You can also use .apply:']"
2,Renaming columns in pandas,"I have a DataFrame using pandas and column labels that I need to edit to replace the original column labels. 

I'd like to change the column names in a DataFrame A where the original column names are:
...",https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas,"['I have a DataFrame using pandas and column labels that I need to edit to replace the original column labels.', 'Just assign it to the .columns attribute:', 'Use the df.rename() function and refer the columns to be renamed. Not all the columns have to be renamed:', 'The rename method can take a function, for example:', 'As documented in Working with text data:', 'There have been some significant updates to column renaming in version 0.21.', 'Since you only want to remove the $ sign in all column names, you could just do:', 'It will replace the existing names with the names you provide, in the order you provide.', 'This way you can manually edit the new_names as you wish.\nWorks great when you need to rename only a few columns to correct mispellings, accents, remove special characters etc.', 'I would like to explain a bit what happens behind the scenes.', ""I'll focus on two things:"", 'Renaming columns in pandas is an easy task.', ""Let's say this is your dataframe."", ""If you've got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns..."", 'If your new list of columns is in the same order as the existing columns, the assignment is simple:', ""Let's Understand renaming by a small example..."", 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html', ""Another way we could replace the original column labels is by stripping the unwanted characters (here '$') from the original column labels."", 'Real simple just use', 'You could use str.slice for that:', 'I know this question and answer has been chewed to death. But I referred to it for inspiration for one of the problem I was having . I was able to solve it using bits and pieces from different answers hence providing my response in case anyone needs it.', 'Another option is to rename using a regular expression:', 'Note that these approach do not work for a MultiIndex. For a MultiIndex, you need to do something like the following:', 'If you have to deal with loads of columns named by the providing system out of your control, I came up with the following approach that is a combination of a general approach and specific replacments in one go.', 'In addition to the solution already provided, you can replace all the columns while you are reading the file. We can use names and header=0 to do that.', ""Here's a nifty little function I like to use to cut down on typing:"", 'Assuming you can use regular expression. This solution removes the need of manual encoding using regex', ""I needed to rename features for XGBoost, it didn't like any of these:""]"
3,Delete column from pandas DataFrame,"When deleting a column in a DataFrame I use:

del df['column_name']
And this works great. Why can't I use the following?

del df.column_name
Since it is possible to access the column/Series as df....",https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe,"['When deleting a column in a DataFrame I use:', ""As you've guessed, the right syntax is"", 'The best way to do this in pandas is to use drop:', 'Use:', 'Delete first, second and fourth columns:', 'The actual question posed, missed by most answers here is:', 'A nice addition is the ability to drop columns only if they exist. This way you can cover more use cases, and it will only drop the existing columns from the labels passed to it:', 'from version 0.16.1 you can do', ""It's good practice to always use the [] notation. One reason is that attribute notation (df.column_name) does not work for numbered indices:"", 'Pandas version 0.21 has changed the drop method slightly to include both the index and columns parameters to match the signature of the rename and reindex methods.', 'In pandas 0.16.1+ you can drop columns only if they exist per the solution posted by @eiTanLaVi.  Prior to that version, you can achieve the same result via a conditional list comprehension:', ""A lot of effort to find a marginally more efficient solution.  Difficult to justify the added complexity while sacrificing the simplicity of df.drop(dlst, 1, errors='ignore')"", 'We can Remove or Delete a specified column or sprcified columns by drop() method.', ""If your original dataframe df is not too big, you have no memory constraints, and you only need to keep a few columns, or, if you don't know beforehand the names of all the extra columns that you do not need, then you might as well create a new dataframe with only the columns you need:"", 'The dot syntax works in JavaScript, but not in Python.', 'or else you can go with', 'Another way of Deleting a Column in Pandas DataFrame']"
4,Selecting multiple columns in a pandas dataframe,"I have data in different columns but I don't know how to extract it to save it in another variable.

index  a   b   c
1      2   3   4
2      3   4   5
How do I select 'a', 'b' and save it in to df1?
...",https://stackoverflow.com/questions/11285613/selecting-multiple-columns-in-a-pandas-dataframe,"[""I have data in different columns but I don't know how to extract it to save it in another variable."", 'The column names (which are strings) cannot be sliced in the manner you tried.', 'As of version 0.11.0, columns can be sliced in the manner you tried using the .loc indexer:', ""Assuming your column names (df.columns) are ['index','a','b','c'], then the data you want is in the \n3rd & 4th columns. If you don't know their names when your script runs, you can do this"", 'I realize this question is quite old, but in the latest version of pandas there is an easy way to do exactly this. Column names (which are strings) can be sliced in whatever manner you like.', 'With pandas,', 'You could provide a list of columns to be dropped and return back the DataFrame with only the columns needed using the drop() function on a Pandas DataFrame.', 'I found this method to be very useful:', 'Starting with 0.21.0, using .loc or [] with a list with one or more missing labels is deprecated in favor of .reindex. So, the answer to your question is:', 'You can use pandas.\nI create the DataFrame:', 'You can use pandas.DataFrame.filter method to either filter or reorder columns like this:', ""If you want to get one element by row index and column name, you can do it just like df['b'][0]. It is as simple as you can image."", 'One different and easy approach : iterating rows', ""The different approaches discussed in above responses are based on the assumption that either the user knows column indices to drop or subset on, or the user wishes to subset a dataframe using a range of columns (for instance between 'C' : 'E'). pandas.DataFrame.drop() is certainly an option to subset data based on a list of columns defined by user (though you have to be cautious that you always use copy of dataframe and inplace parameters should not be set to True!!)"", 'you can also use df.pop()', ""I've seen several answers on that, but on remained unclear to me. How would you select those columns of interest? The answer to that is that if you have them gathered in a list, you can just reference the columns using the list."", 'Try to use pandas.DataFrame.get (see docs):', 'To select multiple columns, extract and view them thereafter: df is previously named data frame, than create new data frame df1, and select the columns A to D which you want to extract and view.']"
5,How do I get the row count of a pandas DataFrame?,"I'm trying to get the number of rows of dataframe df with Pandas, and here is my code.

Method 1:

total_rows = df.count
print total_rows +1
Method 2:

total_rows = df['First_columnn_label'].count
...",https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe,"[""I'm trying to get the number of rows of dataframe df with Pandas, and here is my code."", 'You can use the .shape property or just len(DataFrame.index). However, there are notable performance differences ( len(DataFrame.index) is fastest).', 'Suppose df is your dataframe then:', 'Use len(df).', ""This table summarises the different situations in which you'd want to count something in a DataFrame (or Series, for completeness), along with the recommended method(s)."", 'Short, clear and clean:  use len(df)', 'Apart from above answers use can use df.axes to get the tuple with row and column indexes and then use len() function:', ""...building on Jan-Philip Gehrcke's answer."", 'I come to pandas from R background, and I see that pandas is more complicated when it comes to selecting row or column.\nI had to wrestle with it for a while, then I found some ways to deal with:', 'Hey you can use do this also:', 'In case you want to get the row count in the middle of a chained operation, you can use:', 'For dataframe df, a printed comma formatted row count used while exploring data:', 'Either of this can do (df is the name of the DataFrame):', 'An alternative method to finding out the amount of rows in a dataframe which I think is the most readable variant is pandas.Index.size.', ""I'm not sure if this would work(data COULD be omitted), but this may work:""]"
6,Get list from pandas DataFrame column headers,"I want to get a list of the column headers from a pandas DataFrame.  The DataFrame will come from user input so I won't know how many columns there will be or what they will be called.
For example, if ...",https://stackoverflow.com/questions/19482970/get-list-from-pandas-dataframe-column-headers,"[""I want to get a list of the column headers from a pandas DataFrame.  The DataFrame will come from user input so I won't know how many columns there will be or what they will be called."", 'You can get the values as a list by doing:', 'There is a built in method which is the most performant:', 'Did some quick tests, and perhaps unsurprisingly the built-in version using dataframe.columns.values.tolist() is the fastest:', 'Its gets even simpler (by pandas 0.16.0) :', 'To list the columns of a dataframe while in debugger mode, use a list comprehension:', ""Surprised I haven't seen this posted so far, so I'll just leave this here."", ""That's available as my_dataframe.columns."", 'A DataFrame follows the dict-like convention of iterating over the “keys” of the objects.', ""It's interesting but df.columns.values.tolist() is almost 3 times faster then df.columns.tolist() but I thought that they are the same:"", 'For data exploration in the IPython notebook, my preferred way is this:', 'as answered by Simeon Visser...you could do', 'For a quick, neat, visual check, try this:', 'This gives us the names of columns in a list:', 'I feel question deserves additional explanation.', 'Even though the solution that was provided above is nice. I would also expect something like frame.column_names() to be a function in pandas, but since it is not, maybe it would be nice to use the following syntax. It somehow preserves the feeling that you are using pandas in a proper way by calling the ""tolist"" function: frame.columns.tolist()', 'If the DataFrame happens to have an Index or MultiIndex and you want those included as column names too:', 'This solution lists all the columns of your object my_dataframe:']"
7,Adding new column to existing DataFrame in Python pandas,"I have the following indexed DataFrame with named columns and rows not- continuous numbers:

          a         b         c         d
2  0.671399  0.101208 -0.181532  0.241273
3  0.446172 -0.243316  ...",https://stackoverflow.com/questions/12555323/adding-new-column-to-existing-dataframe-in-python-pandas,"['I have the following indexed DataFrame with named columns and rows not- continuous numbers:', 'Use the original df1 indexes to create the series:', ""This is the simple way of adding a new column: df['e'] = e"", ""I would like to add a new column, 'e', to the existing data frame and do not change anything in the data frame. (The series always got the same length as a dataframe.)"", 'A pandas dataframe is implemented as an ordered dict of columns.', 'It seems that in recent Pandas versions the way to go is to use df.assign:', 'Doing this directly via NumPy will be the most efficient:', 'Easiest ways:-', ""If you want to set the whole new column to an initial base value (e.g. None), you can do this: df1['e'] = None"", ""I got the dreaded SettingWithCopyWarning, and it wasn't fixed by using the iloc syntax. My DataFrame was created by read_sql from an ODBC source. Using a suggestion by lowtech above, the following worked for me:"", 'If the column you are trying to add is a series variable then just :', 'If the data frame and Series object have the same index, pandas.concat also works here:', 'Foolproof:', 'To create an empty column', 'Let me just add that, just like for hum3, .loc didn\'t solve the SettingWithCopyWarning and I had to resort to df.insert(). In my case false positive was generated by ""fake"" chain indexing  dict[\'a\'][\'e\'], where \'e\' is the new column, and dict[\'a\'] is a DataFrame coming from dictionary.', 'to insert a new column at a given location (0 <= loc <= amount of columns) in a data frame, just use Dataframe.insert:', 'Before assigning a new column, if you have indexed data, you need to sort the index. At least in my case I had to:', 'One thing to note, though, is that if you do', 'I was looking for a general way of adding a column of numpy.nans to a dataframe without getting the dumb SettingWithCopyWarning.', ""To add a new column, 'e', to the existing data frame"", 'For the sake of completeness - yet another solution using DataFrame.eval() method:', ""The following is what I did... But I'm pretty new to pandas and really Python in general, so no promises."", 'If you get the SettingWithCopyWarning, an easy fix is to copy the DataFrame you are trying to add a column to.', 'this is a special case of adding a new column to a pandas dataframe. Here, I am adding a new feature/column based on an existing column data of the dataframe.', '']"
8,“Large data” work flows using pandas,"I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it's out-of-core support.  However, SAS is horrible ...",https://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas,"[""I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it's out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons."", 'I routinely use tens of gigabytes of data in just this fashion\ne.g. I have tables on disk that I read via queries, create data and append back.', ""I think the answers above are missing a simple approach that I've found very useful."", ""There is now, two years after the question, an 'out-of-core' pandas equivalent: dask. It is excellent! Though it does not support all of pandas functionality, you can get really far with it."", ""If your datasets are between 1 and 20GB, you should get a workstation with 48GB of RAM. Then Pandas can hold the entire dataset in RAM. I know its not the answer you're looking for here, but doing scientific computing on a notebook with 4GB of RAM isn't reasonable."", ""I know this is an old thread but I think the Blaze library is worth checking out.  It's built for these types of situations."", 'This is the case for pymongo.  I have also prototyped using sql server, sqlite, HDF, ORM (SQLAlchemy) in python.  First and foremost pymongo is a document based DB, so each person would be a document (dict of attributes).  Many people form a collection and you can have many collections (people, stock market, income).', 'I spotted this a little late, but I work with a similar problem (mortgage prepayment models). My solution has been to skip the pandas HDFStore layer and use straight pytables. I save each column as an individual HDF5 array in my final file.', ""One trick I found helpful for large data use cases is to reduce the volume of the data by reducing float precision to 32-bit. It's not applicable in all cases, but in many applications 64-bit precision is overkill and the 2x memory savings are worth it. To make an obvious point even more obvious:"", ""As noted by others, after some years an 'out-of-core' pandas equivalent has emerged: dask. Though dask is not a drop-in replacement of pandas and all of its functionality it stands out for several reasons:"", 'One more variation', ""It is worth mentioning here Ray as well,\nit's a distributed computation framework, that has it's own implementation for pandas in a distributed way."", 'Consider Ruffus if you go the simple path of creating a data pipeline which is broken down into multiple smaller files.', 'I recently came across a similar issue. I found simply reading the data in chunks and appending it as I write it in chunks to the same csv works well. My problem was adding a date column based on information in another table, using the value of certain columns as follows. This may help those confused by dask and hdf5 but more familiar with pandas like myself.', ""I'd like to point out the Vaex package."", 'At the moment I am working ""like"" you, just on a lower scale, which is why I don\'t have a PoC for my suggestion.', 'Why Pandas ? Have you tried Standard Python ?']"
9,How to change the order of DataFrame columns?,"I have the following DataFrame (df):

import numpy as np
import pandas as pd

df = pd.DataFrame(np.random.rand(10, 5))
I add more column(s) by assignment:

df['mean'] = df.mean(1)
How can I move the ...",https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns,"['I have the following DataFrame (df):', 'One easy way would be to reassign the dataframe with a list of the columns, rearranged as needed.', 'You could also do something like this:', 'Just assign the column names in the order you want them:', 'How about:', 'In your case,', 'You need to create a new list of your columns in the desired order, then use df = df[cols] to rearrange the columns in this new order.', 'You can try out the following solutions :', 'If your column names are too long to type then you could specify the new order through a list of integers with the positions:', ""The Most simple way\nSuppose you have df with columns A B C,\nyou can just df.reindex(['B','C','A'],axis=1)"", 'I ran into a similar question myself, and just wanted to add what I settled on. I liked the reindex_axis() method for changing column order. This worked:', 'This function avoids you having to list out every variable in your dataset just to order a few of them.', 'Simply do,', ""You could do the following (borrowing parts from Aman's answer):"", 'I think this is a slightly neater solution:', 'Just type the column name you want to change, and set the index for the new location.', 'Moving any column to any position:', ""Here's a way to move one existing column that will modify the existing data frame in place."", 'A pretty straightforward solution that worked for me is to use .reindex on df.columns:', 'You can use a set which is an unordered collection of unique elements to do keep the ""order of the other columns untouched"":', 'You can use reindex which can be used for both axis:', 'This question has been answered before but reindex_axis is deprecated now so I would suggest to use:', 'Just flipping helps often.', ""You can do that after you added the 'n' column into your df as follows."", 'How about using ""T""?', '@clocker: Your solution was very helpful for me, as I wanted to bring two columns in front from a dataframe where I do not know exactly the names of all columns, because they are generated from a pivot statement before.\nSo, if you are in the same situation: To bring columns in front that you know the name of and then let them follow by ""all the other columns"", I came up with the following general solution;', 'set():', ""I liked Shoresh's answer to use set functionality to remove columns when you don't know the location, however this didn't work for my purpose as I need to keep the original column order (which has arbitrary column labels)."", 'Here is a function to do this for any number of columns.', 'Hackiest method in the book', ""I believe @Aman's answer is the best if you know the location of the other column.""]"
10,Add one row to pandas DataFrame,"I understand that pandas is designed to load fully populated DataFrame but I need to create an empty DataFrame then add rows, one by one.
What is the best way to do this ?

I successfully created an ...",https://stackoverflow.com/questions/10715965/add-one-row-to-pandas-dataframe,"['I understand that pandas is designed to load fully populated DataFrame but I need to create an empty DataFrame then add rows, one by one.\nWhat is the best way to do this ?', 'You can use df.loc[i], where the row with index i will be what you specify it to be in the dataframe.', 'In case you can get all data for the data frame upfront, there is a much faster approach than appending to a data frame:', 'You could use pandas.concat() or DataFrame.append(). For details and examples, see Merge, join, and concatenate.', ""It's been a long time, but I faced the same problem too. And found here a lot of interesting answers. So I was confused what method to use."", 'If you know the number of entries ex ante, you should preallocate the space by also providing the index (taking the data example from a different answer):', 'For efficient appending see How to add an extra row to a pandas dataframe and Setting With Enlargement.', 'You can append a single row as a dictionary using the ignore_index option.', 'Yes, people have already explained that you should NEVER grow a DataFrame, and that you should append your data to a list and convert it to a DataFrame once at the end. But do you understand why?', 'For the sake of Pythonic way, here add my answer:', 'You can also build up a list of lists and convert it to a dataframe -', 'This is not an answer to the OP question but a toy example to illustrate the answer of @ShikharDua above which I found very useful.', 'Figured out a simple and nice way:', 'You can use generator object to create Dataframe, which will be more memory efficient over the list.', 'Create a new record(data frame) and add to old_data_frame.\npass list of values and corresponding column names to create a new_record (data_frame)', 'Here is the way to add/append a row in pandas DataFrame', ""Instead of a list of dictionaries as in ShikharDua's answer, we can also represent our table as a dictionary of lists, where each list stores one column in row-order, given we know our columns beforehand. At the end we construct our DataFrame once."", 'if you want to add row at the end append it as a list', 'Another way to do it (probably not very performant):', 'All you need is loc[df.shape[0]] or loc[len(df)]', 'You can use for loop to iterate through values or can add arrays of values', 'Make it simple. By taking list as input which will be appended as row in data-frame:-', 'We often see the construct df.loc[subscript] = … to assign to one DataFrame row. Mikhail_Sam posted benchmarks containing, among others, this construct as well as the method using dict and create DataFrame in the end. He found the latter to be the fastest by far. But if we replace the df3.loc[i]  = … (with preallocated DataFrame) in his code with df3.values[i] = …, the outcome changes significantly, in that that method performs similar to the one using dict. So we should more often take the use of df.values[subscript] = … into consideration. However note that .values takes a zero-based subscript, which may be different from the DataFrame.index.', 'pandas.DataFrame.append', 'You can concatenate two DataFrames for this. I basically came across this problem to add a new row to an existing DataFrame with a character index(not numeric).\nSo, I input the data for a new row in a duct() and index in a list.', 'If all data in your Dataframe has the same dtype you might use a numpy array. You can write rows directly into the predefined array and convert it to a dataframe at the end.\nSeems to be even faster than converting a list of dicts.', 'This will take care of adding an item to an empty DataFrame. The issue is that df.index.max() == nan for the first index:', 'before going to add a row, we have to convert the dataframe to dictionary there you can see the keys as columns in dataframe and values of the columns are again stored in the dictionary but there key for every column is the index number in dataframe. That idea make me to write the below code.']"
11,Change column type in pandas,"I want to convert a table, represented as a list of lists, into a Pandas DataFrame. As an extremely simplified example:

a = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]
df = pd....",https://stackoverflow.com/questions/15891038/change-column-type-in-pandas,"['I want to convert a table, represented as a list of lists, into a Pandas DataFrame. As an extremely simplified example:', 'You have four main options for converting types in pandas:', 'How about this?', 'this below code will change datatype of column.', ""When I've only needed to specify specific columns, and I want to be explicit, I've used (per DOCS LOCATION):"", 'Here is a function that takes as its arguments a DataFrame and a list of columns and coerces all data in the columns to numbers.', 'How about creating two dataframes, each with different data types for their columns, and then appending them together?', ""Here's a chart that summarises some of the most important conversions in pandas."", 'Starting pandas 1.0.0, we have pandas.DataFrame.convert_dtypes. You can even control what types to convert!', ""I thought I had the same problem but actually I have a slight difference that makes the problem easier to solve. For others looking at this question it's worth checking the format of your input list. In my case the numbers are initially floats not strings as in the question:""]"
12,How to drop rows of Pandas DataFrame whose value in a certain column is NaN,"I have this DataFrame and want only the records whose EPS column is not NaN:

>>> df
                 STK_ID  EPS  cash
STK_ID RPT_Date                   
601166 20111231  601166  NaN   NaN
...",https://stackoverflow.com/questions/13413590/how-to-drop-rows-of-pandas-dataframe-whose-value-in-a-certain-column-is-nan,"['I have this DataFrame and want only the records whose EPS column is not NaN:', ""Don't drop, just take the rows where EPS is not NA:"", 'This question is already resolved, but...', 'I know this has already been answered, but just for the sake of a purely pandas solution to this specific question as opposed to the general description from Aman (which was wonderful) and in case anyone else happens upon this:', 'You can use this:', 'Simplest of all solutions:', 'You could use dataframe method notnull or inverse of isnull, or numpy.isnan:', 'Simple and easy way', 'yet another solution which uses the fact that np.nan != np.nan:', ""This is an old question which has been beaten to death but I do believe there is some more useful information to be surfaced on this thread. Read on if you're looking for the answer to any of the following questions:"", 'Another version:', ""In datasets having large number of columns its even better to see how many columns contain null values and how many don't."", ""It  may be added at that '&' can be used to add additional conditions e.g.""]"
13,Writing a pandas DataFrame to CSV file,"I have a dataframe in pandas which I would like to write to a CSV file. I am doing this using:

df.to_csv('out.csv')
And getting the error:

UnicodeEncodeError: 'ascii' codec can't encode character u'...",https://stackoverflow.com/questions/16923281/writing-a-pandas-dataframe-to-csv-file,"['I have a dataframe in pandas which I would like to write to a CSV file. I am doing this using:', 'To delimit by a tab you can use the sep argument of to_csv:', 'When you are storing a DataFrame object into a csv file using the to_csv method, you probably wont be needing to store the preceding indices of each row of the DataFrame object.', 'To write a pandas DataFrame to a CSV file, you will need DataFrame.to_csv. This function offers many arguments with reasonable defaults that you will more often than not need to override to suit your specific use case. For example, you might want to use a different separator, change the datetime format, or drop the index when writing. to_csv has arguments you can pass to address these requirements.', ""Something else you can try if you are having issues encoding to 'utf-8' and want to go cell by cell you could try the following."", 'Sometimes you face these problems if you specify UTF-8 encoding also.\nI recommend you to specify encoding while reading file and same encoding while writing to file.\nThis might solve your problem.', 'Example of export in file with full path on Windows and in case your file has headers:', 'it could be not the answer for this case, but as I had the same error-message with .to_csvI tried .toCSV(\'name.csv\') and the error-message was different (""SparseDataFrame\' object has no attribute \'toCSV\'). So the problem was solved by turning dataframe to dense dataframe']"
14,Use a list of values to select rows from a pandas dataframe [duplicate],"Lets say I have the following pandas dataframe:

df = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})
df

     A   B
0    5   1
1    6   2
2    3   3
3    4   5
I can subset based on a specific value:
...",https://stackoverflow.com/questions/12096252/use-a-list-of-values-to-select-rows-from-a-pandas-dataframe,"['Lets say I have the following pandas dataframe:', 'You can use isin method:']"
15,Convert list of dictionaries to a pandas DataFrame,"I have a list of dictionaries like this:

[{'points': 50, 'time': '5:00', 'year': 2010}, 
{'points': 25, 'time': '6:00', 'month': ""february""}, 
{'points':90, 'time': '9:00', 'month': 'january'}, 
{'...",https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe,"['I have a list of dictionaries like this:', 'Supposing d is your list of dicts, simply:', 'The other answers are correct, but not much has been explained in terms of advantages and limitations of these methods. The aim of this post will be to show examples of these methods under different situations, discuss when to use (and when not to use), and suggest alternatives.', 'In pandas 16.2, I had to do pd.DataFrame.from_records(d) to get this to work.', 'You can also use pd.DataFrame.from_dict(d) as :', 'Pyhton3:\nMost of the solutions listed previously work. However, there are instances when row_number of the dataframe is not required and the each row (record) has to be written individually.', 'For converting a list of dictionaries to a pandas DataFrame, you can use ""append"":', 'The easiest way I have found to do it is like this:']"
16,Pretty-print an entire Pandas Series / DataFrame,"I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.

Is there a builtin way to ...",https://stackoverflow.com/questions/19124601/pretty-print-an-entire-pandas-series-dataframe,"['I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.', 'You can also use the option_context, with one or more options:', 'No need to hack settings. There is a simple way:', 'Sure, if this comes up a lot, make a function like this one. You can even configure it to load every time you start IPython: https://ipython.org/ipython-doc/1/config/overview.html', 'After importing pandas, as an alternative to using the context manager, set such options for displaying entire dataframes:', 'Use the tabulate package:', 'This answer is a variation of the prior answer by lucidyan. It makes the code more readable by avoiding the use of set_option.', 'If you are using Ipython Notebook (Jupyter). You can use HTML', 'Try this', 'You can achieve this using below method. just pass the total no. of columns present in the DataFrame as arg to', 'Nobody has proposed this simple plain-text solution:', 'Try using display() function. This would automatically use Horizontal and vertical scroll bars and with this you can display different datasets easily instead of using print().']"
17,How to deal with SettingWithCopyWarning in Pandas,"Background
I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:
E:\FinReporter\FM_EXT.py:449: SettingWithCopyWarning: A value ...",https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas,"['I just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:', 'The SettingWithCopyWarning was created to flag potentially confusing ""chained"" assignments, such as the following, which does not always work as expected, particularly when the first selection returns a copy.  [see GH5390 and GH5597 for background discussion.]', 'This post is meant for readers who,', 'In general the point of the SettingWithCopyWarning is to show users (and especially new users) that they may be operating on a copy and not the original as they think. There are false positives (IOW if you know what you are doing it could be ok). One possibility is simply to turn off the (by default warn) warning as @Garrett suggest.', 'When you go and do something like this:', 'Here I answer the question directly. How to deal with it?', 'This topic is really confusing with Pandas. Luckily, it has a relatively simple solution.', 'To remove any doubt, my solution was to make a deep copy of the slice instead of a regular copy.\nThis may not be applicable depending on your context (Memory constraints / size of the slice, potential for performance degradation - especially if the copy occurs in a loop like it did for me, etc...)', 'This should work:', 'Some may want to simply suppress the warning:', ""I had been getting this issue with .apply() when assigning a new dataframe from a pre-existing dataframe on which i've used the .query() method. For instance:"", 'If you have assigned the slice to a variable and want to set using the variable as in the following:', 'You could avoid the whole problem like this, I believe:', 'For me this issue occured in a following >simplified< example. And I was also able to solve it (hopefully with a correct solution):', 'Followup beginner question / remark', 'As this question is already fully explained and discussed in existing answers I will just provide a neat pandas approach to the context manager using pandas.option_context (links to docs and example) - there is absolutely no need to create a custom class with all the dunder methods and other bells and whistles.']"
18,How do I expand the output display to see more columns of a pandas DataFrame?,"Is there a way to widen the display of output in either interactive or script-execution mode?

Specifically, I am using the describe() function on a pandas DataFrame.  When the DataFrame is 5 columns (...",https://stackoverflow.com/questions/11707586/how-do-i-expand-the-output-display-to-see-more-columns-of-a-pandas-dataframe,"['Is there a way to widen the display of output in either interactive or script-execution mode?', 'Update: Pandas 0.23.4 onwards', 'Try this:', 'If you want to set options temporarily to display one large DataFrame, you can use option_context:', 'Only using these 3 lines worked for me:', 'Set column max width using:', 'You can use print df.describe().to_string() to force it to show the whole table.  (You can use to_string() like this for any DataFrame.  The result of describe is just a DataFrame itself.)', 'You can adjust pandas print options with set_printoptions.', 'You can set the output display to match your current terminal width:', ""According to the docs for v0.18.0, if you're running on a terminal (ie not iPython notebook, qtconsole or IDLE), it's a 2-liner to have Pandas auto-detect your screen width and adapt on the fly with how many columns it shows:"", ""It seems like all above answers solve the problem. One more point: instead of pd.set_option('option_name'), you can use the (auto-complete-able)"", 'OutPut:', ""The below line is enough to display all columns from dataframe.\n pd.set_option('display.max_columns', None)"", 'I used these settings when scale of data is high.', ""If you don't want to mess with your display options and you just want to see this one particular list of columns without expanding out every dataframe you view, you could try:"", 'You can also try in a loop:', 'You can simply do the following steps,', 'You can specify the numbers of columns as per your requirement in max_columns.', 'You can use this custom function for displaying things for pandas Dataframe.', ""None of these answers were working for me. A couple of them would indeed print all the columns, but it would look sloppy. As in all the information was there, but it wasn't formatted correctly. I'm using a terminal inside of Neovim so I suspect that to be the reason.""]"
19,How are iloc and loc different?,"Can someone explain how these two methods of slicing are different?
I've seen the docs,
and I've seen these answers, but I still find myself unable to explain how the three are different.  To me, they ...",https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different,"[""Can someone explain how these two methods of slicing are different?\nI've seen the docs,\nand I've seen these answers, but I still find myself unable to explain how the three are different.  To me, they seem interchangeable in large part, because they are at the lower levels of slicing."", 'Note: in pandas version 0.20.0 and above, ix is deprecated and the use of loc and iloc is encouraged instead. I have left the parts of this answer that describe ix intact as a reference for users of earlier versions of pandas. Examples have been added below showing alternatives to  ix.', 'iloc works based on integer positioning. So no matter what your row labels are, you can always, e.g., get the first row by doing', 'In my opinion, the accepted answer is confusing, since it uses a DataFrame with only missing values. I also do not like the term position-based for .iloc and instead, prefer integer location as it is much more descriptive and exactly what .iloc stands for. The key word is INTEGER - .iloc needs INTEGERS.']"
20,Deleting DataFrame row in Pandas based on column value,"I have the following DataFrame:

             daysago  line_race rating        rw    wrating
 line_date                                                 
 2007-03-31       62         11     56  1....",https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value,"['I have the following DataFrame:', ""If I'm understanding correctly, it should be as simple as:"", ""But for any future bypassers you could mention that df = df[df.line_race != 0] doesn't do anything when trying to filter for None/missing values."", 'just to add another solution, particularly useful if you are using the new pandas assessors, other solutions will replace the original pandas and lose the assessors', 'The best way to do this is with boolean masking:', 'If you want to delete rows based on multiple values of the column, you could use:', ""The given answer is correct nontheless as someone above said you can use df.query('line_race != 0') which depending on your problem is much faster. Highly recommend."", 'Though the previou answer are almost similar to what I am going to do, but using the index method does not require using another indexing method .loc(). It can be done in a similar but precise manner as', 'Another way of doing it. May not be the most efficient way as the code looks a bit more complex than the code mentioned in other answers, but still alternate way of doing the same thing.', 'I compiled and run my code. This is accurate code. You can try it your own.', 'Just adding another way for DataFrame expanded over all columns:']"
21,Combine two columns of text in pandas dataframe,"I have a 20 x 4000 dataframe in Python using pandas. Two of these columns are named Year and quarter. I'd like to create a variable called period that makes Year = 2000 and quarter= q2 into 2000q2.

...",https://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-pandas-dataframe,"[""I have a 20 x 4000 dataframe in Python using pandas. Two of these columns are named Year and quarter. I'd like to create a variable called period that makes Year = 2000 and quarter= q2 into 2000q2."", 'if both columns are strings, you can concatenate them directly:', 'Yields this dataframe', 'or slightly slower but more compact:', 'The method cat() of the .str accessor works really well for this:', 'Use of a lamba function this time with string.format().', 'Although the @silvado answer is good if you change df.map(str) to df.astype(str) it will be faster:', 'Let us suppose your  dataframe is df with columns Year and Quarter.', 'generalising to multiple columns, why not:', 'Here is an implementation that I find very versatile:', 'more efficient is', 'Using zip could be even quicker:', 'This solution uses an intermediate step compressing two columns of the DataFrame to a single column containing a list of the values.\nThis works not only for strings but for all kind of column-dtypes', 'Here is my summary of the above solutions to concatenate / combine two columns with int and str value into a new column, using a separator between the values of columns. Three solutions work for this purpose.', 'As many have mentioned previously, you must convert each column to string and then use the plus operator to combine two string columns. You can get a large performance improvement by using NumPy.', 'my take....', 'Use .combine_first.', 'For example:', 'One can use assign method of DataFrame:']"
22,Set value for particular cell in pandas DataFrame using index,"I've created a Pandas DataFrame

df = DataFrame(index=['A','B','C'], columns=['x','y'])
and got this
    x    y
A  NaN  NaN
B  NaN  NaN
C  NaN  NaN
Then I want to assign value to particular cell, ...",https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index,"[""I've created a Pandas DataFrame"", ""RukTech's answer, df.set_value('C', 'x', 10), is far and away faster than the options I've suggested below. However, it has been slated for deprecation."", 'Update: The .set_value method is going to be deprecated. .iat/.at are good replacements, unfortunately pandas provides little documentation', 'You can also use a conditional lookup using .loc as seen here:', 'The recommended way (according to the maintainers) to set a value is:', 'Try using df.loc[row_index,col_indexer] = value', 'This is the only thing that worked for me!', '.iat/.at is the good solution.\nSupposing you have this simple data_frame:', 'To set values, use:', 'you can use .iloc.', 'In my example i just change it in selected cell', 'set_value() is deprecated.', 'I tested and the output is df.set_value is little faster, but the official method df.at looks like the fastest non deprecated way to do it.', 'Here is a summary of the valid solutions provided by all users, for data frames indexed by integer and string.', 'One way to use index with condition is first get the index of all the rows that satisfy your condition and then simply use those row indexes in a multiple of ways', ""df.loc['c','x']=10\nThis will change the value of cth row and\n xth column."", 'In addition to the answers above, here is a benchmark comparing different ways to add rows of data to an already existing dataframe. It shows that using at or set-value is the most efficient way for large dataframes (at least for these test conditions).', 'If you want to change values not for whole row, but only for some columns:', ""From version 0.21.1 you can also use .at method. There are some differences compared to .loc as mentioned here - pandas .at versus .loc, but it's faster on single value replacement"", ""Soo, your question to convert NaN at ['x',C] to value 10"", 'I too was searching for this topic and I put together a way to iterate through a DataFrame and update it with lookup values from a second DataFrame.  Here is my code.']"
23,How to count the NaN values in a column in pandas DataFrame,I want to find the number of NaN in each column of my data so that I can drop a column if it has fewer NaN than some threshold. I looked but wasn't able to find any function for this.  value_counts is ...,https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe,"[""I want to find the number of NaN in each column of my data so that I can drop a column if it has fewer NaN than some threshold. I looked but wasn't able to find any function for this.  value_counts is too slow for me because most of the values are distinct and I'm only interested in the NaN count."", ""You can use the isna() method (or it's alias isnull() which is also compatible with older pandas versions < 0.21.0) and then sum to count the NaN values. For one column:"", 'You could subtract the total length from the count of non-nan values:', 'Lets assume df is a pandas DataFrame.', 'Based on the most voted answer we can easily define a function that gives us a dataframe to preview the missing values and the % of missing values in each column:', 'Since pandas 0.14.1 my suggestion here to have a keyword argument in the value_counts method has been implemented:', 'if its just counting nan values in a pandas column here is a quick way', 'The below will print all the Nan columns in descending order.', 'if you are using Jupyter Notebook, How about....', 'Please use below for particular column count', 'You can use following function, which will give you output in Dataframe', 'To count zeroes:', 'You can use value_counts method and print values of np.nan', 'Hope this helps,', 'One other simple option not suggested yet, to just count NaNs, would be adding in the shape to return the number of rows with NaN.', 'This will do the trick.', 'Here is the code for counting Null values column wise :', 'There is a nice Dzone article from July 2017 which details various ways of summarising NaN values. Check it out here.', 'df.isnull().sum() \n will give the column-wise sum of missing values.', 'based to the answer that was given and some improvements this is my approach', 'In case you need to get the non-NA (non-None) and NA (None) counts across different groups pulled out by groupby:', 'For the 1st part count NaN we have multiple way.', 'Used the solution proposed by @sushmit in my code.', 'Gives as output:', 'Suppose you want to get the number of missing values(NaN) in a column(series) known as price in a dataframe called reviews', 'For your task you can use pandas.DataFrame.dropna (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html):', 'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.count.html#pandas.Series.count']"
24,"Creating an empty Pandas DataFrame, then filling it?","I'm starting from the pandas DataFrame docs here: http://pandas.pydata.org/pandas-docs/stable/dsintro.html

I'd like to iteratively fill the DataFrame with values in a time series kind of calculation.
...",https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it,"[""I'm starting from the pandas DataFrame docs here: http://pandas.pydata.org/pandas-docs/stable/dsintro.html"", ""Here's a couple of suggestions:"", 'TLDR; (just read the bold text)', 'If you simply want to create an empty data frame and fill it with some incoming data frames later, try this:', 'Initialize empty frame with column names', 'Assume a dataframe with 19 rows']"
25,Converting a Pandas GroupBy output from Series to DataFrame,"I'm starting with input data like this

df1 = pandas.DataFrame( { 
    ""Name"" : [""Alice"", ""Bob"", ""Mallory"", ""Mallory"", ""Bob"" , ""Mallory""] , 
    ""City"" : [""Seattle"", ""Seattle"", ""Portland"", ""Seattle"", ""...",https://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-output-from-series-to-dataframe,"[""I'm starting with input data like this"", 'g1 here is a DataFrame. It has a hierarchical index, though:', ""I want to slightly change the answer given by Wes, because version 0.16.2 requires as_index=False. If you don't set it, you get an empty dataframe."", 'Simply, this should do the task:', 'The key is to use the reset_index() method.', 'Maybe I misunderstand the question but if you want to convert the groupby back to a dataframe you can use .to_frame(). I wanted to reset the index when I did this so I included that part as well.', 'I found this worked for me.', 'Below solution may be simpler:', 'I have aggregated with Qty wise data and store to dataframe', 'These solutions only partially worked for me because I was doing multiple aggregations. Here is a sample output of my grouped by that I wanted to convert to a dataframe:']"
26,How to check if any value is NaN in a Pandas DataFrame,"In Python Pandas, what's the best way to check whether a DataFrame has one (or more) NaN values?

I know about the function pd.isnan, but this returns a DataFrame of booleans for each element. This ...",https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe,"[""In Python Pandas, what's the best way to check whether a DataFrame has one (or more) NaN values?"", ""jwilner's response is spot on. I was exploring to see if there's a faster option, since in my experience, summing flat arrays is (strangely) faster than counting. This code seems faster:"", 'You have a couple of options.', 'To find out which rows have NaNs in a specific column:', 'If you need to know how many rows there are with ""one or more NaNs"":', 'df.isnull().any().any() should do it.', 'Adding to Hobs brilliant answer, I am very new to Python and Pandas so please point out if I am wrong.', 'Starting from v0.23.2, you can use DataFrame.isna + DataFrame.any(axis=None) where axis=None specifies logical reduction over the entire DataFrame.', 'Since none have mentioned, there is just another variable called hasnans.', 'let df be the name of the Pandas DataFrame and any value that is numpy.nan is a null value.', ""Since pandas has to find this out for DataFrame.dropna(), I took a look to see how they implement it and discovered that they made use of DataFrame.count(), which counts all non-null values in the DataFrame. Cf. pandas source code. I haven't benchmarked this technique, but I figure the authors of the library are likely to have made a wise choice for how to do it."", 'This will give you count of all NaN values present in the respective coloums of the DataFrame.', ""I've been using the following and type casting it to a string and checking for the nan value"", 'Just using\nmath.isnan(x), Return True if x is a NaN (not a number), and False otherwise.', 'Here is another interesting way of finding null and replacing with a calculated value', 'The best would be to use:', 'We can see the null values present in the dataset by generating heatmap using seaborn moduleheatmap', 'Or you can use .info() on the DF such as :', '', 'Will check for each column if it contains Nan or not.', ""You could not only check if any 'NaN' exist but also get the percentage of 'NaN's in each column using the following,"", ""Depending on the type of data you're dealing with, you could also just get the value counts of each column while performing your EDA by setting dropna to False.""]"
27,Convert pandas dataframe to NumPy array,"I am interested in knowing how to convert a pandas dataframe into a NumPy array.

dataframe:

import numpy as np
import pandas as pd

index = [1, 2, 3, 4, 5, 6, 7]
a = [np.nan, np.nan, np.nan, 0.1, 0....",https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array,"['I am interested in knowing how to convert a pandas dataframe into a NumPy array.', 'To convert a pandas dataframe (df) to a numpy ndarray, use this code:', ""It's time to deprecate your usage of values and as_matrix()."", 'Note: The .as_matrix() method used in this answer is deprecated. Pandas 0.23.4 warns:', 'I would just chain the DataFrame.reset_index() and DataFrame.values functions to get the Numpy representation of the dataframe, including the index:', 'You can use the to_records method, but have to play around a bit with the dtypes if they are not what you want from the get go. In my case, having copied your DF from a string, the index type is string (represented by an object dtype in pandas):', ""It seems like df.to_records() will work for you. The exact feature you're looking for was requested and to_records pointed to as an alternative."", 'Try this:', 'Here is my approach to making a structure array from a pandas DataFrame.', 'A Simpler Way for Example DataFrame:', 'Two ways to convert the data-frame to its Numpy-array representation.', 'Just had a similar problem when exporting from dataframe to arcgis table and stumbled on a solution from usgs (https://my.usgs.gov/confluence/display/cdi/pandas.DataFrame+to+ArcGIS+Table).\nIn short your problem has a similar solution:', 'I went through the answers above. The ""as_matrix()"" method works but its obsolete now. For me, What worked was "".to_numpy()"".', ""Further to meteore's answer, I found the code"", 'Try this:', 'A simple way to convert dataframe to numpy array:']"
28,How to convert index of a pandas dataframe into a column?,"This seems rather obvious, but I can't seem to figure out how to convert an index of data frame to a column?

For example:

df=
        gi       ptt_loc
 0  384444683      593  
 1  384444684      594 ...",https://stackoverflow.com/questions/20461165/how-to-convert-index-of-a-pandas-dataframe-into-a-column,"[""This seems rather obvious, but I can't seem to figure out how to convert an index of data frame to a column?"", 'either:', 'For MultiIndex you can extract its subindex using', ""To provide a bit more clarity, let's look at a DataFrame with two levels in its index (a MultiIndex)."", 'You can first rename your index to a desired label, then elevate to a series:', 'If you want to use the reset_index method and also preserve your existing index you should use:', 'A very simple way of doing this is to use reset_index() method.For a data frame df use the code below:']"
29,Select by partial string from a pandas DataFrame,"I have a DataFrame with 4 columns of which 2 contain string values. I was wondering if there was a way to select rows based on a partial string match against a particular column?

In other words, a ...",https://stackoverflow.com/questions/11350770/select-by-partial-string-from-a-pandas-dataframe,"['I have a DataFrame with 4 columns of which 2 contain string values. I was wondering if there was a way to select rows based on a partial string match against a particular column?', ""Based on github issue #620, it looks like you'll soon be able to do the following:"", 'I tried the proposed solution above:', 'This post is meant for readers who want to', 'If anyone wonders how to perform a related problem: ""Select column by partial string""', 'Quick note: if you want to do selection based on a partial string contained in the index, try the following:', 'Say you have the following DataFrame:', ""Here's what I ended up doing for partial string matches.  If anyone has a more efficient way of doing this please let me know."", ""Using contains didn't work well for my string with special characters. Find worked though."", 'Should you need to do a case insensitive search for a string in a pandas dataframe column:', 'There are answers before this which accomplish the asked feature, anyway I would like to show the most generally way:', 'Maybe you want to search for some text in all columns of the Pandas dataframe, and not just in the subset of them. In this case, the following code will help.']"
30,How to replace NaN values by Zeroes in a column of a Pandas Dataframe?,"I have a Pandas Dataframe as below:
      itm Date                  Amount 
67    420 2012-09-30 00:00:00   65211
68    421 2012-09-09 00:00:00   29424
69    421 2012-09-16 00:00:00   29877
70    421 ...",https://stackoverflow.com/questions/13295735/how-to-replace-nan-values-by-zeroes-in-a-column-of-a-pandas-dataframe,"['I have a Pandas Dataframe as below:', 'I believe DataFrame.fillna() will do this for you.', 'It is not guaranteed that the slicing returns a view or a copy. You can do', 'You could use replace to change NaN to 0:', ""I just wanted to provide a bit of an update/special case since it looks like people still come here. If you're using a multi-index or otherwise using an index-slicer the inplace=True option may not be enough to update the slice you've chosen. For example in a 2x2 level multi-index this will not change any values (as of pandas 0.15):"", 'The below code worked for me.', 'Easy way to fill the missing values:-', 'You can also use dictionaries to fill NaN values of the specific columns in the DataFrame rather to fill all the DF with some oneValue.', '', 'To replace na values in pandas', 'To replace nan in different columns with different ways:', 'If you were to convert it to a pandas dataframe, you can also accomplish this by using fillna.', 'There are two options available primarily; in case of imputation or filling of missing values NaN / np.nan with only numerical replacements (across column(s):']"
31,How to filter Pandas dataframe using 'in' and 'not in' like in SQL,"How can I achieve the equivalents of SQL's IN and NOT IN?
I have a list with the required values.
Here's the scenario:
df = pd.DataFrame({'country': ['US', 'UK', 'Germany', 'China']})
...",https://stackoverflow.com/questions/19960077/how-to-filter-pandas-dataframe-using-in-and-not-in-like-in-sql,"[""How can I achieve the equivalents of SQL's IN and NOT IN?"", 'You can use pd.Series.isin.', 'Alternative solution that uses .query() method:', 'Pandas offers two methods: Series.isin and DataFrame.isin for Series and DataFrames, respectively.', ""I've been usually doing generic filtering over rows like this:"", 'Collating possible solutions from the answers:', 'I wanted to filter out dfbc rows that had a BUSINESS_ID that was also in the BUSINESS_ID of dfProfilesBusIds', 'implement in:', 'A trick if you want to keep the order of the list:', 'Try the following:']"
32,"Difference between map, applymap and apply methods in Pandas","Can you tell me when to use these vectorization methods with basic examples? 

I see that map is a Series method whereas the rest are DataFrame methods. I got confused about apply and applymap methods ...",https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas,"['Can you tell me when to use these vectorization methods with basic examples?', ""Straight from Wes McKinney's Python for Data Analysis book, pg. 132 (I highly recommended this book):"", 'First major difference: DEFINITION', 'DataFrame.apply operates on entire rows or columns at a time.', 'Adding to the other answers, in a Series there are also map and apply.', '@jeremiahbuddha mentioned that apply works on row/columns, while applymap works element-wise. But it seems you can still use apply for element-wise computation....', 'Just wanted to point out, as I struggled with this for a bit', 'Probably simplest explanation the difference between apply and applymap:', 'My understanding:', 'Based on the answer of cs95', 'FOMO:']"
33,Shuffle DataFrame rows,"I have the following DataFrame:

    Col1  Col2  Col3  Type
0      1     2     3     1
1      4     5     6     1
...
20     7     8     9     2
21    10    11    12     2
...
45    13    14    15     ...",https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows,"['I have the following DataFrame:', 'The idiomatic way to do this with Pandas is to use the .sample method of your dataframe to sample all rows without replacement:', 'You can simply use sklearn for this', 'You can shuffle the rows of a dataframe by indexing with a shuffled index. For this, you can eg use np.random.permutation (but np.random.choice is also a possibility):', 'TL;DR: np.random.shuffle(ndarray) can do the job.\nSo, in your case', ""(I don't have enough reputation to comment this on the top post, so I hope someone else can do that for me.) There was a concern raised that the first method:"", 'What is also useful, if you use it for Machine_learning and want to seperate always the same data, you could use:', 'AFAIK the simplest solution is:', 'shuffle the pandas data frame by taking a sample array in this case index and randomize its order then set the array as an index of data frame. Now sort the data frame according to index. Here goes your shuffled dataframe', 'Here is another way:']"
34,"Get statistics for each group (such as count, mean, etc) using pandas GroupBy?","I have a data frame df and I use several columns from it to groupby:

df['col1','col2','col3','col4'].groupby(['col1','col2']).mean()
In the above way I almost get the table (data frame) that I need. ...",https://stackoverflow.com/questions/19384532/get-statistics-for-each-group-such-as-count-mean-etc-using-pandas-groupby,"['I have a data frame df and I use several columns from it to groupby:', 'On groupby object, the agg function can take a list to apply several aggregation methods at once. This should give you the result you need:', 'The simplest way to get row counts per group is by calling .size(), which returns a Series:', 'Returns count, mean, std, and other useful statistics per-group.', 'We can easily do it by using groupby and count. But, we should remember to use reset_index().', 'To get multiple stats, collapse the index, and retain column names:', 'Please try this code', 'Create a group object and call methods like below example:']"
35,UnicodeDecodeError when reading CSV file in Pandas with Python,"I'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error...
File ""C:\Importer\src\dfman\importer.py"", line 26, in ...",https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python,"[""I'm running a program which is processing 30,000 similar files. A random number of them are stopping and producing this error..."", 'read_csv takes an encoding option to deal with files in different formats. I mostly use read_csv(\'file\', encoding = ""ISO-8859-1""), or alternatively encoding = ""utf-8"" for reading, and generally utf-8 for to_csv.', 'Simplest of all Solutions:', 'Pandas allows to specify encoding, but does not allow to ignore errors not to automatically replace the offending bytes. So there is no one size fits all method but different ways depending on the actual use case.', ""after executing this code you will find encoding of 'filename.csv' then execute code as following"", 'In my case, a file has USC-2 LE BOM encoding, according to Notepad++. \nIt is encoding=""utf_16_le"" for python.', ""Try specifying the engine='python'. \nIt worked for me but I'm still trying to figure out why."", 'In my case this worked for python 2.7:', 'Struggled with this a while and thought I\'d post on this question as it\'s the first search result.  Adding the encoding=""iso-8859-1"" tag to pandas read_csv didn\'t work, nor did any other encoding, kept giving a UnicodeDecodeError.', ""I am posting an answer to provide an updated solution and explanation as to why this problem can occur. Say you are getting this data from a database or Excel workbook. If  you have special characters like La Cañada Flintridge city, well unless you are exporting the data using UTF-8 encoding, you're going to introduce errors. La Cañada Flintridge city will become La Ca\\xf1ada Flintridge city. If you are using pandas.read_csv without any adjustments to the default parameters, you'll hit the following error"", 'Please try to add', 'Try changing the encoding.\nIn my case, encoding = ""utf-16"" worked.', 'This answer seems to be the catch-all for CSV encoding issues. If you are getting a strange encoding problem with your header like this:', 'I am posting an update to this old thread. I found one solution that worked, but requires opening each file. I opened my csv file in LibreOffice, chose Save As > edit filter settings. In the drop-down menu I chose UTF8 encoding. Then I added encoding=""utf-8-sig"" to the data = pd.read_csv(r\'C:\\fullpathtofile\\filename.csv\', sep = \',\', encoding=""utf-8-sig"").', 'I have trouble opening a CSV file in simplified Chinese downloaded from an online bank, \nI have tried latin1, I have tried iso-8859-1, I have tried cp1252, all to no avail.', '^This line resulted in the same error because I am reading an excel file using read_csv() method. Use read_excel() for reading .xlxs', 'You can try this.', ""I am using Jupyter-notebook. And in my case, it was showing the file in the wrong format. The 'encoding' option was not working.\nSo I save the csv in utf-8 format, and it works."", 'Try this:', 'Check the encoding before you pass to pandas. It will slow you down, but...', 'You can try with:', ""Sometimes the problem is with the .csv file only. The file may be corrupted.\nWhen faced with this issue. 'Save As' the file as csv again.""]"
36,Filter dataframe rows if value in column is in a set list of values [duplicate],"I have a Python pandas DataFrame rpt:

rpt
<class 'pandas.core.frame.DataFrame'>
MultiIndex: 47518 entries, ('000002', '20120331') to ('603366', '20091231')
Data columns:
STK_ID                  ...",https://stackoverflow.com/questions/12065885/filter-dataframe-rows-if-value-in-column-is-in-a-set-list-of-values,"['I have a Python pandas DataFrame rpt:', 'Use the isin method:', 'isin() is ideal if you have a list of exact matches, but if you have a list of partial matches or substrings to look for, you can filter using the str.contains method and regular expressions.', 'you can also use ranges by using:', 'You can also directly query your DataFrame for this information.', 'Given a dataframe like this:', ""You can also achieve similar results by using 'query' and @:"", 'You can use query, i.e.:']"
37,Pandas Merging 101,"How to perform a (INNER| (LEFT|RIGHT|FULL) OUTER) JOIN with pandas?
How do I add NaNs for missing rows after merge?
How do I get rid of NaNs after merging?
Can I merge on the index?
Cross join with ...",https://stackoverflow.com/questions/53645882/pandas-merging-101,"[""... and more. I've seen these recurring questions asking about various facets of the pandas merge functionality. Most of the information regarding merge and its various use cases today is fragmented across dozens of badly worded, unsearchable posts. The aim here is to collate some of the more important points for posterity."", 'This post aims to give readers a primer on SQL-flavored merging with pandas, how to use it, and when not to use it.', ""A supplemental visual view of pd.concat([df0, df1], kwargs). \nNotice that, kwarg axis=0 or axis=1 's meaning is not as intuitive as df.mean() or df.apply(func)"", 'In this answer, I will consider a practical example of the pandas.concat.']"
38,How to avoid Python/Pandas creating an index in a saved csv?,"I am trying to save a csv to a folder after making some edits to the file. 

Every time I use pd.to_csv('C:/Path of file.csv') the csv file has a separate column of indexes. I want to avoid printing ...",https://stackoverflow.com/questions/20845213/how-to-avoid-python-pandas-creating-an-index-in-a-saved-csv,"['I am trying to save a csv to a folder after making some edits to the file.', 'Use index=False.', 'There are two ways to handle the situation where we do not want the index to be stored in csv file.', 'If you want no index, read file using:', ""As others have stated, if you don't want to save the index column in the first place, you can use df.to_csv('processed.csv', index=False)"", 'Another solution if you want to keep this column as index.', 'If you want a good format the next statement is the best:']"
39,Import multiple csv files into pandas and concatenate into one DataFrame,"I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:

import glob
...",https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe,"['I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:', 'If you have same columns in all your csv files then you can try the code below.\nI have added header=0 so that after reading csv first row can be assigned as the column names.', ""An alternative to darindaCoder's answer:"", 'The Dask library can read a dataframe from multiple files:', 'Almost all of the answers here are either unnecessarily complex (glob pattern matching) or rely on additional 3rd party libraries. You can do this in 2 lines using everything Pandas and python (all versions) already have built in.', ""Import two or more csv's without having to make a list of names."", 'Edit: I googled my way into https://stackoverflow.com/a/21232849/186078.\nHowever of late I am finding it faster to do any manipulation using numpy and then assigning it once to dataframe rather than manipulating the dataframe itself on an iterative basis and it seems to work in this solution too.', 'If you want to search recursively (Python 3.5 or above), you can do the following:', ""one liner using map, but if you'd like to specify additional args, you could do:"", 'If the multiple csv files are zipped, you may use zipfile to read all and concatenate as below:', 'Another on-liner with list comprehension which allows to use arguments with read_csv.', ""Based on @Sid's good answer."", 'Alternative using the pathlib library (often preferred over os.path).', 'You can do it this way also:', 'This is how you can do using Colab on Google Drive']"
40,"Truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()","Having issue filtering my result dataframe with an or condition. I want my result df to extract all column var values that are above 0.25 and below -0.25.
This logic below gives me an ambiguous truth ...",https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o,"['Having issue filtering my result dataframe with an or condition. I want my result df to extract all column var values that are above 0.25 and below -0.25.', 'The or and and python statements require truth-values. For pandas these are considered ambiguous so you should use ""bitwise"" | (or) or & (and) operations:', 'For boolean logic, use & and |.', 'Well pandas use bitwise & | and each condition should be wrapped in a ()', 'Or, alternatively, you could use Operator module. More detailed information is here Python docs', 'This excellent answer explains very well what is happening and provides a solution. I would like to add another solution that might be suitable in similar cases: using the query method:', 'I encountered the same error and got stalled with a pyspark dataframe for few days, I was able to resolve it successfully by filling na values with 0 since I was comparing integer values from 2 fields.', ""You need to use bitwise operators | instead of or and & instead of and in pandas, you can't simply use the bool statements from python.\n\nFor much complex filtering create a mask and apply the mask on the dataframe.\nPut all your query in the mask and apply it.\nSuppose,"", 'One minor thing, which wasted my time.', ""I'll try to give the benchmark of the three most common way (also mentioned above):""]"
41,Selecting a row of pandas series/dataframe by integer index,"I am curious as to why df[2] is not supported, while df.ix[2] and df[2:3] both work. 

In [26]: df.ix[2]
Out[26]: 
A    1.027680
B    1.514210
C   -1.466963
D   -0.162339
Name: 2000-01-03 00:00:00

In ...",https://stackoverflow.com/questions/16096627/selecting-a-row-of-pandas-series-dataframe-by-integer-index,"['I am curious as to why df[2] is not supported, while df.ix[2] and df[2:3] both work.', 'echoing @HYRY, see the new docs in 0.11', 'When the indexing operator is passed a string or integer, it attempts to find a column with that particular name and return it as a Series.', 'You can think DataFrame as a dict of Series. df[key] try to select the column index by key and returns a Series object.', 'To index-based access to the pandas table, one can also consider numpy.as_array option to convert the table to Numpy array as', 'You can take a look at the source code .', 'you can loop through the data frame like this .']"
42,"Constructing pandas DataFrame from values in variables gives “ValueError: If using all scalar values, you must pass an index”","This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.

a = 2
b = 3
I want to construct a DataFrame from this:

df2 = pd.DataFrame({'A':...",https://stackoverflow.com/questions/17839973/constructing-pandas-dataframe-from-values-in-variables-gives-valueerror-if-usi,"['This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.', ""The error message says that if you're passing scalar values, you have to pass an index.  So you can either not use scalar values for the columns -- e.g. use a list:"", 'You can also use pd.DataFrame.from_records which is more convenient when you already have the dictionary in hand:', 'You need to create a pandas series first. The second step is to convert the pandas series to pandas dataframe.', 'You may try wrapping your dictionary in to list', 'Maybe Series would provide all the functions you need:', 'You need to provide iterables as the values for the Pandas DataFrame columns:', 'I had the same problem with numpy arrays and the solution is to flatten them:', 'Pandas magic at work. All logic is out.', 'You could try:', 'If you intend to convert a dictionary of scalars, you have to include an index:', 'the input does not have to be a list of records - it can be a single dictionary as well:', 'This is because a DataFrame has two intuitive dimensions - the columns and the rows.', ""You could try this:\ndf2 = pd.DataFrame.from_dict({'a':a,'b':b}, orient = 'index')"", ""Change your 'a' and 'b' values to a list, as follows:"", 'I usually use the following to to quickly create a small table from dicts.', 'Convert Dictionary to Data Frame', 'If you have a dictionary you can turn it into a pandas data frame with the following line of code:', 'Just pass the dict on a list:']"
43,How to apply a function to two columns of Pandas dataframe,"Suppose I have a df which has columns of 'ID', 'col_1', 'col_2'. And I define a function :

f = lambda x, y : my_function_expression.

Now I want to apply the f to df's two columns 'col_1', 'col_2' to ...",https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe,"[""Suppose I have a df which has columns of 'ID', 'col_1', 'col_2'. And I define a function :"", ""Here's an example using apply on the dataframe, which I am calling with axis = 1."", 'There is a clean, one-line way of doing this in Pandas:', 'A simple solution is:', 'A interesting question! my answer as below:', 'The method you are looking for is Series.combine. \nHowever, it seems some care has to be taken around datatypes. \nIn your example, you would (as I did when testing the answer) naively call', ""The way you have written f it needs two inputs. If you look at the error message it says you are not providing two inputs to f, just one. The error message is correct.\nThe mismatch is because df[['col1','col2']] returns a single dataframe with two columns, not two separate columns."", ""I'm going to put in a vote for np.vectorize. It allows you to just shoot over x number of columns and not deal with the dataframe in the function, so it's great for functions you don't control or doing something like sending 2 columns and a constant into a function (i.e. col_1, col_2, 'foo')."", ""Returning a list from apply is a dangerous operation as the resulting object is not guaranteed to be either a Series or a DataFrame. And exceptions might be raised in certain cases. Let's walk through a simple example:"", ""I'm sure this isn't as fast as the solutions using Pandas or Numpy operations, but if you don't want to rewrite your function you can use map.  Using the original example data -"", 'My example to your questions:', 'If you have a huge data-set, then you can use an easy but faster(execution time) way of doing this using swifter:', ""I suppose you don't want to change get_sublist function, and just want to use DataFrame's apply method to do the job. To get the result you want, I've wrote two help functions: get_sublist_list and unlist. As the function name suggest, first get the list of sublist, second extract that sublist from that list. Finally, We need to call apply function to apply those two functions to the df[['col_1','col_2']] DataFrame subsequently.""]"
44,How to get a value from a cell of a dataframe?,"I have constructed a condition that extract exactly one row from my data frame:

d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]
Now I would like to ...",https://stackoverflow.com/questions/16729574/how-to-get-a-value-from-a-cell-of-a-dataframe,"['I have constructed a condition that extract exactly one row from my data frame:', 'If you have a DataFrame with only one row, then access the first (only) row as a Series using iloc, and then the value using the column name:', 'These are fast access for scalars', 'You can turn your 1x1 dataframe into a numpy array, then access the first and only value of that array:', 'Most answers are using iloc which is good for selection by position.', 'I needed the value of one cell, selected by column and index names.\nThis solution worked for me:', 'It looks like changes after pandas 10.1/13.1', 'The quickest/easiest options I have found are the following. 501 represents the row index.', ""It doesn't need to be complicated:"", 'For pandas 0.10, where iloc is unavalable, filter a DF and get the first row data for the column VALUE:', 'Not sure if this is a good practice, but I noticed I can also get just the value by casting the series as float.', ""Index([u'Country', u'Country Code', u'Indicator Name', u'Indicator Code',\n         u'1960', u'1961', u'1962', u'1963', u'1964', u'1965', u'1966', u'1967',\n         u'1968', u'1969', u'1970', u'1971', u'1972', u'1973', u'1974', u'1975',\n         u'1976', u'1977', u'1978', u'1979', u'1980', u'1981', u'1982', u'1983',\n         u'1984', u'1985', u'1986', u'1987', u'1988', u'1989', u'1990', u'1991',\n         u'1992', u'1993', u'1994', u'1995', u'1996', u'1997', u'1998', u'1999',\n         u'2000', u'2001', u'2002', u'2003', u'2004', u'2005', u'2006', u'2007',\n         u'2008', u'2009', u'2010', u'2011', u'2012', u'2013', u'2014', u'2015',\n         u'2016'],\n        dtype='object')"", ""To get the full row's value as JSON (instead of a Serie):""]"
45,How to pivot a dataframe?,"What is pivot?
How do I pivot?
Is this a pivot?
Long format to wide format?

I've seen a lot of questions that ask about pivot tables.  Even if they don't know that they are asking about pivot tables, ...",https://stackoverflow.com/questions/47152691/how-to-pivot-a-dataframe,"[""I've seen a lot of questions that ask about pivot tables.  Even if they don't know that they are asking about pivot tables, they usually are.  It is virtually impossible to write a canonical question and answer that encompasses all aspects of pivoting..."", 'We start by answering the first question:', ""To extend @piRSquared's answer another version of Question 10""]"
46,Python Pandas Error tokenizing data,"I'm trying to use pandas to manipulate a .csv file but I get this error:
  pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12
I have tried to read the ...",https://stackoverflow.com/questions/18039057/python-pandas-error-tokenizing-data,"[""I'm trying to use pandas to manipulate a .csv file but I get this error:"", 'you could also try;', 'It might be an issue with', ""The parser is getting confused by the header of the file.  It reads the first row and infers the number of columns from that row.  But the first two rows aren't representative of the actual data in the file."", 'Your CSV file might have variable number of columns and read_csv inferred the number of columns from the first few rows. Two ways to solve it in this case:', ""This is definitely an issue of delimiter, as most of the csv CSV are got create using sep='/t' so try to read_csv using the tab character (\\t) using separator /t. so, try to open using following code line."", 'I had this problem, where I was trying to read in a CSV without passing in column names.', 'I had this problem as well but perhaps for a different reason. I had some trailing commas in my CSV that were adding an additional column that pandas was attempting to read. Using the following works but it simply ignores the bad lines:', 'I\'ve had this problem a few times myself. Almost every time, the reason is that the file I was attempting to open was not a properly saved CSV to begin with. And by ""properly"", I mean each row had the same number of separators or columns.', 'I came across the same issue. Using pd.read_table() on the same source file seemed to work.  I could not trace the reason for this but it was a useful workaround for my case. Perhaps someone more knowledgeable can shed more light on why it worked.', 'The following worked for me (I posted this answer, because I specifically had this problem in a Google Colaboratory Notebook):', 'The dataset that I used had a lot of quote marks ("") used extraneous of the formatting. I was able to fix the error by including this parameter for read_csv():', ""I've had a similar problem while trying to read a tab-delimited table with spaces, commas and quotes:"", 'Use delimiter in parameter', 'For those who are having similar issue with Python 3 on linux OS.', 'Although not the case for this question, this error may also appear with compressed data. Explicitly setting the value for kwarg compression resolved my problem.', 'An alternative that I have found to be useful in dealing with similar parsing errors uses the CSV module to re-route data into a pandas df. For example:', 'following sequence of commands works (I lose the first line of the data -no header=None present-, but at least it loads):', 'In my case the separator was not the default "","" but Tab.', 'Sometimes the problem is not how to use python, but with the raw data.\nI got this error message', ""use \npandas.read_csv('CSVFILENAME',header=None,sep=', ')"", 'I had a similar case as this and setting', 'You can try;', 'I had a dataset with prexisting row numbers, I used index_col:', 'This is what I did.', 'I have the same problem when read_csv: ParserError: Error tokenizing data.\nI just saved the old csv file to a new csv file. The problem is solved!', 'The issue for me was that a new column was appended to my CSV intraday. The accepted answer solution would not work as every future row would be discarded if I used error_bad_lines=False.', 'Simple resolution: Open the csv file in excel & save it with different name file of csv format. Again try importing it spyder, Your problem will be resolved!', 'I have encountered this error with a stray quotation mark.  I use mapping software which will put quotation marks around text items when exporting comma-delimited files.  Text which uses quote marks (e.g. \' = feet and "" = inches) can be problematic when then induce delimiter collisions.  Consider this example which notes that a 5-inch well log print is poor:', ""As far as I can tell, and after taking a look at your file, the problem is that the csv file you're trying to load has multiple tables. There are empty lines, or lines that contain table titles. Try to have a look at this Stackoverflow answer. It shows how to achieve that programmatically."", 'In my case, it is because the format of the first and last two lines of the csv file is different from the middle content of the file.']"
47,How to reset index in a pandas dataframe? [duplicate],"I have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like that: [1,5,6,10,11] and I would like to reset it to [0,1,2,3,4]. How can I do it?
The ...",https://stackoverflow.com/questions/20490274/how-to-reset-index-in-a-pandas-dataframe,"['I have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like that: [1,5,6,10,11] and I would like to reset it to [0,1,2,3,4]. How can I do it?', ""DataFrame.reset_index is what you're looking for. If you don't want it saved as a column, then do:"", 'Another solutions are assign RangeIndex or range:']"
48,Remap values in pandas column with a dict,"I have a dictionary which looks like this: di = {1: ""A"", 2: ""B""}

I would like to apply it to the ""col1"" column of a dataframe similar to:

     col1   col2
0       w      a
1       1      2
2       2 ...",https://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict,"['I have a dictionary which looks like this: di = {1: ""A"", 2: ""B""}', 'You can use .replace.  For example:', 'If your dictionary has more than a couple of keys, using map can be much faster than replace.  There are two versions of this approach, depending on whether your dictionary exhaustively maps all possible values (and also whether you want non-matches to keep their values or be converted to NaNs):', 'There is a bit of ambiguity in your question. There are at least three two interpretations:', 'Adding to this question if you ever have more than one columns to remap in a data dataframe:', ""DSM has the accepted answer, but the coding doesn't seem to work for everyone.  Here is one that works with the current version of pandas (0.23.4 as of 8/2018):"", 'Or do apply:', ""Given map is faster than replace (@JohnE's solution) you need to be careful with Non-Exhaustive mappings where you intend to map specific values to NaN. The proper method in this case requires that you mask the Series when you .fillna, else you undo the mapping to NaN."", 'A nice complete solution that keeps a map of your class labels:', 'As an extension to what have been proposed by Nico Coallier (apply to multiple columns) and U10-Forward(using apply style of methods), and summarising it into a one-liner I propose:', 'A more native pandas approach is to apply a replace function as below:']"
49,Pandas read_csv low_memory and dtype options,"When calling

df = pd.read_csv('somefile.csv')
I get:
  /Users/josh/anaconda/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py:1130:
  DtypeWarning: Columns (4,5,7,16) have mixed types.  ...",https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options,"['When calling', 'The low_memory option is not properly deprecated, but it should be, since it does not actually do anything differently[source]', 'Try:', 'This should solve the issue. I got exactly the same error, when reading 1.8M rows from a CSV.', 'As mentioned earlier by firelynx if dtype is explicitly specified and there is mixed data that is not compatible with that dtype then loading will crash. I used a converter like this as a workaround to change the values with incompatible data type so that the data could still be loaded.', ""I had a similar issue with a ~400MB file. Setting low_memory=False did the trick for me. Do the simple things first,I would check that your dataframe isn't bigger than your system memory, reboot, clear the RAM before proceeding. If you're still running into errors, its worth making sure your .csv file is ok, take a quick look in Excel and make sure there's no obvious corruption. Broken original data can wreak havoc..."", 'It worked for me with low_memory = False while importing a DataFrame. That is all the change that worked for me:', 'I was facing a similar issue when processing a huge csv file (6 million rows). I had three issues:\n1. the file contained strange characters (fixed using encoding)\n2. the datatype was not specified (fixed using dtype property)\n3. Using the above I still faced an issue which was related with the file_format that could not be defined based on the filename (fixed using try .. except..)', ""According to the pandas documentation, specifying low_memory=False as long as the engine='c' (which is the default) is a reasonable solution to this problem."", 'As the error says, you should specify the datatypes when using the read_csv() method.\nSo, you should write']"
50,How do I create test and train samples from one dataframe with pandas?,"I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.

Thanks!",https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas,"['I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.', ""I would just use numpy's randn:"", ""scikit learn's train_test_split is a good one."", 'Pandas random sample will also work', ""I would use scikit-learn's own training_test_split, and generate it from the index"", 'There are many ways to create a train/test and even validation samples.', 'You can use below code to create test and train samples :', 'No need to convert to numpy. Just use a pandas df to do the split and it will return a pandas df.', 'There are many valid answers. Adding one more to the bunch.\nfrom sklearn.cross_validation import train_test_split', 'You may also consider stratified division into training and testing set. Startified division also generates training and testing set randomly but in such a way that original class proportions are preserved. This makes training and testing sets better reflect the properties of the original dataset.', 'If you need to split your data with respect to the lables column in your data set you can use this:', 'You can use ~ (tilde operator) to exclude the rows sampled using df.sample(), letting pandas alone handle sampling and filtering of indexes, to obtain two sets.', 'To split into more than two classes such as train, test, and validation, one can do:', ""This is what I wrote when I needed to split a DataFrame. I considered using Andy's approach above, but didn't like that I could not control the size of the data sets exactly (i.e., it would be sometimes 79, sometimes 81, etc.)."", 'Just select range row from df like this', 'There are many great answers above so I just wanna add one more example in the case that you want to specify the exact number of samples for the train and test sets by using just the numpy library.', 'you need to convert pandas dataframe into numpy array and then convert numpy array back to dataframe', 'If your wish is to have one dataframe in and two dataframes out (not numpy arrays), this should do the trick:', 'You can make use of df.as_matrix() function and create Numpy-array and pass it.', 'A bit more elegant to my taste is to create a random column and then split by it, this way we can get a split that will suit our needs and will be random.', 'I think you also need to a get a copy not a slice of dataframe if you wanna add columns later.', 'How about this?\ndf is my dataframe']"
51,Pandas - How to flatten a hierarchical index in columns,"I have a data frame with a hierarchical index in axis 1 (columns) (from a groupby.agg operation):

     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       
                            ...",https://stackoverflow.com/questions/14507794/pandas-how-to-flatten-a-hierarchical-index-in-columns,"['I have a data frame with a hierarchical index in axis 1 (columns) (from a groupby.agg operation):', 'I think the easiest way to do this would be to set the columns to the top level:', 'All of the current answers on this thread must have been a bit dated. As of pandas version 0.24.0, the .to_flat_index() does what you need.', ""Andy Hayden's answer is certainly the easiest way -- if you want to avoid duplicate column labels you need to tweak a bit"", 'And if you want to retain any of the aggregation info from the second level of the multiindex you can try this:', 'The most pythonic way to do this to use map function.', 'The easiest and most intuitive solution for me was to combine the column names using get_level_values. This prevents duplicate column names when you do more than one aggregation on the same column:', 'After reading through all the answers, I came up with this:', 'A general solution that handles multiple levels and mixed types:', 'A bit late maybe, but if you are not worried about duplicate column names:', 'In case you want to have a separator in the name between levels, this function works well.', 'Following @jxstanford and @tvt173, I wrote a quick function which should do the trick, regardless of string/int column names:', ""I'll share a straight-forward way that worked for me."", 'You could also do as below. Consider df to be your dataframe and assume a two level index (as is the case in your example)', 'To flatten a MultiIndex inside a chain of other DataFrame methods, define a function like this:', 'Another simple routine.']"
52,Delete rows from a pandas DataFrame based on a conditional expression involving len(string) giving KeyError [duplicate],"I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.

I expect to be able to do this (per this answer):

df[(len(df['...",https://stackoverflow.com/questions/13851535/delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression-involving,"['I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.', ""When you do len(df['column name']) you are just getting one number, namely the number of rows in the DataFrame (i.e., the length of the column itself).  If you want to apply len to each element in the column, use df['column name'].map(len).  So try"", 'To directly answer this question\'s original title ""How to delete rows from a pandas DataFrame based on a conditional expression"" (which I understand is not necessarily the OP\'s problem but could help other users coming across this question) one way to do this is to use the drop method:', 'You can assign the DataFrame to a filtered version of itself:', ""I will expand on @User's generic solution to provide a drop free alternative. This is for folks directed here based on the question's title (not OP 's problem)"", 'In pandas you can do str.len with your boundary and using the Boolean result to filter it .', ""If you want to drop rows of data frame on the basis of some complicated condition on the column value then writing that in the way shown above can be complicated. I have the following simpler solution which always works. Let us assume that you want to drop the column with 'header' so get that column in a list first.""]"
53,How to store a dataframe using Pandas,Right now I'm importing a fairly large CSV as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend ...,https://stackoverflow.com/questions/17098654/how-to-store-a-dataframe-using-pandas,"[""Right now I'm importing a fairly large CSV as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?"", 'The easiest way is to pickle it using to_pickle:', 'Although there are already some answers I found a nice comparison in which they tried several ways to serialize Pandas DataFrames: Efficiently Store Pandas DataFrames.', ""If I understand correctly, you're already using pandas.read_csv() but would like to speed up the development process so that you don't have to load the file in every time you edit your script, is that right? I have a few recommendations:"", 'Pickle works good!', 'You can use feather format file. It is extremely fast.', 'As already mentioned there are different options and file formats (HDF5, JSON, CSV, parquet, SQL) to store a data frame. However, pickle is not a first-class citizen (depending on your setup), because:', 'Pandas DataFrames have the to_pickle function which is useful for saving a DataFrame:', ""I prefer to use numpy files since they're fast and easy to work with.\nHere's a simple benchmark for saving and loading a dataframe with 1 column of 1million points."", 'Another quite fresh test with to_pickle().', 'https://docs.python.org/3/library/pickle.html', ""Overall move has been to pyarrow/feather (deprecation warnings from pandas/msgpack).  However I have a challenge with pyarrow with transient in specification Data serialized with pyarrow 0.15.1 cannot be deserialized with 0.16.0 ARROW-7961. I'm using serialization to use redis so have to use a binary encoding."", 'Arctic is a high performance datastore for Pandas, numpy and other numeric data. It sits on top of MongoDB. Perhaps overkill for the OP, but worth mentioning for other folks stumbling across this post']"
54,pandas: filter rows of DataFrame with operator chaining,"Most operations in pandas can be accomplished with operator chaining (groupby, aggregate, apply, etc), but the only way I've found to filter rows is via normal bracket indexing

df_filtered = df[df['...",https://stackoverflow.com/questions/11869910/pandas-filter-rows-of-dataframe-with-operator-chaining,"[""Most operations in pandas can be accomplished with operator chaining (groupby, aggregate, apply, etc), but the only way I've found to filter rows is via normal bracket indexing"", ""I'm not entirely sure what you want, and your last line of code does not help either, but anyway:"", 'Filters can be chained using a Pandas query:', 'The answer from @lodagro is great. I would extend it by generalizing the mask function as:', 'Since version 0.18.1 the .loc method accepts a callable for selection. Together with lambda functions you can create very flexible chainable filters:', 'I offer this for additional examples.  This is the same answer as https://stackoverflow.com/a/28159296/', 'I had the same question except that I wanted to combine the criteria into an OR condition.  The format given by Wouter Overmeire combines the criteria into an AND condition such that both must be satisfied:', ""pandas provides two alternatives to Wouter Overmeire's answer which do not require any overriding. One is .loc[.] with a callable, as in"", 'My answer is similar to the others. If you do not want to create a new function you can use what pandas has defined for you already. Use the pipe method.', 'If you would like to apply all of the common boolean masks as well as a general purpose mask you can chuck the following in a file and then simply assign them all as follows:', 'This solution is more hackish in terms of implementation, but I find it much cleaner in terms of usage, and it is certainly more general than the others proposed.', 'This is unappealing as it requires I assign df to a variable before being able to filter on its values.', 'You can also leverage the numpy library for logical operations. Its pretty fast.', 'Just want to add a demonstration using loc to filter not only by rows but also by columns and some merits to the chained operation.', 'If you set your columns to search as indexes, then you can use DataFrame.xs() to take a cross section. This is not as versatile as the query answers, but it might be useful in some situations.']"
55,How to check whether a pandas DataFrame is empty?,How to check whether a pandas DataFrame is empty? In my case I want to print some message in terminal if the DataFrame is empty.,https://stackoverflow.com/questions/19828822/how-to-check-whether-a-pandas-dataframe-is-empty,"['How to check whether a pandas DataFrame is empty? In my case I want to print some message in terminal if the DataFrame is empty.', ""You can use the attribute df.empty to check whether it's empty or not:"", ""I use the len function. It's much faster than empty. len(df.index) is even faster."", 'I prefer going the long route. These are the checks I follow to avoid using a try-except clause -', ""To see if a dataframe is empty, I argue that one should test for the length of a dataframe's columns index:"", 'and the function:']"
56,Pandas conditional creation of a series/dataframe column,"I have a dataframe along the lines of the below:
    Type       Set
1    A          Z
2    B          Z           
3    B          X
4    C          Y

I want to add another column to the dataframe (...",https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column,"['I have a dataframe along the lines of the below:', 'If you only have two choices to select from:', 'List comprehension is another way to create another column conditionally. If you are working with object dtypes in columns, like in your example, list comprehensions typically outperform most other methods.', 'Another way in which this could be achieved is', ""Here's yet another way to skin this cat, using a dictionary to map new values onto the keys in the list:"", 'The following is slower than the approaches timed here, but we can compute the extra column based on the contents of more than one column, and more than two values can be computed for the extra column.', 'Maybe this has been possible with newer updates of Pandas (tested with pandas=1.0.5), but I think the following is the shortest and maybe best answer for the question, so far. You can use the .loc method and use one condition or several depending on your need.', 'One liner with .apply() method is following:', ""If you're working with massive data, a memoized approach would be best:""]"
57,Selecting/excluding sets of columns in pandas [duplicate],"I would like to create views or dataframes from an existing dataframe based on column selections.

For example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from ...",https://stackoverflow.com/questions/14940743/selecting-excluding-sets-of-columns-in-pandas,"['I would like to create views or dataframes from an existing dataframe based on column selections.', 'You can either Drop the columns you do not need OR Select the ones you need', 'There is a new index method called difference. It returns the original columns, with the columns passed as argument removed.', ""You don't really need to convert that into a set:"", 'Another option, without dropping or filtering in a loop:', 'Also have a look into the built-in DataFrame.filter function.', 'You just need to convert your set to a list', 'You have 4 columns A,B,C,D', ""Here's how to create a copy of a DataFrame excluding a list of columns:"", 'In a similar vein, when reading a file, one may wish to exclude columns upfront, rather than wastefully reading unwanted data into memory and later discarding them.']"
58,What is the most efficient way to loop through dataframes with pandas? [duplicate],"I want to perform my own complex operations on financial data in dataframes in a sequential manner.

For example I am using the following MSFT CSV file taken from Yahoo Finance:

Date,Open,High,Low,...",https://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas,"['I want to perform my own complex operations on financial data in dataframes in a sequential manner.', 'The newest versions of pandas now include a built-in function for iterating over rows.', 'Pandas is based on NumPy arrays.\nThe key to speed with NumPy arrays is to perform your operations on the whole array at once, never row-by-row or item-by-item.', 'Like what has been mentioned before, pandas object is most efficient when process the whole array at once. However for those who really need to loop through a pandas DataFrame to perform something, like me, I found at least three ways to do it. I have done a short test to see which one of the three is the least time consuming.', 'You can loop through the rows by transposing and then calling iteritems:', 'You have three options:', ""I checked out iterrows after noticing Nick Crawford's answer, but found that it yields (index, Series) tuples. Not sure which would work best for you, but I ended up using the itertuples method for my problem, which yields (index, row_value1...) tuples."", 'Just as a small addition, you can also do an apply if you have a complex function that you apply to a single column:', 'As @joris pointed out, iterrows is much slower than itertuples and itertuples is approximately 100 times fater than iterrows, and I tested speed of both methods in a DataFrame with 5027505 records the result is for iterrows, it is 1200it/s, and  itertuples is 120000it/s.', 'For sure, the fastest way to iterate over a dataframe is to access the underlying numpy ndarray either via df.values (as you do) or by accessing each column separately df.column_name.values. Since you want to have access to the index too, you can use df.index.values for that.', 'Another suggestion would be to combine groupby with vectorized calculations if subsets of the rows shared characteristics which allowed you to do so.']"
59,Pandas - Get first row value of a given column,"This seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting.

So, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested ...",https://stackoverflow.com/questions/25254016/pandas-get-first-row-value-of-a-given-column,"[""This seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting."", 'To select the ith row, use iloc:', 'Note that the answer from @unutbu will be correct until you want to set the value to something new, then it will not work if your dataframe is a view.', 'Another way to do this:', 'In a general way, if you want to pick up the first N rows from the J column from pandas dataframe the best way to do this is:', ""To get e.g the value from column 'test' and row 1 it works like"", 'Another way of getting the first row and preserving the index:']"
60,Convert Python dict into a dataframe,"I have a Python dictionary like the following:

{u'2012-06-08': 388,
 u'2012-06-09': 388,
 u'2012-06-10': 388,
 u'2012-06-11': 389,
 u'2012-06-12': 389,
 u'2012-06-13': 389,
 u'2012-06-14': 389,
 u'...",https://stackoverflow.com/questions/18837262/convert-python-dict-into-a-dataframe,"['I have a Python dictionary like the following:', 'The error here, is since calling the DataFrame constructor with scalar values (where it expects values to be a list/dict/... i.e. have multiple columns):', 'When converting a dictionary into a pandas dataframe where you want the keys to be the columns of said dataframe and the values to be the row values, you can do simply put brackets around the dictionary like this:', 'As explained on another answer using pandas.DataFrame() directly here will not act as you think.', 'Pass the items of the dictionary to the DataFrame constructor, and give the column names. After that parse the Date column to get Timestamp values.', '', 'Pandas have built-in function for conversion of dict to data frame.', 'This is what worked for me, since I wanted to have a separate index column', 'You can also just pass the keys and values of the dictionary to the new dataframe, like so:', 'In my case I wanted keys and values of a dict to be columns and values of DataFrame. So the only thing that worked for me was:', 'This is how it worked for me :', 'Accepts a dict as argument and returns a dataframe with the keys of the dict as index and values as a column.', ""If you don't encapsulate yourDict.keys() inside of list() , then you will end up with all of your keys and values being placed in every row of every column. Like this:"", 'I have run into this several times and have an example dictionary that I created from a function get_max_Path(),  and it returns the sample dictionary:', 'I think that you can make some changes in your data format when you create dictionary, then you can easily convert it to DataFrame:', 'The above answer seems the simplest and works. Copied below.']"
61,count the frequency that a value occurs in a dataframe column,"I have a dataset

|category|
cat a
cat b
cat a
I'd like to be able to return something like (showing unique values and frequency)

category | freq |
cat a       2
cat b       1",https://stackoverflow.com/questions/22391433/count-the-frequency-that-a-value-occurs-in-a-dataframe-column,"['I have a dataset', 'Use groupby and count:', 'If you want to apply to all columns you can use:', 'This short little line of code will give you the output you want.', 'value_counts - Returns object containing counts of unique values', 'In 0.18.1 groupby together with count does not give the frequency of unique values:', 'If your DataFrame has values with the same type, you can also set return_counts=True in numpy.unique().', 'Using list comprehension and value_counts for multiple columns in a df', 'Without any libraries, you could do this instead:', 'You can also do this with pandas by broadcasting your columns as categories first, e.g. dtype=""category"" e.g.', 'First unique value  count', ""@metatoaster has already pointed this out.\n Go for Counter. It's blazing fast."", 'solution:', 'I believe this should work fine for any DataFrame columns list.']"
62,Re-ordering columns in pandas dataframe based on column name [duplicate],"I have a dataframe with over 200 columns. The issue is as they were generated the order is

['Q1.3','Q6.1','Q1.2','Q1.1',......]
I need to re-order the columns as follows:

['Q1.1','Q1.2','Q1.3',........",https://stackoverflow.com/questions/11067027/re-ordering-columns-in-pandas-dataframe-based-on-column-name,"['I have a dataframe with over 200 columns. The issue is as they were generated the order is', ""This assumes that sorting the column names will give the order you want.  If your column names won't sort lexicographically (e.g., if you want column Q10.3 to appear after Q9.1), you'll need to sort differently, but that has nothing to do with pandas."", 'You can also do more succinctly:', 'You can just do:', ""Tweet's answer can be passed to BrenBarn's answer above with"", 'For several columns, You can put columns order what you want:', 'Don\'t forget to add ""inplace=True"" to Wes\' answer or set the result to a new DataFrame.', 'If you need an arbitrary sequence instead of sorted sequence, you could do:', 'The quickest method is:', 'The sort method and sorted function allow you to provide a custom function to extract the key used for comparison:', 'One use-case is that you have named (some of) your columns with some prefix, and you want the columns sorted with those prefixes all together and in some particular order (not alphabetical).', 'where by is the name of the column,if you want to sort the dataset based on column']"
63,get list from pandas dataframe column,"I have an excel document which looks like this..

cluster load_date   budget  actual  fixed_price
A   1/1/2014    1000    4000    Y
A   2/1/2014    12000   10000   Y
A   3/1/2014    36000   2000    Y
...",https://stackoverflow.com/questions/22341271/get-list-from-pandas-dataframe-column,"['I have an excel document which looks like this..', 'Pandas DataFrame columns are Pandas Series when you pull them out, which you can then call x.tolist() on to turn them into a Python list. Alternatively you cast it with list(x).', 'This returns a numpy array:', 'Numpy Array -> Panda Data Frame -> List from one Panda Column', 'As this question attained a lot of attention and there are several ways to fulfill your task, let me present several options.', 'If your column will only have one value something like pd.series.tolist() will produce an error. To guarantee that it will work for all cases, use the code below:', 'Assuming the name of the dataframe after reading the excel sheet is df, take an empty list (e.g. dataList), iterate through the dataframe row by row and append to your empty list like-']"
64,How to group dataframe rows into list in pandas groupby,"I have a pandas data frame df like:

a b
A 1
A 2
B 5
B 5
B 4
C 6
I want to group by the first column and get second column as lists in rows:

A [1,2]
B [5,5,4]
C [6]
Is it possible to do something ...",https://stackoverflow.com/questions/22219004/how-to-group-dataframe-rows-into-list-in-pandas-groupby,"['I have a pandas data frame df like:', 'You can do this using groupby to group on the column of interest and then apply list to every group:', 'A handy way to achieve this would be:', 'To solve this for several columns of a dataframe:', 'As you were saying the groupby method of a pd.DataFrame object can do the job.', 'Use any of the following groupby and agg recipes.', 'If looking for a unique list while grouping multiple columns this could probably help:', 'It is time to use agg instead of apply .', 'Let us using df.groupby with list and Series constructor', ""The easiest way I have see no achieve most of the same thing at least for one column which is similar to Anamika's answer just with the tuple syntax for the aggregate function."", 'Here I have grouped elements with ""|"" as a separator', ""Answer based on @EdChum's comment on his answer. Comment is this -""]"
65,"How to select all columns, except one column in pandas?","I have a dataframe look like this:

import pandas
import numpy as np
df = DataFrame(np.random.rand(4,4), columns = list('abcd'))
df
      a         b         c         d
0  0.418762  0.042369  0....",https://stackoverflow.com/questions/29763620/how-to-select-all-columns-except-one-column-in-pandas,"['I have a dataframe look like this:', 'When the columns are not a MultiIndex, df.columns is just an array of column names so you can do:', ""Don't use ix. It's deprecated. The most readable and idiomatic way of doing this is df.drop():"", 'You can use df.columns.isin()', 'Here is another way:', 'Another slight modification to @Salvador Dali enables a list of columns to exclude:', 'I think the best way to do is the way mentioned by @Salvador Dali. Not that the others are wrong.', 'Here is a one line lambda:', 'I think a nice solution is with the function filter of pandas and regex (match everything except ""b""):']"
66,"Converting between datetime, Timestamp and datetime64","How do I convert a numpy.datetime64 object to a datetime.datetime (or Timestamp)?

In the following code, I create a datetime, timestamp and datetime64 objects.

import datetime
import numpy as np
...",https://stackoverflow.com/questions/13703720/converting-between-datetime-timestamp-and-datetime64,"['How do I convert a numpy.datetime64 object to a datetime.datetime (or Timestamp)?', 'To convert numpy.datetime64 to datetime object that represents time in UTC on numpy-1.8:', 'You can just use the pd.Timestamp constructor. The following diagram may be useful for this and related questions.', 'Welcome to hell.', ""I think there could be a more consolidated effort in an answer to better explain the relationship between Python's datetime module, numpy's datetime64/timedelta64 and pandas' Timestamp/Timedelta objects."", 'For DatetimeIndex, the tolist returns a list of datetime objects. For a single datetime64 object it returns a single datetime object.', 'If you want to convert an entire pandas series of datetimes to regular python datetimes, you can also use .to_pydatetime().', 'One option is to use str, and then to_datetime (or similar):', ""This post has been up for 4 years and I still struggled with this conversion problem - so the issue is still active in 2017 in some sense. I was somewhat shocked that the numpy documentation does not readily offer a simple conversion algorithm but that's another story."", ""I've come back to this answer more times than I can count, so I decided to throw together a quick little class, which converts a Numpy datetime64 value to Python datetime value. I hope it helps others out there."", 'use this function to get pythons native datetime object', 'Some solutions work well for me but numpy will deprecate some parameters. \nThe solution that work better for me is to read the date as a pandas datetime and excract explicitly the year, month and day of a pandas object.\nThe following code works for the most common situation.', 'indeed, all of these datetime types can be difficult, and potentially problematic (must keep careful track of timezone information). here\'s what i have done, though i admit that i am concerned that at least part of it is ""not by design"". also, this can be made a bit more compact as needed.\nstarting with a numpy.datetime64 dt_a:']"
67,"Convert DataFrame column type from string to datetime, dd/mm/yyyy format",How can I convert a DataFrame column of strings (in dd/mm/yyyy format) to datetimes?,https://stackoverflow.com/questions/17134716/convert-dataframe-column-type-from-string-to-datetime-dd-mm-yyyy-format,"['How can I convert a DataFrame column of strings (in dd/mm/yyyy format) to datetimes?', 'The easiest way is to use to_datetime:', ""If your date column is a string of the format '2017-01-01'\nyou can use pandas astype to convert it to datetime."", 'You can use the following if you want to specify tricky formats:', ""If you have a mixture of formats in your date, don't forget to set infer_datetime_format=True to make life easier""]"
68,How to check if a column exists in Pandas,"Is there a way to check if a column exists in a Pandas DataFrame?

Suppose that I have the following DataFrame:

>>> import pandas as pd
>>> from random import randint
>>> ...",https://stackoverflow.com/questions/24870306/how-to-check-if-a-column-exists-in-pandas,"['Is there a way to check if a column exists in a Pandas DataFrame?', 'This will work:', 'To check if one or more columns all exist, you can use set.issubset, as in:', 'Just to suggest another way without using if statements, you can use the get() method for DataFrames. For performing the sum based on the question:']"
69,Python Pandas: Get index of rows which column matches certain value,"Given a DataFrame with a column ""BoolCol"", we want to find the indexes of the DataFrame in which the values for ""BoolCol"" == True

I currently have the iterating way to do it, which works perfectly:

...",https://stackoverflow.com/questions/21800169/python-pandas-get-index-of-rows-which-column-matches-certain-value,"['Given a DataFrame with a column ""BoolCol"", we want to find the indexes of the DataFrame in which the values for ""BoolCol"" == True', 'df.iloc[i] returns the ith row of df. i does not refer to the index label, i is a 0-based index.', 'Can be done using numpy where() function:', 'If you want to use your dataframe object only once, use:', 'Simple way is to reset the index of the DataFrame prior to filtering:', 'First you may check query when the target column is type bool  (PS: about how to use it please check link )', 'I extended this question that is how to gets the row, columnand value of all matches value?']"
70,Creating a Pandas DataFrame from a Numpy array: How do I specify the index column and column headers?,"I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:

data = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])
...",https://stackoverflow.com/questions/20763012/creating-a-pandas-dataframe-from-a-numpy-array-how-do-i-specify-the-index-colum,"['I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:', 'You need to specify data, index and columns to DataFrame constructor, as in:', 'Here is an easy to understand solution', 'I agree with Joris; it seems like you should be doing this differently, like with numpy record arrays. Modifying ""option 2"" from this great answer, you could do it like this:', 'This can be done simply by using from_records of pandas DataFrame', '', ""Adding to @behzad.nouri 's answer - we can create a helper routine to handle this common scenario:"", 'Here simple example to create pandas dataframe by using numpy array.', 'I think this is a simple and intuitive method:', ""It's not so short, but maybe can help you.""]"
71,Pandas count(distinct) equivalent,"I am using pandas as a db substitute as I have multiple databases (oracle, mssql, etc) and I am unable to make a sequence of commands to a SQL equivalent.

I have a table loaded in a DataFrame with ...",https://stackoverflow.com/questions/15411158/pandas-countdistinct-equivalent,"['I am using pandas as a db substitute as I have multiple databases (oracle, mssql, etc) and I am unable to make a sequence of commands to a SQL equivalent.', 'I believe this is what you want:', 'Here is another method, much simple, lets say your dataframe name is daat and column name is YEARMONTH', 'Interestingly enough, very often len(unique()) is a few times (3x-15x) faster than nunique().', ""I am also using nunique but it will be very helpful if you have to use an aggregate function like 'min', 'max', 'count' or 'mean' etc."", 'Using crosstab, this will return more information than groupby nunique', 'To get the distinct number of values for any column (CLIENTCODE in your case), we can use nunique. We can pass the input as a dictionary in agg function, along with aggregations on other columns:', 'With new pandas version, it is easy to get as dataframe', ""Here an approach to have count distinct over multiple columns. Let's have some data:""]"
72,Create Pandas DataFrame from a string,"In order to test some functionality I would like to create a DataFrame from a string. Let's say my test data looks like:

TESTDATA=""""""col1;col2;col3
1;4.4;99
2;4.5;200
3;4.7;65
4;3.2;140
""""""
What is ...",https://stackoverflow.com/questions/22604564/create-pandas-dataframe-from-a-string,"[""In order to test some functionality I would like to create a DataFrame from a string. Let's say my test data looks like:"", 'A simple way to do this is to use StringIO.StringIO (python2) or io.StringIO (python3) and pass that to the pandas.read_csv function. E.g:', 'Split Method', 'A quick and easy solution for interactive work is to copy-and-paste the text by loading the data from the clipboard.', ""This answer applies when a string is manually entered, not when it's read from somewhere."", 'Simplest way is to save it to temp file and then read it:']"
73,How can I use the apply() function for a single column?,I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. ...,https://stackoverflow.com/questions/34962104/how-can-i-use-the-apply-function-for-a-single-column,"['I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. How can I do that using apply in pandas?', 'Given a sample dataframe df as:', 'For a single column better to use map(), like this:', ""You don't need a function at all. You can work on a whole column directly."", 'Although the given responses are correct, they modify the initial data frame, which is not always desirable (and, given the OP asked for examples ""using apply"", it might be they wanted a version that returns a new data frame, as apply does).', 'If you are really concerned about the execution speed of your apply function and you have a huge dataset to work on, you could use swifter to make faster execution, here is an example for swifter on pandas dataframe:', 'Given the following dataframe df and the function complex_function,', ""Let me try a complex computation using datetime and considering nulls or empty spaces. I am reducing 30 years on a datetime column and using apply method as well as lambda and converting datetime format. Line if x != '' else x will take care of all empty spaces or nulls accordingly.""]"
74,How to add an empty column to a dataframe?,"What's the easiest way to add an empty column to a pandas DataFrame object?  The best I've stumbled upon is something like

df['foo'] = df.apply(lambda _: '', axis=1)
Is there a less perverse method?",https://stackoverflow.com/questions/16327055/how-to-add-an-empty-column-to-a-dataframe,"[""What's the easiest way to add an empty column to a pandas DataFrame object?  The best I've stumbled upon is something like"", 'If I understand correctly, assignment should fill:', ""To add to DSM's answer and building on this associated question, I'd split the approach into two cases:"", 'an even simpler solution is:', 'Starting with v0.16.0, DF.assign() could be used to assign new columns (single/multiple) to a DF. These columns get inserted in alphabetical order at the end of the DF.', 'I like:', 'if you want to add column name from a list', ""@emunsing's answer is really cool for adding multiple columns, but I couldn't get it to work for me in python 2.7.  Instead, I found this works:"", 'The below code address the question ""How do I add n number of empty columns to my existing dataframe"". In the interest of keeping solutions to similar problems in one place, I am adding it here.', 'You can do', 'One can use df.insert(index_to_insert_at, column_header, init_value) to insert new column at a specific index.', 'Sorry for I did not explain my answer really well at beginning. There is another way to add an new column to an existing dataframe.\n1st step, make a new empty data frame (with all the columns in your data frame, plus a new or few columns you want to add) called df_temp\n2nd step, combine the df_temp and your data frame.']"
75,What does axis in pandas mean?,"Here is my code to generate a dataframe:

import pandas as pd
import numpy as np

dff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))
then I got the dataframe:

+------------+---------+-------...",https://stackoverflow.com/questions/22149584/what-does-axis-in-pandas-mean,"['Here is my code to generate a dataframe:', ""It specifies the axis along which the means are computed. By default axis=0. This is consistent with the numpy.mean usage when axis is specified explicitly (in numpy.mean, axis==None by default, which computes the mean value over the flattened array) , in which axis=0 along the rows (namely, index in pandas), and axis=1 along the columns. For added clarity, one may choose to specify axis='index' (instead of axis=0) or axis='columns' (instead of axis=1)."", 'These answers do help explain this, but it still isn\'t perfectly intuitive for a non-programmer (i.e. someone like me who is learning Python for the first time in context of data science coursework). I still find using the terms ""along"" or ""for each"" wrt to rows and columns to be confusing.', ""Let's visualize (you gonna remember always),"", 'axis refers to the dimension of the array, in the case of pd.DataFrames axis=0 is the dimension that points downwards and axis=1 the one that points to the right.', 'The easiest way for me to understand is to talk about whether you are calculating a statistic for each column (axis = 0) or each row (axis = 1). If you calculate a statistic, say a mean, with axis = 0 you will get that statistic for each column. So if each observation is a row and each variable is in a column, you would get the mean of each variable. If you set axis = 1 then you will calculate your statistic for each row. In our example, you would get the mean for each observation across all of your variables (perhaps you want the average of related measures).', ""Let's look at the table from Wiki. This is an IMF estimate of GDP from 2010 to 2019 for top ten countries."", 'Axis in view of programming is the position in the shape tuple. Here is an example:', ""The designer of pandas, Wes McKinney, used to work intensively on finance data. Think of columns as stock names and index as daily prices. You can then guess what the default behavior is (i.e., axis=0) with respect to this finance data. axis=1 can be simply thought as 'the other direction'."", 'The problem with using axis= properly is for its use for 2 main different cases:', 'one of easy ways to remember axis 1 (columns), vs axis 0 (rows) is the output you expect.', ""This is based on @Safak's answer.\nThe best way to understand the axes in pandas/numpy is to create a 3d array and check the result of the sum function along the 3 different axes."", 'I understand this way :', 'axis = 0 means up to down \naxis = 1 means left to right', ""I will explicitly avoid using 'row-wise' or 'along the columns', since people may interpret them in exactly the wrong way."", '', 'My thinking : Axis = n, where n = 0, 1, etc. means that the matrix is collapsed (folded) along that axis. So in a 2D matrix, when you collapse along 0 (rows), you are really operating on one column at a time. Similarly for higher order matrices.', ""I'm a newbie to pandas. But this is how I understand axis in pandas:"", 'I think there is an another way to understand it.', 'I have been trying to figure out the axis for the last hour as well. The language in all the above answers, and also the documentation is not at all helpful.', 'Many answers here helped me a lot!', 'Arrays are designed with so-called axis=0 and rows positioned vertically versus axis=1 and columns positioned horizontally. Axis refers to the dimension of the array.']"
76,Combining two Series into a DataFrame in pandas,I have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column?,https://stackoverflow.com/questions/18062135/combining-two-series-into-a-dataframe-in-pandas,"['I have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column?', 'I think concat is a nice way to do this. If they are present it uses the name attributes of the Series as the columns (otherwise it simply numbers them):', ""Why don't you just use .to_frame if both have the same indexes?"", 'Pandas will automatically align these passed in series and create the joint index\nThey happen to be the same here. reset_index moves the index to a column.', 'Example code:', 'If I may answer this.', 'Not sure I fully understand your question, but is this what you want to do?', 'A simplification of the solution based on join():', ""I used pandas to convert my numpy array or iseries to an dataframe then added and additional the additional column by key as 'prediction'. If you need dataframe converted back to a list then use values.tolist()"", ""If you are trying to join Series of equal length but their indexes don't match (which is a common scenario), then concatenating them will generate NAs wherever they don't match.""]"
77,How to add pandas data to an existing csv file?,I want to know if it is possible to use the pandas to_csv() function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data.,https://stackoverflow.com/questions/17530542/how-to-add-pandas-data-to-an-existing-csv-file,"['I want to know if it is possible to use the pandas to_csv() function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data.', ""You can specify a python write mode in the pandas to_csv function. For append it is 'a'."", 'You can append to a csv by opening the file in append mode:', 'A little helper function I use with some header checking safeguards to handle it all:', ""Initially starting with a pyspark dataframes - I got type conversion errors (when converting to pandas df's and then appending to csv) given the schema/column types in my pyspark dataframes"", ""A bit late to the party but you can also use a context manager, if you're opening and closing your file multiple times, or logging data, statistics, etc.""]"
78,how to sort pandas dataframe from one column,"I have a data frame like this: 

print(df)

        0          1     2
0   354.7      April   4.0
1    55.4     August   8.0
2   176.5   December  12.0
3    95.5   February   2.0
4    85.6    January  ...",https://stackoverflow.com/questions/37787698/how-to-sort-pandas-dataframe-from-one-column,"['I have a data frame like this:', ""Use sort_values to sort the df by a specific column's values:"", 'I tried the solutions above and I do not achieve results, so I found a different solution that works for me. The ascending=False is to order the dataframe in descending order, by default it is True. I am using python 3.6.6 and pandas 0.23.4 versions.', 'Just as another solution:', 'Using column name worked for me.', 'Just adding some more operations on data. Suppose we have a dataframe df, we can do several operations to get desired outputs', 'Here is template of sort_values according to pandas documentation.']"
79,How to sort a dataFrame in python pandas by two or more columns?,"Suppose I have a dataframe with columns a, b and c, I want to sort the dataframe by column b in ascending order, and by column c in descending order, how do I do this?",https://stackoverflow.com/questions/17141558/how-to-sort-a-dataframe-in-python-pandas-by-two-or-more-columns,"['Suppose I have a dataframe with columns a, b and c, I want to sort the dataframe by column b in ascending order, and by column c in descending order, how do I do this?', 'As of the 0.17.0 release, the sort method was deprecated in favor of sort_values.  sort was completely removed in the 0.20.0 release. The arguments (and results) remain the same:', 'As of pandas 0.17.0, DataFrame.sort() is deprecated, and set to be removed in a future version of pandas. The way to sort a dataframe by its values is now is DataFrame.sort_values', 'For large dataframes of numeric data, you may see a significant performance improvement via numpy.lexsort, which performs an indirect sort using a sequence of keys:']"
80,Remove rows with duplicate indices (Pandas DataFrame and TimeSeries),"I'm reading some automated weather data from the web. The observations occur every 5 minutes and are compiled into monthly files for each weather station. Once I'm done parsing a file, the DataFrame ...",https://stackoverflow.com/questions/13035764/remove-rows-with-duplicate-indices-pandas-dataframe-and-timeseries,"[""I'm reading some automated weather data from the web. The observations occur every 5 minutes and are compiled into monthly files for each weather station. Once I'm done parsing a file, the DataFrame looks something like this:"", 'I would suggest using the duplicated method on the Pandas Index itself:', 'This adds the index as a dataframe column, drops duplicates on that, then removes the new column:', 'Oh my. This is actually so simple!', ""Unfortunately, I don't think Pandas allows one to drop dups off the indices. I would suggest the following:"", 'Remove duplicates (Keeping First)', 'If anyone like me likes chainable data manipulation using the pandas dot notation (like piping), then the following may be useful:']"
81,How to take column-slices of dataframe in pandas,"I load some machine learning data from a CSV file. The first 2 columns are observations and the remaining columns are features.

Currently, I do the following:

data = pandas.read_csv('mydata.csv')
...",https://stackoverflow.com/questions/10665889/how-to-take-column-slices-of-dataframe-in-pandas,"['I load some machine learning data from a CSV file. The first 2 columns are observations and the remaining columns are features.', 'See the deprecation in the docs', 'Note: .ix has been deprecated since Pandas v0.20. You should instead use .loc or .iloc, as appropriate.', 'Lets use the titanic dataset from the seaborn package as an example', 'Also, Given a DataFrame', 'You can slice along the columns of a DataFrame by referring to the names of each column in a list, like so:', 'And if you came here looking for slicing two ranges of columns and combining them together (like me) you can do something like', ""Here's how you could use different methods to do selective column slicing, including selective label based, index based and the selective ranges based column slicing."", 'Its equivalent', 'if Data frame look like that:', ""Another way to get a subset of columns from your DataFrame, assuming you want all the rows, would be to do:\ndata[['a','b']] and data[['c','d','e']]\nIf you want to use numerical column indexes you can do:\ndata[data.columns[:2]] and data[data.columns[2:]]""]"
82,What does `ValueError: cannot reindex from a duplicate axis` mean?,"I am getting a ValueError: cannot reindex from a duplicate axis when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.

Here is my ...",https://stackoverflow.com/questions/27236275/what-does-valueerror-cannot-reindex-from-a-duplicate-axis-mean,"['I am getting a ValueError: cannot reindex from a duplicate axis when I am trying to set an index to a certain value. I tried to reproduce this with a simple example, but I could not do it.', 'This error usually rises when you join / assign to a column when the index has duplicate values. Since you are assigning to a row, I suspect that there is a duplicate value in affinity_matrix.columns, perhaps not shown in your question.', ""As others have said, you've probably got duplicate values in your original index. To find them do this:"", ""Indices with duplicate values often arise if you create a DataFrame by concatenating other DataFrames. IF you don't care about preserving the values of your index, and you want them to be unique values, when you concatenate the the data, set ignore_index=True."", 'For people who are still struggling with this error, it can also happen if you accidentally create a duplicate column with the same name.  Remove duplicate columns like so:', 'Simply skip the error using .values at the end.', 'I came across this error today when I wanted to add a new column like this', 'In my case, this error popped up not because of duplicate values, but because I attempted to join a shorter Series to a Dataframe: both had the same index, but the Series had fewer rows (missing the top few). The following worked for my purposes:', 'Simple Fix that Worked for Me', 'I wasted couple of hours on the same issue. In my case, I had to reset_index() of a dataframe before using apply function.\nBefore merging, or looking up from another indexed dataset, you need to reset the index as 1 dataset can have only 1 Index.']"
83,How to drop a list of rows from Pandas dataframe?,"I have a dataframe df :

>>> df
                  sales  discount  net_sales    cogs
STK_ID RPT_Date                                     
600141 20060331   2.709       NaN      2.709   2.245
 ...",https://stackoverflow.com/questions/14661701/how-to-drop-a-list-of-rows-from-pandas-dataframe,"['I have a dataframe df :', 'Use DataFrame.drop and pass it a Series of index labels:', 'Note that it may be important to use the ""inplace"" command when you want to do the drop in line.', 'If the DataFrame is huge, and the number of rows to drop is large as well, then simple drop by index df.drop(df.index[]) takes too much time.', 'You can also pass to DataFrame.drop the label itself (instead of Series of index labels):', 'I solved this in a simpler way - just in 2 steps.', ""If I want to drop a row which has let's say index x, I would do the following:"", 'Here is a bit specific example, I would like to show. Say you have many duplicate entries in some of your rows. If you have string entries you could easily use string methods to find all indexes to drop.', ""In a comment to @theodros-zelleke's answer, @j-jones asked about what to do if the index is not unique.  I had to deal with such a situation.  What I did was to rename the duplicates in the index before I called drop(), a la:"", 'Determining the index from the boolean as described above e.g.', 'Use only the Index arg to drop row:-', 'Look at the following dataframe df', 'Consider an example dataframe']"
84,Pandas index column title or name,"How do I get the index column name in python pandas?  Here's an example dataframe:

             Column 1
Index Title          
Apples              1
Oranges             2
Puppies             3
Ducks  ...",https://stackoverflow.com/questions/18022845/pandas-index-column-title-or-name,"[""How do I get the index column name in python pandas?  Here's an example dataframe:"", 'You can just get/set the index via its name property', 'You can use rename_axis, for removing set to None:', 'df.index.name should do the trick.', ""Use df.index.rename('foo', inplace=True) to set the index name."", 'If you do not want to create a new row but simply put it in the empty cell then use:', 'df.columns.values also give us the column names', 'Setting the index name can also be accomplished at creation:', ""The solution for multi-indexes is inside jezrael's cyclopedic answer, but it took me a while to find it so I am posting a new answer:"", 'To just get the index column names df.index.names will work for both a single Index or MultiIndex as of the most recent version of pandas.']"
85,Convert Pandas Column to DateTime,"I have one field in a pandas DataFrame that was imported as string format. 
It should be a datetime variable.
How do I convert it to a datetime column and then filter based on date.

Example:
...",https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime,"['I have one field in a pandas DataFrame that was imported as string format. \nIt should be a datetime variable.\nHow do I convert it to a datetime column and then filter based on date.', 'Use the to_datetime function, specifying a format to match your data.', 'You can use the DataFrame method .apply() to operate on the values in Mycol:', 'If you have more than one column to be converted you can do the following:', 'works, however it results in a Python warning of \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead', 'Use the pandas to_datetime function to parse the column as DateTime. Also, by using infer_datetime_format=True, it will automatically detect the format and convert the mentioned column to DateTime.']"
86,How do I convert a pandas Series or index to a Numpy array? [duplicate],Do you know how to get the index or column of a DataFrame as a NumPy array or python list?,https://stackoverflow.com/questions/17241004/how-do-i-convert-a-pandas-series-or-index-to-a-numpy-array,"['Do you know how to get the index or column of a DataFrame as a NumPy array or python list?', 'To get a NumPy array, you should use the values attribute:', ""You can use df.index to access the index object and then get the values in a list using df.index.tolist(). Similarly, you can use df['col'].tolist() for Series."", 'From v0.24.0 onwards, we will have two brand spanking new, preferred methods for obtaining NumPy arrays from Index, Series, and DataFrame objects: they are to_numpy(), and .array. Regarding usage, the docs mention:', 'If you are dealing with a multi-index dataframe, you may be interested in extracting only the column of one name of the multi-index. You can do this as', 'Since pandas v0.13 you can also use get_values:', 'I converted the pandas dataframe to list and then used the basic list.index(). Something like this:', 'A more recent way to do this is to use the .to_numpy() function.', 'Below is a simple way to convert dataframe column into numpy array.']"
87,Get the Row(s) which have the max count in groups using groupby,"How do I find all rows in a pandas dataframe which have the max value for count column, after grouping by ['Sp','Mt'] columns?

Example 1: the following dataFrame, which I group by ['Sp','Mt']:

   Sp ...",https://stackoverflow.com/questions/15705630/get-the-rows-which-have-the-max-count-in-groups-using-groupby,"[""How do I find all rows in a pandas dataframe which have the max value for count column, after grouping by ['Sp','Mt'] columns?"", 'To get the indices of the original DF you can do:', ""You can sort the dataFrame by count and then remove duplicates. I think it's easier:"", 'Easy solution would be to apply : idxmax() function to get indices of rows with max values. \nThis would filter out all the rows with max value in the group.', 'Having tried the solution suggested by Zelazny on a relatively large DataFrame (~400k rows) I found it to be very slow.  Here is an alternative that I found to run orders of magnitude faster on my data set.', 'You may not need to do with group by , using sort_values+ drop_duplicates', 'For me, the easiest solution would be keep value when count is equal to the maximum. Therefore, the following one line command is enough :', 'Use groupby and idxmax methods:', 'Realizing that ""applying"" ""nlargest"" to groupby object works just as fine:', 'Try using ""nlargest"" on the groupby object. The advantage of using nlargest is that it returns the index of the rows where ""the nlargest item(s)"" were fetched from. \nNote: we slice the second(1) element of our index since our index in this case consist of tuples(eg.(s1, 0)).', ""I've been using this functional style for many group operations:""]"
88,Show DataFrame as table in iPython Notebook,"I am using iPython notebook.  When I do this:

df
I get a beautiful table with cells.  However, if i do this:

df1
df2 
it doesn't print the first beautiful table.  If I try this:

print df1
print ...",https://stackoverflow.com/questions/26873127/show-dataframe-as-table-in-ipython-notebook,"['I am using iPython notebook.  When I do this:', ""You'll need to use the HTML() or display() functions from IPython's display module:"", 'This answer is based on the 2nd tip from this blog post: 28 Jupyter Notebook tips, tricks and shortcuts', 'I prefer not messing with HTML and use as much as native infrastructure as possible. You can use Output widget with Hbox or VBox:', ""It seems you can just display both dfs using a comma in between in display.\nI noticed this on some notebooks on github. This code is from Jake VanderPlas's notebook."", 'In order to show the DataFrame in Jupyter Notebook just type:', 'To display dataframes contained in a list:']"
89,Pandas: drop a level from a multi-level column index?,"If I've got a multi-level column index:

>>> cols = pd.MultiIndex.from_tuples([(""a"", ""b""), (""a"", ""c"")])
>>> pd.DataFrame([[1,2], [3,4]], columns=cols)
    a
   ---+--
    b | c
--+--...",https://stackoverflow.com/questions/22233488/pandas-drop-a-level-from-a-multi-level-column-index,"[""If I've got a multi-level column index:"", 'You can use MultiIndex.droplevel:', 'Another way to drop the index is to use a list comprehension:', 'Another way to do this is to reassign df based on a cross section of df, using the .xs method.', 'As of Pandas 0.24.0, we can now use DataFrame.droplevel():', 'You could also achieve that by renaming the columns:', 'A small trick using sum  with level=1(work when level=1 is all unique)', 'I have struggled with this problem since I don’t know why my droplevel() function does not work. Work through several and learn that ‘a’ in your table is columns name and ‘b’, ‘c’ are index. Do like this will help']"
90,How to find the installed pandas version,I am having trouble with some of pandas functionalities. How do I check what is my installation version?,https://stackoverflow.com/questions/20612645/how-to-find-the-installed-pandas-version,"['I am having trouble with some of pandas functionalities. How do I check what is my installation version?', 'Check pandas.__version__:', 'Run:', 'Simplest Solution', 'Run', 'Windows', 'In a jupyter notebook cell: pip freeze | grep pandas']"
91,Normalize columns of pandas data frame,"I have a dataframe in pandas where each column has different value range. For example:

df:

A     B   C
1000  10  0.5
765   5   0.35
800   7   0.09
Any idea how I can normalize the columns of this ...",https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame,"['I have a dataframe in pandas where each column has different value range. For example:', 'You can use the package sklearn and its associated preprocessing utilities to normalize the data.', 'one easy way by using Pandas: (here I want to use mean normalization)', 'Based on this post: https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range', 'Your problem is actually a simple transform acting on the columns:', 'If you like using the sklearn package, you can keep the column and index names by using pandas loc like so:', 'Simple is Beautiful:', 'You can create a list of columns that you want to normalize', 'Example of different standardizations in python.', 'I think that a better way to do that in pandas is just', 'The solution given by Sandman and Praveen is very well. The only problem with that if you have categorical variables in other columns of your data frame this method will need some adjustments.', ""You might want to have some of columns being normalized and the others be unchanged like some of regression tasks which data labels or categorical columns are unchanged So I suggest you this pythonic way (It's a combination of @shg and @Cina answers ):"", 'It is only simple mathematics. The answer should as simple as below.', 'This is how you do it column-wise using list comprehension:', 'From the document of pandas,DataFrame structure can apply an operation (function) to itself .', 'The following function calculates the Z score:', 'You can simply use the pandas.DataFrame.transform1 function in this way:', 'You can do this in one line', 'Pandas does column wise normalization by default. Try the code below.', 'If your data is positively skewed, the best way to normalize is to use the log transformation:']"
92,pandas get rows which are NOT in other dataframe,"I've two pandas data frames which have some rows in common.

Suppose dataframe2 is a subset of dataframe1.

How can I get the rows of dataframe1 which are not in dataframe2?

df1 = pandas.DataFrame(...",https://stackoverflow.com/questions/28901683/pandas-get-rows-which-are-not-in-other-dataframe,"[""I've two pandas data frames which have some rows in common."", ""One method would be to store the result of an inner merge form both dfs, then we can simply select the rows when one column's values are not in this common:"", 'The currently selected solution produces incorrect results. To correctly solve this problem, we can perform a left-join from df1 to df2, making sure to first get just the unique rows for df2.', 'Assuming that the indexes are consistent in the dataframes (not taking into account the actual col values):', 'As already hinted at, isin requires columns and indices to be the same for a match. If match should only be on row contents, one way to get the mask for filtering the rows present is to convert the rows to a (Multi)Index:', 'Suppose you have two dataframes, df_1 and df_2 having multiple fields(column_names) and you want to find the only those entries in df_1 that are not in df_2 on the basis of some fields(e.g. fields_x, fields_y), follow the following steps.', 'a bit late, but it might be worth checking the ""indicator"" parameter of pd.merge.', 'you can do it using isin(dict) method:', 'You can also concat df1, df2:', 'How about this:', 'Here is another way of solving this:', 'My way of doing this involves adding a new column that is unique to one dataframe and using this to choose whether to keep an entry', ""Note that drop duplicated is used to minimize the comparisons. It would work without them as well. The best way is to compare the row contents themselves and not the index or one/two columns and same code can be used for other filters like 'both' and 'right_only' as well to achieve similar results. For this syntax dataframes can have any number of columns and even different indices. Only the columns should occur in both the dataframes.""]"
93,How to select rows with one or more nulls from a pandas DataFrame without listing columns explicitly?,"I have a dataframe with ~300K rows and ~40 columns.
I want to find out if any rows contain null values - and put these 'null'-rows into a separate dataframe so that I could explore them easily.

I can ...",https://stackoverflow.com/questions/14247586/how-to-select-rows-with-one-or-more-nulls-from-a-pandas-dataframe-without-listin,"[""I have a dataframe with ~300K rows and ~40 columns.\nI want to find out if any rows contain null values - and put these 'null'-rows into a separate dataframe so that I could explore them easily."", '[Updated to adapt to modern pandas, which has isnull as a method of DataFrames..]', 'then when ever you need it  you can type:', "".any() and .all() are great for the extreme cases, but not when you're looking for a specific number of null values. Here's an extremely simple way to do what I believe you're asking. It's pretty verbose, but functional."", 'If you want to filter rows by a certain number of columns with null values, you may use this:', 'You can do smth as:']"
94,Get column index from column name in python pandas,"In R when you need to retrieve a column index based on the name of the column you could do

idx <- which(names(my_data)==my_colum_name)
Is there a way to do the same with pandas dataframes?",https://stackoverflow.com/questions/13021654/get-column-index-from-column-name-in-python-pandas,"['In R when you need to retrieve a column index based on the name of the column you could do', 'Sure, you can use .get_loc():', 'Here is a solution through list comprehension. cols is the list of columns to get index for:', ""DSM's solution works, but if you wanted a direct equivalent to which you could do (df.columns == name).nonzero()"", 'When you might be looking to find multiple column matches, a vectorized solution using searchsorted method could be used. Thus, with df as the dataframe and query_cols as the column names to be searched for, an implementation would be -', 'In case you want the column name from the column location (the other way around to the OP question), you can use:', 'how about this:', 'For returning multiple column indices, I recommend using the pandas.Index method get_indexer, if you have unique labels:']"
95,Pandas read in table without headers,"How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do usecols",https://stackoverflow.com/questions/29287224/pandas-read-in-table-without-headers,"['How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do usecols', ""In order to read a csv in that doesn't have a header and for only certain columns you need to pass params header=None and usecols=[3,6] for the 4th and 7th columns:"", 'Previous answers were good and correct, but in my opinion, an extra names parameter will make it perfect, and it should be the recommended way, especially when the csv has no headers.', 'Make sure you specify pass header=None and add usecols=[3,6] for the 4th and 7th columns.']"
96,Count unique values with pandas per groups [duplicate],"I need to count unique ID values in every domain
I have data

ID, domain
123, 'vk.com'
123, 'vk.com'
123, 'twitter.com'
456, 'vk.com'
456, 'facebook.com'
456, 'vk.com'
456, 'google.com'
789, 'twitter....",https://stackoverflow.com/questions/38309729/count-unique-values-with-pandas-per-groups,"['I need to count unique ID values in every domain\nI have data', 'You need nunique:', 'Generally to count distinct values in single column, you can use Series.value_counts:', 'df.domain.value_counts()', 'IIUC you want the number of different ID for every domain, then you can try this:']"
97,Extracting just Month and Year separately from Pandas Datetime column,"I have a Dataframe, df, with the following column:

df['ArrivalDate'] =
...
936   2012-12-31
938   2012-12-29
965   2012-12-31
966   2012-12-31
967   2012-12-31
968   2012-12-31
969   2012-12-31
970   ...",https://stackoverflow.com/questions/25146121/extracting-just-month-and-year-separately-from-pandas-datetime-column,"['I have a Dataframe, df, with the following column:', 'If you want new columns showing year and month separately you can do this:', 'Best way found!!', 'You can directly access the year and month attributes, or request a datetime.datetime:', 'If you want the month year unique pair, using apply is pretty sleek.', ""Extracting the Year say from ['2018-03-04']"", 'You can first convert your date strings with pandas.to_datetime, which gives you access to all of the numpy datetime and timedelta facilities. For example:', ""@KieranPC's solution is the correct approach for Pandas, but is not easily extendible for arbitrary attributes. For this, you can use getattr within a generator comprehension and combine using pd.concat:"", 'Thanks to jaknap32, I wanted to aggregate the results according to Year and Month, so this worked:', ""SINGLE LINE: Adding a column with 'year-month'-paires:\n('pd.to_datetime' first changes the column dtype to date-time before the operation)"", ""This worked fine for me, didn't think pandas would interpret the resultant string date as date, but when i did the plot, it knew very well my agenda and the string year_month where ordered properly... gotta love pandas!"", 'There is two steps to extract year for all the dataframe without using method apply.']"
98,How to display full (non-truncated) dataframe information in html when converting from pandas dataframe to html?,"I converted a pandas dataframe to an html output using the DataFrame.to_html function. When I save this to a separate html file, the file shows truncated output.

For example, in my TEXT column, 

df....",https://stackoverflow.com/questions/25351968/how-to-display-full-non-truncated-dataframe-information-in-html-when-convertin,"['I converted a pandas dataframe to an html output using the DataFrame.to_html function. When I save this to a separate html file, the file shows truncated output.', 'Set the display.max_colwidth option to -1:', 'id (second argument) can fully show the columns.', ""While pd.set_option('display.max_columns', None) sets the number of the maximum columns shown, the option pd.set_option('display.max_colwidth', -1) sets the maximum width of each single field."", 'The following code results in the error below:', 'For those looking to do this in dask. I could not find a similar option in dask but if I simply do this in same notebook for pandas it works for dask too.', 'Whenever I need this for just one cell, I use this:', ""For those who like to reduce typing (i.e., everyone!): pd.set_option('max_colwidth', None) does the same thing""]"
99,Selecting with complex criteria from pandas.DataFrame,"For example I have simple DF:

import pandas as pd
from random import randint

df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],
                   'B': [randint(1, 9)*10 for x in xrange(10)...",https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe,"['For example I have simple DF:', 'Sure!  Setup:', 'Another solution is to use the query method:', 'And remember to use parenthesis!', 'You can use pandas it has some built in functions for comparison. So if you want to select values of ""A"" that are met by the conditions of ""B"" and ""C"" (assuming you want back a DataFrame pandas object)']"
100,How can I obtain the element-wise logical NOT of a pandas Series?,"I have a pandas Series object containing boolean values. How can I get a series containing the logical NOT of each value?

For example, consider a series containing:

True
True
True
False
The series ...",https://stackoverflow.com/questions/15998188/how-can-i-obtain-the-element-wise-logical-not-of-a-pandas-series,"['I have a pandas Series object containing boolean values. How can I get a series containing the logical NOT of each value?', 'To invert a boolean Series, use ~s:', ""@unutbu's answer is spot on, just wanted to add a warning that your mask needs to be dtype bool, not 'object'.  Ie your mask can't have ever had any nan's. See here - even if your mask is nan-free now, it will remain 'object' type."", 'I just give it a shot:', 'You can also use numpy.invert:', 'NumPy is slower because it casts the input to boolean values (so None and 0 becomes False and everything else becomes True).', 'In support to the excellent answers here, and for future convenience, there may be a case where you want to flip the truth values in the columns and have other values remain the same (nan values for instance)']"
101,Apply multiple functions to multiple groupby columns,"The docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:

In [563]: grouped['D'].agg({'result1' : np.sum,
   .....:            ...",https://stackoverflow.com/questions/14529838/apply-multiple-functions-to-multiple-groupby-columns,"['The docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:', 'The second half of the currently accepted answer is outdated and has two deprecations. First and most important, you can no longer pass a dictionary of dictionaries to the agg groupby method. Second, never use .ix.', 'For the first part you can pass a dict of column names for keys and a list of functions for the values:', ""As an alternative (mostly on aesthetics) to Ted Petrou's answer, I found I preferred a slightly more compact listing. Please don't consider accepting it, it's just a much-more-detailed comment on Ted's answer, plus code/data. Python/pandas is not my first/best, but I found this to read well:"", 'Since pandas version 0.25.0 or higher, we are moving away from the dictionary based aggregation and renaming, and moving towards named aggregations which accepts a tuple. Now we can simultaneously aggregate + rename to a more informative column name:', 'New in version 0.25.0.', ""Ted's answer is amazing. I ended up using a smaller version of that in case anyone is interested. Useful when you are looking for one aggregation that depends on values from multiple columns:"", ""This is a twist on 'exans' answer that uses Named Aggregations. It's the same but with argument unpacking which allows you to still pass in a dictionary to the agg function.""]"
102,Why isn't my Pandas 'apply' function referencing multiple columns working? [closed],"I have some problems with the Pandas apply function, when using multiple columns with the following dataframe

df = DataFrame ({'a' : np.random.randn(6),
                 'b' : ['foo', 'bar'] * 3,
    ...",https://stackoverflow.com/questions/16353729/why-isnt-my-pandas-apply-function-referencing-multiple-columns-working,"[""Want to improve this question? Update the question so it's on-topic for Stack Overflow."", ""Seems you forgot the '' of your string."", ""If you just want to compute (column a) % (column b), you don't need apply, just do it directly:"", ""Let's say we want to apply a function add5 to columns 'a' and 'b' of DataFrame df"", 'All of the suggestions above work, but if you want your computations to by more efficient, you should take advantage of numpy vector operations (as pointed out here).', 'This is same as the previous solution but I have defined the function in df.apply itself:', 'I have given the comparison of all three discussed above.']"
103,Pandas Replace NaN with blank/empty string,"I have a Pandas Dataframe as shown below:

    1    2       3
 0  a  NaN    read
 1  b    l  unread
 2  c  NaN    read
I want to remove the NaN values with an empty string so that it looks like so:

 ...",https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string,"['I have a Pandas Dataframe as shown below:', 'This might help. It will replace all NaNs with an empty string.', 'or just', 'If you are reading the dataframe from a file (say CSV or Excel) then use :', 'Use a formatter, if you only want to format it so that it renders nicely when printed. Just use the df.to_string(... formatters to define custom string-formatting, without needlessly modifying your DataFrame or wasting memory:', 'Try this,', 'using keep_default_na=False  should help you:', 'If you are converting DataFrame to JSON, NaN will give error so best solution is in this use case is to replace NaN with None.\nHere is how:', 'I tried with one column of string values with nan.']"
104,Pandas group-by and sum,"I am using this data frame:

Fruit   Date      Name  Number
Apples  10/6/2016 Bob    7
Apples  10/6/2016 Bob    8
Apples  10/6/2016 Mike   9
Apples  10/7/2016 Steve 10
Apples  10/7/2016 Bob    1
...",https://stackoverflow.com/questions/39922986/pandas-group-by-and-sum,"['I am using this data frame:', 'Use GroupBy.sum:', 'Also you can use agg function,', 'If you want to keep the original columns Fruit and Name, use reset_index(). Otherwise Fruit and Name will become part of the index.', 'Both the other answers accomplish what you want.', 'You can select different columns to sum numbers.', 'You can set the groupby column to  index  then using sum with level', 'A variation on the .agg() function; provides the ability to (1) persist type DataFrame, (2) apply averages, counts, summations, etc. and (3) enables groupby on multiple columns while maintaining legibility.']"
105,dropping infinite values from dataframes in pandas?,"what is the quickest/simplest way to drop nan and inf/-inf values from a pandas DataFrame without resetting mode.use_inf_as_null? I'd like to be able to use the subset and how arguments of dropna, ...",https://stackoverflow.com/questions/17477979/dropping-infinite-values-from-dataframes-in-pandas,"[""what is the quickest/simplest way to drop nan and inf/-inf values from a pandas DataFrame without resetting mode.use_inf_as_null? I'd like to be able to use the subset and how arguments of dropna, except with inf values considered missing, like:"", 'The simplest way would be to first replace infs to NaN:', 'With option context, this is possible without permanently setting use_inf_as_na. For example:', 'Here is another method using .loc to replace inf with nan on a Series:', 'Use (fast and simple):', 'Yet another solution would be to use the isin method. Use it to determine whether each value is infinite or missing and then chain the all method to determine if all the values in the rows are infinite or missing.', 'The above solution will modify the infs that are not in the target columns. To remedy that,', 'You can use pd.DataFrame.mask with np.isinf. You should ensure first your dataframe series are all of type float. Then use dropna with your existing logic.']"
106,Update a dataframe in pandas while iterating row by row,"I have a pandas data frame that looks like this (its a pretty big one)

           date      exer exp     ifor         mat  
1092  2014-03-17  American   M  528.205  2014-04-19 
1093  2014-03-17  ...",https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row,"['I have a pandas data frame that looks like this (its a pretty big one)', 'You can assign values in the loop using df.set_value:', ""Pandas DataFrame object should be thought of as a Series of Series.  In other words, you should think of it in terms of columns.  The reason why this is important is because when you use pd.DataFrame.iterrows you are iterating through rows as Series.  But these are not the Series that the data frame is storing and so they are new Series that are created for you while you iterate.  That implies that when you attempt to assign tho them, those edits won't end up reflected in the original data frame."", 'A method you can use is itertuples(), it iterates over DataFrame rows as namedtuples, with index value as first element of the tuple. And it is much much faster compared with iterrows(). For itertuples(), each row contains its Index in the DataFrame, and you can use loc to set the value.', ""You should assign value by df.ix[i, 'exp']=X or df.loc[i, 'exp']=X instead of df.ix[i]['ifor'] = x."", ""Well, if you are going to iterate anyhow, why don't use the simplest method of all, df['Column'].values[i]"", ""It's better to use lambda functions using df.apply() -"", 'Increment the MAX number from a column.  For Example :']"
107,Appending to an empty DataFrame in Pandas?,"Is it possible to append to an empty data frame that doesn't contain any indices or columns?

I have tried to do this, but keep getting an empty dataframe at the end.

e.g.

df = pd.DataFrame()
data = ...",https://stackoverflow.com/questions/16597265/appending-to-an-empty-dataframe-in-pandas,"[""Is it possible to append to an empty data frame that doesn't contain any indices or columns?"", 'That should work:', 'And if you want to add a row, you can use a dictionary:', 'You can concat the data in this way:']"
108,Plot correlation matrix using pandas,"I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using dataframe.corr() function from ...",https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas,"['I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using dataframe.corr() function from pandas library. Is there any built-in function provided by the pandas library to plot this matrix?', 'You can use pyplot.matshow()  from matplotlib:', 'If your main goal is to visualize the correlation matrix, rather than creating a plot per se, the convenient pandas styling options is a viable built-in solution:', ""Seaborn's heatmap version:"", 'Try this function, which also displays variable names for the correlation matrix:', 'You can observe the relation between features either by drawing a heat map from seaborn or scatter matrix from pandas.', 'For completeness, the simplest solution i know with seaborn as of late 2019, if one is using Jupyter:', 'You can use imshow() method from matplotlib', 'If you dataframe is df you can simply use:', 'statmodels graphics also gives a nice view of correlation matrix', 'Along with other methods it is also good to have pairplot which will give scatter plot for all the cases-', 'Form correlation matrix, in my case zdf is the dataframe which i need perform correlation matrix.']"
109,Keep only date part when using pandas.to_datetime,"I use pandas.to_datetime to parse the dates in my data. Pandas by default represents the dates with datetime64[ns] even though the dates are all daily only.
I wonder whether there is an elegant/clever ...",https://stackoverflow.com/questions/16176996/keep-only-date-part-when-using-pandas-to-datetime,"['I use pandas.to_datetime to parse the dates in my data. Pandas by default represents the dates with datetime64[ns] even though the dates are all daily only.\nI wonder whether there is an elegant/clever way to convert the dates to datetime.date or datetime64[D] so that, when I write the data to CSV, the dates are not appended with 00:00:00. I know I can convert the type manually element-by-element:', 'Since version 0.15.0 this can now be easily done using .dt to access just the date component:', 'Simple Solution:', ""While I upvoted EdChum's answer, which is the most direct answer to the question the OP posed, it does not really solve the performance problem (it still relies on python datetime objects, and hence any operation on them will be not vectorized - that is, it will be slow)."", 'Avoid, where possible, converting your datetime64[ns] series to an object dtype series of datetime.date objects. The latter, often constructed using pd.Series.dt.date, is stored as an array of pointers and is inefficient relative to a pure NumPy-based series.', 'Pandas DatetimeIndex and Series have a method called normalize that does exactly what you want.', 'This is a simple way to extract the date:', 'Just giving a more up to date answer in case someone sees this old post.', 'Converting to datetime64[D]:', 'I wanted to be able to change the type for a set of columns in a data frame and then remove the time keeping the day. round(), floor(), ceil() all work', 'This worked for me on UTC Timestamp (2020-08-19T09:12:57.945888)']"
110,Apply pandas function to column to create multiple new columns?,"How to do this in pandas:

I have a function extract_text_features on a single text column, returning multiple output columns. Specifically, the function returns 6 values.

The function works, however ...",https://stackoverflow.com/questions/16236684/apply-pandas-function-to-column-to-create-multiple-new-columns,"['How to do this in pandas:', ""Building off of user1827356 's answer, you can do the assignment in one pass using df.merge:"", 'I usually do this using zip:', ""This is what I've done in the past"", 'This is the correct and easiest way to accomplish this for 95% of use cases:', ""In 2018, I use apply() with argument result_type='expand'"", 'Just use result_type=""expand""', 'For me this worked:', ""Summary: If you only want to create a few columns, use df[['new_col1','new_col2']] = df[['data1','data2']].apply( function_of_your_choosing(x), axis=1)"", ""I've looked several ways of doing this and the method shown here (returning a pandas series) doesn't seem to be most efficient."", 'The accepted solution is going to be extremely slow for lots of data. The solution with the greatest number of upvotes is a little difficult to read and also slow with numeric data. If each new column can be calculated independently of the others, I would just assign each of them directly without using apply.', 'Have posted the same answer in two other similar questions. The way I prefer to do this is to wrap up the return values of the function in a series:', 'you can return the entire row instead of values:', 'Here the a dataframe with a single feature is being converted to two new features.\nGive this a try too.', 'I have a more complicated situation, the dataset has a nested structure:']"
111,Label encoding across multiple columns in scikit-learn,"I'm trying to use scikit-learn's LabelEncoder to encode a pandas DataFrame of string labels. As the dataframe has many (50+) columns, I want to avoid creating a LabelEncoder object for each column; I'...",https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn,"[""I'm trying to use scikit-learn's LabelEncoder to encode a pandas DataFrame of string labels. As the dataframe has many (50+) columns, I want to avoid creating a LabelEncoder object for each column; I'd rather just have one big LabelEncoder objects that works across all my columns of data."", 'You can easily do this though,', ""As mentioned by larsmans, LabelEncoder() only takes a 1-d array as an argument. That said, it is quite easy to roll your own label encoder that operates on multiple columns of your choosing, and returns a transformed dataframe. My code here is based in part on Zac Stewart's excellent blog post found here."", 'Since scikit-learn 0.20 you can use sklearn.compose.ColumnTransformer and sklearn.preprocessing.OneHotEncoder:', ""We don't need a LabelEncoder."", 'this does not directly answer your question (for which Naputipulu Jon and PriceHardman have fantastic replies)', ""No, LabelEncoder does not do this. It takes 1-d arrays of class labels and produces 1-d arrays. It's designed to handle class labels in classification problems, not arbitrary data, and any attempt to force it into other uses will require code to transform the actual problem to the problem it solves (and the solution back to the original space)."", 'Assuming you are simply trying to get a sklearn.preprocessing.LabelEncoder() object that can be used to represent your columns, all you have to do is:', 'This is a year-and-a-half after the fact, but I too, needed to be able to .transform() multiple pandas dataframe columns at once (and be able to .inverse_transform() them as well). This expands upon the excellent suggestion of @PriceHardman above:', 'After lots of search and experimentation with some answers here and elsewhere, I think your answer is here:', 'A short way to LabelEncoder() multiple columns with a dict():', 'It is possible to do this all in pandas directly and is well-suited for a unique ability of the replace method.', 'I checked the source code (https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/preprocessing/label.py) of LabelEncoder. It was based on a set of numpy transformation, which one of those is np.unique(). And this function only takes 1-d array input. (correct me if I am wrong).', 'Following up on the comments raised on the solution of @PriceHardman I would propose the following version of the class:', 'If you have numerical and categorical both type of data in dataframe \nYou can use : here X is my dataframe having categorical and numerical both variables', 'if we have single column to do the label encoding and its inverse transform its easy how to do it when there are multiple columns in python', 'TLDR; You here can use the FlattenForEach wrapper class to simply transform your df like: FlattenForEach(LabelEncoder(), then_unflatten=True).fit_transform(df).', 'Mainly used @Alexander answer but had to make some changes -', ""The problem is the shape of the data (pd dataframe) you are passing to the fit function.\nYou've got to pass 1d list."", 'Here i am reading a csv from location and in function i am passing the column list i want to labelencode and the dataframe I want to apply this.', 'How about this?', 'If you have all the features of type object then the first answer written above works well https://stackoverflow.com/a/31939145/5840973.', 'Instead of LabelEncoder we can use OrdinalEncoder from scikit learn, which allows multi-column encoding.']"
112,What is the difference between join and merge in Pandas?,"Suppose I have two DataFrames like so:

left = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})

right = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})
I want to merge them, so I try ...",https://stackoverflow.com/questions/22676081/what-is-the-difference-between-join-and-merge-in-pandas,"['Suppose I have two DataFrames like so:', 'I always use join on indices:', 'pandas.merge() is the underlying function used for all merge/join behavior.', 'From this documentation', 'I believe that join() is just a convenience method. Try df1.merge(df2) instead, which allows you to specify left_on and right_on:', ""One of the difference is that merge is creating a new index, and join is keeping the left side index. It can have a big consequence on your later transformations if you wrongly assume that your index isn't changed with merge."", 'To put it analogously to SQL ""Pandas merge is to outer/inner join and Pandas join is to natural join"". Hence when you use merge in pandas, you want to specify which kind of sqlish join you want to use whereas when you use pandas join, you really want to have a matching column label to ensure it joins']"
113,Select DataFrame rows between two dates,"I am creating a DataFrame from a csv as follows:

stock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)
The DataFrame has a date column. Is there a way to create a new DataFrame (...",https://stackoverflow.com/questions/29370057/select-dataframe-rows-between-two-dates,"['I am creating a DataFrame from a csv as follows:', 'There are two possible solutions:', 'I feel the best option will be to use the direct checks rather than using loc function:', 'You can also use between:', 'You can use the isin method on the date column like so\ndf[df[""date""].isin(pd.date_range(start_date, end_date))]', 'Keeping the solution simple and pythonic, I would suggest you to try this.', 'With my testing of pandas version 0.22.0 you can now answer this question easier with more readable code by simply using between.', 'Another option, how to achieve this, is by using pandas.DataFrame.query() method. Let me show you an example on the following data frame called df.', 'I prefer not to alter the df.', ""you can do it with pd.date_range() and Timestamp.\nLet's say you have read a csv file with a date column using parse_dates option:"", 'Inspired by unutbu']"
114,Split (explode) pandas dataframe string entry to separate rows,I have a pandas dataframe in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be ...,https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows,"[""I have a pandas dataframe in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be split on ','). For example, a should become b:"", 'How about something like this:', 'UPDATE2: more generic vectorized function, which will work for multiple normal and multiple list columns', 'After painful experimentation to find something faster than the accepted answer, I got this to work. It ran around 100x faster on the dataset I tried it on.', 'Series and DataFrame methods define a .explode() method that explodes lists into separate rows. See the docs section on Exploding a list-like column.', ""Here's a function I wrote for this common task. It's more efficient than the Series/stack methods. Column order and names are retained."", 'Similar question as: pandas: How do I split text in a column into multiple rows?', ""Let's create a new dataframe d that has lists"", 'There is a possibility to split and explode the dataframe without changing the structure of dataframe', ""I came up with a solution for dataframes with arbitrary numbers of columns (while still only separating one column's entries at a time)."", 'Here is a fairly straightforward message that uses the split method from pandas str accessor and then uses NumPy to flatten each row into a single array.', 'I have been struggling with out-of-memory experience using various way to explode my lists so I prepared some benchmarks to help me decide which answers to upvote. I tested five scenarios with varying proportions of the list length to the number of lists. Sharing the results below:', ""Based on the excellent @DMulligan's solution, here is a generic vectorized (no loops) function which splits a column of a dataframe into multiple rows, and merges it back to the original dataframe. It also uses a great generic change_column_order function from this answer."", ""The string function split can take an option boolean argument 'expand'."", 'One-liner using split(___, expand=True) and the level and name arguments to reset_index():', ""Just used jiln's excellent answer from above, but needed to expand to split multiple columns. Thought I would share."", ""upgraded MaxU's answer with MultiIndex support"", 'I have come up with the following solution to this problem:', 'Another solution that uses python copy package', ""There are a lot of answers here but I'm surprised no one has mentioned the built in pandas explode function. Check out the link below:\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html#pandas.DataFrame.explode"", 'My version of the solution to add to this collection! :-)', 'I had a similar problem, my solution was converting the dataframe to a list of dictionaries first, then do the transition. Here is the function:', 'Upon adding few bits and pieces from all the solutions on this page, I was able to get something like this(for someone who need to use it right away).\nparameters to the function are df(input dataframe) and key(column that has delimiter separated string). Just replace with your delimiter if that is different to semicolon "";"".']"
115,How to make good reproducible pandas examples,"Having spent a decent amount of time watching both the r and pandas tags on SO, the impression that I get is that pandas questions are less likely to contain reproducible data. This is something that ...",https://stackoverflow.com/questions/20109391/how-to-make-good-reproducible-pandas-examples,"['Having spent a decent amount of time watching both the r and pandas tags on SO, the impression that I get is that pandas questions are less likely to contain reproducible data. This is something that the R community has been pretty good about encouraging, and thanks to guides like this, newcomers are able to get some help on putting together these examples. People who are able to read these guides and come back with reproducible data will often have much better luck getting answers to their questions.', 'Note: The ideas here are pretty generic for Stack Overflow, indeed questions.', ""This is to mainly to expand on @AndyHayden's answer by providing examples of how you can create sample dataframes.  Pandas and (especially) numpy give you a variety of tools for this such that you can generally create a reasonable facsimile of any real dataset with just a few lines of code."", ""My best advice for asking questions would be to play on the psychology of the people who answer questions.  Being one of those people, I can give insight into why I answer certain questions and why I don't answer others."", ""The Challenge One of the most challenging aspects of responding to SO questions is the time it takes to recreate the problem (including the data).  Questions which don't have a clear way to reproduce the data are less likely to be answered.  Given that you are taking the time to write a question and you have an issue that you'd like help with, you can easily help yourself by providing data that others can then use to help solve your problem."", 'Here is my version of dput - the standard R tool to produce reproducible reports - for Pandas DataFrames.\nIt will probably fail for more complex frames, but it seems to do the job in simple cases:']"
116,Find row where values for column is maximal in a pandas DataFrame,"How can I find the row for which the value of a specific column is maximal?

df.max() will give me the maximal value for each column, I don't know how to get the corresponding row.",https://stackoverflow.com/questions/10202570/find-row-where-values-for-column-is-maximal-in-a-pandas-dataframe,"['How can I find the row for which the value of a specific column is maximal?', ""Use the pandas idxmax function. It's straightforward:"", 'You might also try idxmax:', 'Both above answers would only return one index if there are multiple rows that take the maximum value. If you want all the rows, there does not seem to have a function.\nBut it is not hard to do. Below is an example for Series; the same can be done for DataFrame:', 'argmax() would provide the index corresponding to the max value for the columnX. iloc can be used to get the row of the DataFrame df for this index.', 'The direct "".argmax()"" solution does not work for me.', 'Very simple: we have df as below and we want to print a row with max value in C:', 'This one line of code will give you how to find the maximum value from a row in dataframe, here mx is the dataframe and iloc[0] indicates the 0th index.', 'The idmax of the DataFrame returns the label index of the row with the maximum value and the behavior of argmax depends on version of pandas (right now it returns a warning). If you want to use the positional index, you can do the following:']"
117,Detect and exclude outliers in Pandas data frame,"I have a pandas data frame with few columns.

Now I know that certain rows are outliers based on a certain column value.

For instance
  column 'Vol' has all values around 12xx and one value is 4000 (...",https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame,"['I have a pandas data frame with few columns.', 'If you have multiple columns in your dataframe and would like to remove all rows that have outliers in at least one column, the following expression would do that in one shot.', 'Use boolean indexing as you would do in numpy.array', 'For each of your dataframe column, you could get quantile with:', 'This answer is similar to that provided by @tanemaki, but uses a lambda expression instead of scipy stats.', ""Since I haven't seen an answer that deal with numerical and non-numerical attributes, here is a complement answer."", 'For each series in the dataframe, you could use between and quantile to remove outliers.', 'scipy.stats has methods trim1() and trimboth() to cut the outliers out in a single row, according to the ranking and an introduced percentage of removed values.', 'Another option is to transform your data so that the effect of outliers is mitigated. You can do this by winsorizing your data.', 'If you like method chaining, you can get your boolean condition for all numeric columns like this:', 'You can use boolean mask:', 'Since I am in a very early stage of my data science journey, I am treating outliers with the code below.', 'Get the 98th and 2nd percentile as the limits of our outliers', 'a full example with data and 2 groups follows:', 'My function for dropping outliers', 'I prefer to clip rather than drop.  the following will clip inplace at the 2nd and 98th pecentiles.', 'Deleting and dropping outliers I believe is wrong statistically. \nIt makes the data different from original data.\nAlso makes data unequally shaped and hence best way is to reduce or avoid the effect of outliers by log transform the data.\nThis worked for me:']"
118,Add column to dataframe with constant value,"I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.

Existing df:

Date, Open, High, Low, Close
01-01-2015, 565, 600, 400, 450
...",https://stackoverflow.com/questions/29517072/add-column-to-dataframe-with-constant-value,"['I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.', ""df['Name']='abc' will add the new column and set all rows to that value:"", 'You can use insert to specify where you want to new column to be.  In this case, I use 0 to place the new column at the left.', 'Single liner works', 'Summing up what the others have suggested, and adding a third way']"
119,how do I insert a column at a specific column index in pandas?,"Can I insert a column at a specific column index in pandas? 

import pandas as pd
df = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})
df['n'] = 0
This will put column n as the last column of df,...",https://stackoverflow.com/questions/18674064/how-do-i-insert-a-column-at-a-specific-column-index-in-pandas,"['Can I insert a column at a specific column index in pandas?', 'see docs: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.insert.html', 'If you want a single value for all rows:', 'You could try to extract columns as list, massage this as you want, and reindex your dataframe:', ""You can do that after you added the 'n' column into your df as follows.""]"
120,Add x and y labels to a pandas plot,"Suppose I have the following code that plots something very simple using pandas:

import pandas as pd
values = [[1, 2], [2, 5]]
df2 = pd.DataFrame(values, columns=['Type A', 'Type B'], 
               ...",https://stackoverflow.com/questions/21487329/add-x-and-y-labels-to-a-pandas-plot,"['Suppose I have the following code that plots something very simple using pandas:', 'The df.plot() function returns a matplotlib.axes.AxesSubplot object. You can set the labels on that object.', 'You can use do it like this:', 'If you label the columns and index of your DataFrame, pandas will automatically supply appropriate labels:', 'It is possible to set both labels together with axis.set function. Look for the example:', 'For cases where you use pandas.DataFrame.hist:', 'what about ...', 'pandas uses matplotlib for basic dataframe plots. So, if you are using pandas for basic plot you can use matplotlib for plot customization. However, I propose an alternative method here using seaborn which allows more customization of the plot while not going into the basic level of matplotlib.']"
121,How to split a dataframe string column into two columns?,"I have a data frame with one (string) column and I'd like to split it into two (string) columns, with one column header as 'fips' and the other 'row'
My dataframe df looks like this:
          row
0   ...",https://stackoverflow.com/questions/14745022/how-to-split-a-dataframe-string-column-into-two-columns,"[""I have a data frame with one (string) column and I'd like to split it into two (string) columns, with one column header as 'fips' and the other 'row'"", ""There might be a better way, but this here's one approach:"", 'For the simple case of:', 'You can extract the different parts out quite neatly using a regex pattern:', ""If you don't want to create a new dataframe, or if your dataframe has more columns than just the ones you want to split, you could:"", 'You can use str.split by whitespace (default separator) and parameter expand=True for DataFrame with assign to new columns:', ""If you want to split a string into more than two columns based on a delimiter you can omit the 'maximum splits' parameter.\nYou can use:"", ""Surprised I haven't seen this one yet. If you only need two splits, I highly recommend. . ."", 'Use df.assign to create a new df. See http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy', 'I prefer exporting the corresponding pandas series (i.e. the columns I need), using the apply function to split the column content into multiple series and then join the generated columns to the existing DataFrame. Of course, the source column should be removed.', 'I saw that no one had used the slice method, so here I put my 2 cents here.']"
122,How do I read a large csv file with pandas?,"I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting a memory error:

MemoryError                               Traceback (most recent call last)
<ipython-input-58-...",https://stackoverflow.com/questions/25962114/how-do-i-read-a-large-csv-file-with-pandas,"['I am trying to read a large csv file (aprox. 6 GB) in pandas and i am getting a memory error:', 'The error shows that the machine does not have enough memory to read the entire\nCSV into a DataFrame at one time. Assuming you do not need the entire dataset in\nmemory all at one time, one way to avoid the problem would be to process the CSV in\nchunks (by specifying the chunksize parameter):', ""Chunking shouldn't always be the first port of call for this problem."", 'For large data l recommend you use the library ""dask""  e.g:', 'I proceeded like this:', 'The above answer is already satisfying the topic. Anyway, if you need all the data in memory - have a look at bcolz. Its compressing the data in memory. I have had really good experience with it. But its missing a lot of pandas features', 'You can read in the data as chunks and save each chunk as pickle.', 'The function read_csv and read_table is almost the same. But you must assign the delimiter “，” when you use the function read_table in your program.', 'I want to make a more comprehensive answer based off of the most of the potential solutions that are already provided. I also want to point out one more potential aid that may help reading process.', 'Solution 1:', 'Here follows an example:', 'You can try sframe, that have the same syntax as pandas but allows you to manipulate files that are bigger than your RAM.', 'If you use pandas read large file into chunk and then yield row by row, here is what I have done', 'In addition to the answers above, for those who want to process CSV and then export to csv, parquet or SQL, d6tstack is another good option. You can load multiple files and it deals with data schema changes (added/removed columns). Chunked out of core support is already built in.', ""In case someone is still looking for something like this, I found that this new library called modin can help. It uses distributed computing that can help with the read. Here's a nice article comparing its functionality with pandas. It essentially uses the same functions as pandas."", 'Before using chunksize option if you want to be sure about the process function that you want to write inside the chunking for-loop as mentioned by @unutbu you can simply use nrows option.']"
123,why should I make a copy of a data frame in pandas,"When selecting a sub dataframe from a parent dataframe, I noticed that some programmers make a copy of the data frame using the .copy() method. For example,
X = my_dataframe[features_list].copy()

......",https://stackoverflow.com/questions/27673231/why-should-i-make-a-copy-of-a-data-frame-in-pandas,"['When selecting a sub dataframe from a parent dataframe, I noticed that some programmers make a copy of the data frame using the .copy() method. For example,', ""This expands on Paul's answer. In Pandas, indexing a DataFrame returns a reference to the initial DataFrame. Thus, changing the subset will change the initial DataFrame. Thus, you'd want to use the copy if you want to make sure the initial DataFrame shouldn't change. Consider the following code:"", ""Because if you don't make a copy then the indices can still be manipulated elsewhere even if you assign the dataFrame to a different name."", ""It's necessary to mention that returning copy or view depends on kind of indexing."", 'The primary purpose is to avoid chained indexing and eliminate the SettingWithCopyWarning.', ""In general it is safer to work on copies than on original data frames, except when you know that you won't be needing the original anymore and want to proceed with the manipulated version. Normally, you would still have some use for the original data frame to compare with the manipulated version, etc. Therefore, most people work on copies and merge at the end."", 'Assumed you have data frame as below']"
124,Extracting specific selected columns to new DataFrame as a copy,I have a pandas DataFrame with 4 columns and I want to create a new DataFrame that only has three of the columns.  This question is similar to: Extracting specific columns from a data frame but for ...,https://stackoverflow.com/questions/34682828/extracting-specific-selected-columns-to-new-dataframe-as-a-copy,"['I have a pandas DataFrame with 4 columns and I want to create a new DataFrame that only has three of the columns.  This question is similar to: Extracting specific columns from a data frame but for pandas not R.  The following code does not work, raises an error, and is certainly not the pandasnic way to do it.', 'There is a way of doing this and it actually looks similar to R', 'The easiest way is', 'Another simpler way seems to be:', 'columns by index:', 'Generic functional form', 'If you want to have a new data frame then:', ""As far as I can tell, you don't necessarily need to specify the axis when using the filter function.""]"
125,Python pandas Filtering out nan from a data selection of a column of strings,"Without using groupby how would I filter out data without NaN?

Let say I have a matrix where customers will fill in 'N/A','n/a' or any of its variations and others leave it blank:

import pandas as ...",https://stackoverflow.com/questions/22551403/python-pandas-filtering-out-nan-from-a-data-selection-of-a-column-of-strings,"['Without using groupby how would I filter out data without NaN?', 'Just drop them:', 'Simplest of all solutions:']"
126,pandas three-way joining multiple dataframes on columns,"I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person. 

How can I ""join"" together all three CSV ...",https://stackoverflow.com/questions/23668427/pandas-three-way-joining-multiple-dataframes-on-columns,"['I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person.', 'Assumed imports:', 'You could try this if you have 3 dataframes', 'The join method is built exactly for these types of situations. You can join any number of DataFrames together with it. The calling DataFrame joins with the index of the collection of passed DataFrames. To work with multiple DataFrames, you must put the joining columns in the index.', 'This can also be done as follows for a list of dataframes df_list:', 'In python 3.6.3 with pandas 0.22.0 you can also use concat as long as you set as index the columns you want to use for the joining', 'Here is a method to merge a dictionary of data frames while keeping the column names in sync with the dictionary. Also it fills in missing values if needed:', 'Simple Solution:', ""One does not need a multiindex to perform join operations.\nOne just need to set correctly the index column on which to perform the join operations (which command df.set_index('Name') for example)"", ""There is another solution from the pandas documentation (that I don't see here),"", 'The three dataframes are']"
127,Changing a specific column name in pandas DataFrame,"I was looking for an elegant way to change a specified column name in a DataFrame.

play data ...

import pandas as pd
d = {
         'one': [1, 2, 3, 4, 5],
         'two': [9, 8, 7, 6, 5],
         '...",https://stackoverflow.com/questions/20868394/changing-a-specific-column-name-in-pandas-dataframe,"['I was looking for an elegant way to change a specified column name in a DataFrame.', 'A one liner does exist:', ""Since inplace argument is available, you don't need to copy and assign the original data frame back to itself, but do as follows:"", 'What about?', 'The rename method has gained an axis parameter to match most of the rest of the pandas API.', 'If you know which column # it is (first / second / nth) then this solution posted on a similar question works regardless of whether it is named or unnamed, and in one line: https://stackoverflow.com/a/26336314/4355695', 'For renaming the columns here is the simple one which will work for both Default(0,1,2,etc;) and existing columns but not much useful for a larger data sets(having many columns).', 'Following short code can help:', 'pandas version 0.23.4', 'Another option would be to simply copy & drop the column:', 'size = 10\ndf.rename(columns={df.columns[i]: someList[i] for i in range(size)}, inplace = True)']"
128,Convert columns to string in Pandas,"I have the following DataFrame from a SQL query:

(Pdb) pp total_rows
     ColumnID  RespondentCount
0          -1                2
1  3030096843                1
2  3030096845                1
and I ...",https://stackoverflow.com/questions/22005911/convert-columns-to-string-in-pandas,"['I have the following DataFrame from a SQL query:', 'One way to convert to string is to use astype:', 'If you need to convert ALL columns to strings, you can simply use:', ""Here's the other one, particularly useful to convert the multiple columns to string instead of just single column:"", 'Prior to pandas 1.0 (well, 0.25 actually) this was the defacto way of declaring a Series/column as as string:', 'Using .apply() with a lambda conversion function also works in this case:']"
129,What are the differences between Pandas and NumPy+SciPy in Python? [closed],They both seem exceedingly similar and I'm curious as to which package would be more beneficial for financial data analysis.,https://stackoverflow.com/questions/11077023/what-are-the-differences-between-pandas-and-numpyscipy-in-python,"['Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.', 'pandas provides high level data manipulation tools built on top of NumPy. NumPy by itself is a fairly low-level tool, similar to MATLAB. pandas on the other hand provides rich time series functionality, data alignment, NA-friendly statistics, groupby, merge and join methods, and lots of other conveniences. It has become very popular in recent years in financial applications. I will have a chapter dedicated to financial data analysis using pandas in my upcoming book.', 'Numpy is required by pandas (and by virtually all numerical tools for Python).  Scipy is not strictly required for pandas but is listed as an ""optional dependency"".  I wouldn\'t say that pandas is an alternative to Numpy and/or Scipy.  Rather, it\'s an extra tool that provides a more streamlined way of working with numerical and tabular data in Python.  You can use pandas data structures but freely draw on Numpy and Scipy functions to manipulate them.', 'Pandas offer a great way to manipulate tables, as you can make binning easy (binning a dataframe in pandas in Python) and calculate statistics. Other thing that is great in pandas is the Panel class that you can join series of layers with different properties and combine it using groupby function.']"
130,Rename specific column(s) in pandas,"I've got a dataframe called data. How would I rename the only one column header? For example gdp to log(gdp)?

data =
    y  gdp  cap
0   1    2    5
1   2    3    9
2   8    7    2
3   3    4    7
4  ...",https://stackoverflow.com/questions/19758364/rename-specific-columns-in-pandas,"[""I've got a dataframe called data. How would I rename the only one column header? For example gdp to log(gdp)?"", 'The rename show that it accepts a dict as a param for columns so you just pass a dict with a single entry.', 'A much faster implementation would be to use list-comprehension if you need to rename a single column.', 'From v0.24+, to rename one (or more) columns at a time,', 'There are at least five different ways to rename specific columns in pandas, and I have listed them below along with links to the original answers. I also timed these methods and found them to perform about the same (though YMMV depending on your data set and scenario). The test case below is to rename columns A M N Z to A2 M2 N2 Z2 in a dataframe with columns A to Z containing a million rows.', 'Use the pandas.DataFrame.rename funtion.\nCheck this link for description.']"
131,How do I retrieve the number of columns in a Pandas data frame?,"How do you programmatically retrieve the number of columns in a pandas dataframe? I was hoping for something like:

df.num_columns",https://stackoverflow.com/questions/20297332/how-do-i-retrieve-the-number-of-columns-in-a-pandas-data-frame,"['How do you programmatically retrieve the number of columns in a pandas dataframe? I was hoping for something like:', 'Like so:', 'Alternative:', 'If the variable holding the dataframe is called df, then:', 'df.info() function will give you result something like as below. \nIf you are using read_csv method of Pandas without sep parameter or sep with "","".', 'This worked for me len(list(df)).', ""There are multiple option to get column number and column information such as:\nlet's check them.""]"
132,Convert Pandas column containing NaNs to dtype `int`,"I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.

When I ...",https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int,"['I read data from a .csv file to a Pandas dataframe as below. For one of the columns, namely id, I want to specify the column type as int. The problem is the id series has missing/empty values.', 'The lack of NaN rep in integer columns is a pandas ""gotcha"".', 'In version 0.24.+ pandas has gained the ability to hold integer dtypes with missing values.', 'My use case is munging data prior to loading into a DB table:', ""If you absolutely want to combine integers and NaNs in a column, you can use the 'object' data type:"", 'It is now possible to create a pandas column containing NaNs as dtype int, since it is now officially added on pandas 0.24.0', 'If you can modify your stored data, use a sentinel value for missing id. A common use case, inferred by the column name,  being that id is an integer, strictly greater than zero, you could use 0 as a sentinel value so that you can write', 'You could use .dropna() if it is OK to drop the rows with the NaN values.', ""Most solutions here tell you how to use a placeholder integer to represent nulls. That approach isn't helpful if you're uncertain that integer won't show up in your source data though. My method with will format floats without their decimal values and convert nulls to None's. The result is an object datatype that will look like an integer field with null values when loaded into a CSV."", 'I ran into this issue working with pyspark. As this is a python frontend for code running on a jvm, it requires type safety and using float instead of int is not an option. I worked around the issue by wrapping the pandas pd.read_csv in a function that will fill user-defined columns with user-defined fill values before casting them to the required type. Here is what I ended up using:', 'Try this:', ""I had the problem a few weeks ago with a few discrete features which were formatted as 'object'. This solution seemed to work."", 'First remove the rows which contain NaN. Then do Integer conversion on remaining rows.\nAt Last insert the removed rows again.\nHope it will work', 'use pd.to_numeric()', 'As of Pandas 1.0.0 you can now use pandas.NA values. This does not force integer columns with missing values to be floats.', 'If you want to use it when you chain methods, you can use assign:', 'Assuming your DateColumn formatted 3312018.0 should be converted to 03/31/2018 as a string.  And, some records are missing or 0.']"
133,get list of pandas dataframe columns based on data type,"If I have a dataframe with the following columns: 

1. NAME                                     object
2. On_Time                                      object
3. On_Budget                               ...",https://stackoverflow.com/questions/22470690/get-list-of-pandas-dataframe-columns-based-on-data-type,"['If I have a dataframe with the following columns:', 'If you want a list of columns of a certain type, you can use groupby:', 'As of pandas v0.14.1, you can utilize select_dtypes() to select columns by dtype', ""Using dtype will give you desired column's data type:"", 'You can use boolean mask on the dtypes attribute:', 'This should do the trick', 'use df.info(verbose=True) where df is a pandas datafarme, by default verbose=False', ""The most direct way to get a list of columns of certain dtype e.g. 'object':"", 'If you want a list of only the object columns you could do:', 'I came up with this three liner.', 'for yoshiserry;', 'I use infer_objects()', 'If after 6 years you still have the issue, this should solve it :)']"
134,pandas resample documentation,"So I completely understand how to use resample, but the documentation does not do a good job explaining the options.

So most options in the resample function are pretty straight forward except for ...",https://stackoverflow.com/questions/17001389/pandas-resample-documentation,"['So I completely understand how to use resample, but the documentation does not do a good job explaining the options.', ""See the timeseries documentation. It includes a list of offsets (and 'anchored' offsets), and a section about resampling."", ""There's more to it than this, but you're probably looking for this list:""]"
135,How to iterate over columns of pandas dataframe to run regression,"I'm sure this is simple, but as a complete newbie to python, I'm having trouble figuring out how to iterate over variables in a pandas dataframe and run a regression with each.

Here's what I'm doing:
...",https://stackoverflow.com/questions/28218698/how-to-iterate-over-columns-of-pandas-dataframe-to-run-regression,"[""I'm sure this is simple, but as a complete newbie to python, I'm having trouble figuring out how to iterate over variables in a pandas dataframe and run a regression with each."", 'You can use iteritems():', 'This answer is to iterate over selected columns as well as all columns in a DF.', 'You can index dataframe columns by the position using ix.', 'A workaround is to transpose the DataFrame and iterate over the rows.', 'Using list comprehension, you can get all the columns names (header):', 'Based on the accepted answer, if an index corresponding to each column is also desired:', ""I'm a bit late but here's how I did this. The steps:""]"
136,Using Pandas to pd.read_excel() for multiple worksheets of the same workbook,I have a large spreadsheet file (.xlsx) that I'm processing using python pandas. It happens that I need data from two tabs in that large file. One of the tabs has a ton of data and the other is just a ...,https://stackoverflow.com/questions/26521266/using-pandas-to-pd-read-excel-for-multiple-worksheets-of-the-same-workbook,"[""I have a large spreadsheet file (.xlsx) that I'm processing using python pandas. It happens that I need data from two tabs in that large file. One of the tabs has a ton of data and the other is just a few square cells."", 'Try pd.ExcelFile:', 'There are a few options:', 'You can also use the index for the sheet:', 'You could also specify the sheet name as a parameter:', 'by default read the first sheet of workbook.', 'If you are interested in reading all sheets and merging them together. The best and fastest way to do it', ""Yes unfortunately it will always load the full file. If you're doing this repeatedly probably best to extract the sheets to separate CSVs and then load separately. You can automate that process with d6tstack which also adds additional features like checking if all the columns are equal across all sheets or multiple Excel files."", 'If you have saved the excel file in the same folder as your python program (relative paths) then you just need to mention sheet number along with file name.', 'If:']"
137,pandas DataFrame: replace nan values with average of columns,"I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.

How can I replace the nans with averages of columns where they are?

This question is very ...",https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns,"[""I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well."", ""You can simply use DataFrame.fillna to fill the nan's directly:"", 'Try:', 'Apply per-column the mean of that columns and fill', 'If you want to impute missing values with mean and you want to go column by column, then this will only impute with the mean of that column. This might be a little more readable.', 'Directly use df.fillna(df.mean()) to fill all the null value with mean', 'Another option besides those above is:', 'Pandas: How to replace NaN (nan) values with the average (mean), median or other statistics of one column', 'Although, the below code does the job, BUT its performance takes a big hit, as you deal with a DataFrame with # records 100k or more:', 'using sklearn library preprocessing class']"
138,Convert a Pandas DataFrame to a dictionary,"I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be keys and the elements of other columns in same row be values. 

...",https://stackoverflow.com/questions/26716616/convert-a-pandas-dataframe-to-a-dictionary,"['I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be keys and the elements of other columns in same row be values.', ""The to_dict() method sets the column names as dictionary keys so you'll need to reshape your DataFrame slightly. Setting the 'ID' column as the index and then transposing the DataFrame is one way to achieve this."", 'Try to use Zip', 'Suppose your dataframe is as follows:', 'should a dictionary like:', ""If you don't mind the dictionary values being tuples, you can use itertuples:"", ""For my use (node names with xy positions) I found @user4179775's answer to the most helpful / intuitive:"", 'DataFrame.to_dict() converts DataFrame to dictionary.']"
139,Delete the first three rows of a dataframe in pandas,"I need to delete the first three rows of a dataframe in pandas.

I know df.ix[:-1] would remove the last row, but I can't figure out how to remove first n rows.",https://stackoverflow.com/questions/16396903/delete-the-first-three-rows-of-a-dataframe-in-pandas,"['I need to delete the first three rows of a dataframe in pandas.', 'Use iloc:', 'I think a more explicit way of doing this is to use drop.', 'n drops the first n rows.', 'Pandas uses zero based numbering, so 0 is the first row, 1 is the second row and 2 is the third row.', ""You can use python slicing, but note it's not in-place."", 'A simple way is to use tail(-n) to remove the first n rows', 'inp0= pd.read_csv(""bank_marketing_updated_v1.csv"",skiprows=2)']"
140,pandas: merge (join) two data frames on multiple columns,"I am trying to join two pandas data frames using two columns:

new_df = pd.merge(A_df, B_df,  how='left', left_on='[A_c1,c2]', right_on = '[B_c1,c2]')
but got the following error:

pandas/index.pyx ...",https://stackoverflow.com/questions/41815079/pandas-merge-join-two-data-frames-on-multiple-columns,"['I am trying to join two pandas data frames using two columns:', 'Try this', 'the problem here is that by using the apostrophes you are setting the value being passed to be a string, when in fact, as @Shijo stated from the documentation, the function is expecting a label or list, but not a string! If the list contains each of the name of the columns beings passed for both the left and right dataframe, then each column-name must individually be within apostrophes. With what has been stated, we can understand why this is inccorect:', ""Another way of doing this:\nnew_df = A_df.merge(B_df, left_on=['A_c1','c2'], right_on = ['B_c1','c2'], how='left')""]"
141,How to get the last N rows of a pandas DataFrame?,"I have pandas dataframe df1 and df2 (df1 is vanila dataframe, df2 is indexed by 'STK_ID' & 'RPT_Date') :

>>> df1
    STK_ID  RPT_Date  TClose   sales  discount
0   000568  20060331    3....",https://stackoverflow.com/questions/14663004/how-to-get-the-last-n-rows-of-a-pandas-dataframe,"[""I have pandas dataframe df1 and df2 (df1 is vanila dataframe, df2 is indexed by 'STK_ID' & 'RPT_Date') :"", ""Don't forget DataFrame.tail! e.g. df1.tail(10)"", 'This is because of using integer indices (ix selects those by label over -3 rather than position, and this is by design: see integer indexing in pandas ""gotchas""*).', ""If you are slicing by position, __getitem__ (i.e., slicing with[]) works well, and is the most succinct solution I've found for this problem.""]"
142,How to add header row to a pandas DataFrame,"I am reading a csv file into pandas. This csv file constists of four columns and some rows, but does not have a header row, which I want to add. I have been trying the following: 

Cov = pd.read_csv(""...",https://stackoverflow.com/questions/34091877/how-to-add-header-row-to-a-pandas-dataframe,"['I am reading a csv file into pandas. This csv file constists of four columns and some rows, but does not have a header row, which I want to add. I have been trying the following:', 'You can use names directly in the read_csv', 'Alternatively you could read you csv with header=None and then add it with df.columns:', 'having done this, just check it with[well obviously I know, u know that. But still...', 'To fix your code you can simply change [Cov] to Cov.values, the first parameter of pd.DataFrame will become a multi-dimensional numpy array:']"
143,pandas groupby sort within groups,"I want to group my dataframe by two columns and then sort the aggregated results within the groups.

In [167]:
df

Out[167]:
count   job source
0   2   sales   A
1   4   sales   B
2   6   sales   C
3  ...",https://stackoverflow.com/questions/27842613/pandas-groupby-sort-within-groups,"['I want to group my dataframe by two columns and then sort the aggregated results within the groups.', 'What you want to do is actually again a groupby (on the result of the first groupby): sort and take the first three elements per group.', 'You could also just do it in one go, by doing the sort first and using head to take the first 3 of each group.', ""Here's other example of taking top 3 on sorted order, and sorting within the groups:"", ""If you don't need to sum a column, then use @tvashtar's answer. If you do need to sum, then you can use @joris' answer or this one which is very similar to it."", 'You can do it in one line -']"
144,Getting list of lists into pandas DataFrame,"I am reading contents of a spreadsheet into pandas.   DataNitro has a method that returns a rectangular selection of cells as a list of lists.   So

table = Cell(""A1"").table
gives

table = [['...",https://stackoverflow.com/questions/19112398/getting-list-of-lists-into-pandas-dataframe,"['I am reading contents of a spreadsheet into pandas.   DataNitro has a method that returns a rectangular selection of cells as a list of lists.   So', 'Call the pd.DataFrame constructor directly:', 'With approach explained by EdChum above, the values in the list are shown as rows. To show the values of lists as columns in DataFrame instead, simply use transpose() as following:', 'Even without pop the list we can do with set_index']"
145,Load data from txt with pandas,"I am loading a txt file containig a mix of float and string data. I want to store them in an array where I can access each element. Now I am just doing 

import pandas as pd

data = pd.read_csv('...",https://stackoverflow.com/questions/21546739/load-data-from-txt-with-pandas,"['I am loading a txt file containig a mix of float and string data. I want to store them in an array where I can access each element. Now I am just doing', 'You can use:', ""I'd like to add to the above answers, you could directly use"", ""@Pietrovismara's solution is correct but I'd just like to add: rather than having a separate line to add column names, it's possible to do this from pd.read_csv."", 'you can use this', ""If you don't have an index assigned to the data and you are not sure what the spacing is, you can use to let pandas assign an index and look for multiple spaces."", 'You can do as:', 'Based on the latest changes in pandas, you can use, read_csv , read_table is deprecated:', 'You can import the text file using the read_table command as so:', 'I usually take a look at the data first or just try to import it and do data.head(), if you see that the columns are separated with \\t then you should specify sep=""\\t"" otherwise, sep = "" "".', 'You can use it which is most helpful.']"
146,Apply vs transform on a group object,"Consider the following dataframe:

     A      B         C         D
0  foo    one  0.162003  0.087469
1  bar    one -1.156319 -1.526272
2  foo    two  0.833892 -1.666304
3  bar  three -2.026673 -0....",https://stackoverflow.com/questions/27517425/apply-vs-transform-on-a-group-object,"['Consider the following dataframe:', 'There are two major differences between the transform and apply groupby methods.', 'As I felt similarly confused with .transform operation vs. .apply I found a few answers shedding some light on the issue. This answer for example was very helpful.', 'I am going to use a very simple snippet to illustrate the difference:', 'is like']"
147,How to print pandas DataFrame without index,"I want to print the whole dataframe, but I don't want to print the index

Besides, one column is datetime type, I just want to print time, not date.

The dataframe looks like:

   User ID           ...",https://stackoverflow.com/questions/24644656/how-to-print-pandas-dataframe-without-index,"[""I want to print the whole dataframe, but I don't want to print the index"", 'Or possibly:', 'The line below would hide the index column of DataFrame when you print', 'To retain ""pretty-print"" use', 'If you want to pretty print the data frames, then you can use tabulate package.', 'If you just want a string/json to print it can be solved with:', 'To answer the ""How to print dataframe without an index"" question, you can set the index to be an array of empty strings (one for each row in the dataframe), like this:', 'Similar to many of the answers above that use df.to_string(index=False), I often find it necessary to extract a single column of values in which case you can specify an individual column with .to_string using the following:']"
148,How to determine whether a Pandas Column contains a particular value,"I am trying to determine whether there is an entry in a Pandas column that has a particular value. I tried to do this with if x in df['id']. I thought this was working, except when I fed it a value ...",https://stackoverflow.com/questions/21319929/how-to-determine-whether-a-pandas-column-contains-a-particular-value,"[""I am trying to determine whether there is an entry in a Pandas column that has a particular value. I tried to do this with if x in df['id']. I thought this was working, except when I fed it a value that I knew was not in the column 43 in df['id'] it still returned True. When I subset to a data frame only containing entries matching the missing id df[df['id'] == 43] there are, obviously, no entries in it. How to I determine if a column in a Pandas data frame contains a particular value and why doesn't my current method work? (FYI, I have the same problem when I use the implementation in this answer to a similar question)."", 'in of a Series checks whether the value is in the index:', ""You can also use pandas.Series.isin although it's a little bit longer than 'a' in s.values:"", 'the found.count() will contains number of matches', 'I did a few simple tests:', 'Or use Series.tolist or Series.any:', 'Simple condition:', 'Use', 'I don\'t suggest to use ""value in series"", which can lead many errors. Please see this answer for detail: Using in operator with Pandas series', 'Suppose you dataframe looks like :']"
149,What is the difference between a pandas Series and a single-column DataFrame?,"Why does pandas make a distinction between a Series and a single-column DataFrame?
In other words: what is the reason of existence of the Series class? 

I'm mainly using time series with datetime ...",https://stackoverflow.com/questions/26047209/what-is-the-difference-between-a-pandas-series-and-a-single-column-dataframe,"['Why does pandas make a distinction between a Series and a single-column DataFrame?\nIn other words: what is the reason of existence of the Series class?', 'Quoting the Pandas docs', 'from the pandas doc http://pandas.pydata.org/pandas-docs/stable/dsintro.html\nSeries is a one-dimensional labeled array capable of holding any data type.\nTo read data in form of panda Series:', 'Series is a one-dimensional object that can hold any data type such as integers, floats and strings e.g', 'Series is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call:', 'Import cars data']"
150,How to save a Seaborn plot into a file,"I tried the following code (test_seaborn.py):

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
matplotlib.style.use('ggplot')
import seaborn as sns
sns.set()
df = sns....",https://stackoverflow.com/questions/32244753/how-to-save-a-seaborn-plot-into-a-file,"['I tried the following code (test_seaborn.py):', ""Remove the get_figure and just use sns_plot.savefig('output.png')"", 'The suggested solutions are incompatible with Seaborn 0.8.1', 'Some of the above solutions did not work for me. The .fig attribute was not found when I tried that and I was unable to use .savefig() directly. However, what did work was:', 'You should just be able to use the savefig method of sns_plot directly.', 'Fewer lines for 2019 searchers:', 'I use distplot and get_figure to save picture successfully.', 'This works for me', 'Its also possible to just create a matplotlib figure object and then use plt.savefig(...):', 'You would get an error for using sns.figure.savefig(""output.png"") in seaborn 0.8.1.', 'Just FYI, the below command worked in seaborn 0.8.1 so I guess the initial answer is still valid.']"
151,Pandas DataFrame to List of Dictionaries,"I have the following DataFrame:
customer    item1      item2    item3
1           apple      milk     tomato
2           water      orange   potato
3           juice      mango    chips
which I want ...",https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries,"['I have the following DataFrame:', ""As John Galt mentions in his answer , you should probably instead use df.to_dict('records'). It's faster than transposing manually."", ""Use df.to_dict('records') -- gives the output without having to transpose externally."", ""As an extension to John Galt's answer -"", 'If you are interested in only selecting one column this will work.']"
152,Filtering Pandas DataFrames on dates,"I have a Pandas DataFrame with a 'date' column. Now I need to filter out all rows in the DataFrame that have dates outside of the next two months. Essentially, I only need to retain the rows that are ...",https://stackoverflow.com/questions/22898824/filtering-pandas-dataframes-on-dates,"[""I have a Pandas DataFrame with a 'date' column. Now I need to filter out all rows in the DataFrame that have dates outside of the next two months. Essentially, I only need to retain the rows that are within the next two months."", 'If date column is the index, then use .loc for label based indexing or .iloc for positional indexing.', ""Previous answer is not correct in my experience, you can't pass it a simple string, needs to be a datetime object. So:"", 'And if your dates are standardized by importing datetime package, you can simply use:', 'If your datetime column have the Pandas datetime type (e.g. datetime64[ns]), for proper filtering you need the pd.Timestamp object, for example:', 'If the dates are in the index then simply:', 'You can use pd.Timestamp to perform a query and a local reference', ""So when loading the csv data file, we'll need to set the date column as index now as below, in order to filter data based on a range of dates. This was not needed for the now deprecated method: pd.DataFrame.from_csv()."", 'How about using pyjanitor', 'If you have already converted the string to a date format using pd.to_datetime you can just use:', 'The shortest way to filter your dataframe by date:\nLets suppose your date column is type of datetime64[ns]', ""I'm not allowed to write any comments yet, so I'll write an answer, if somebody will read all of them and reach this one."", ""You could just select the time range by doing: df.loc['start_date':'end_date']""]"
153,Merge two dataframes by index,"I have the following dataframes:
> df1
  id begin conditional confidence discoveryTechnique  
0 278    56       false        0.0                  1   
1 421    18       false        0.0             ...",https://stackoverflow.com/questions/40468069/merge-two-dataframes-by-index,"['I have the following dataframes:', 'Use merge, which is inner join by default:', 'you can use concat([df1, df2, ...], axis=1) in order to concatenate two or more DFs aligned by indexes:', 'By default:\njoin is a column-wise left join\npd.merge is a column-wise inner join\npd.concat is a row-wise outer join', 'A silly bug that got me: the joins failed because index dtypes differed. This was not obvious as both tables were pivot tables of the same original table. After reset_index, the indices looked identical in Jupyter. It only came to light when saving to Excel...', 'If u want to join two dataframes in pandas you can simply use available attributes like merge or concatenate.\nFor example if I have two dataframes df1 and df2 I can join them by:']"
154,Progress indicator during pandas operations,"I regularly perform pandas operations on data frames in excess of 15 million or so rows and I'd love to have access to a progress indicator for particular operations.

Does a text based progress ...",https://stackoverflow.com/questions/18603270/progress-indicator-during-pandas-operations,"[""I regularly perform pandas operations on data frames in excess of 15 million or so rows and I'd love to have access to a progress indicator for particular operations."", ""Due to popular demand, tqdm has added support for pandas. Unlike the other answers, this will not noticeably slow pandas down -- here's an example for DataFrameGroupBy.progress_apply:"", ""To tweak Jeff's answer (and have this as a reuseable function)."", ""In case you need support for how to use this in a Jupyter/ipython notebook, as I did, here's a helpful guide and source to relevant article:"", ""For anyone who's looking to apply tqdm on their custom parallel pandas-apply code."", 'You can easily do this with a decorator', 'I\'ve changed Jeff\'s answer, to include a total, so that you can track progress and a variable to  just print every X iterations (this actually improves the performance by a lot, if the ""print_at"" is reasonably high)']"
155,pandas loc vs. iloc vs. ix vs. at vs. iat?,Recently began branching out from my safe place (R) into Python and and am a bit confused by the cell localization/selection in Pandas. I've read the documentation but I'm struggling to understand the ...,https://stackoverflow.com/questions/28757389/pandas-loc-vs-iloc-vs-ix-vs-at-vs-iat,"[""Recently began branching out from my safe place (R) into Python and and am a bit confused by the cell localization/selection in Pandas. I've read the documentation but I'm struggling to understand the practical implications of the various localization/selection options."", ""loc: only work on index\niloc: work on position\nix: You can get data from dataframe without it being in the index\nat: get scalar values. It's a very fast loc\niat: Get scalar values. It's a very fast iloc"", 'Updated for pandas 0.20 given that ix is deprecated.  This demonstrates not only how to use loc, iloc, at, iat, set_value, but how to accomplish, mixed positional/label based indexing.', 'There are two primary ways that pandas makes selections from a DataFrame.', ""Let's start with this small df:""]"
156,Pandas DataFrame Groupby two columns and get counts,"I have a pandas dataframe in the following format:

df = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,...",https://stackoverflow.com/questions/17679089/pandas-dataframe-groupby-two-columns-and-get-counts,"['I have a pandas dataframe in the following format:', ""Followed by @Andy's answer, you can do following to solve your second question:"", 'You are looking for size:', 'Inserting data into a pandas dataframe and providing column name.', 'Explanation', ""Should you want to add a new column (say 'count_column') containing the groups' counts into the dataframe:"", 'You can just use the built-in function count follow by the groupby function']"
157,How to filter rows in pandas by regex,"I would like to cleanly filter a dataframe using regex on one of the columns.

For a contrived example:

In [210]: foo = pd.DataFrame({'a' : [1,2,3,4], 'b' : ['hi', 'foo', 'fat', 'cat']})
In [211]: ...",https://stackoverflow.com/questions/15325182/how-to-filter-rows-in-pandas-by-regex,"['I would like to cleanly filter a dataframe using regex on one of the columns.', 'Use contains instead:', ""There is already a string handling function Series.str.startswith(). \nYou should try foo[foo.b.str.startswith('f')]."", 'Multiple column search with dataframe:', 'It may be a bit late, but this is now easier to do in Pandas by calling Series.str.match. The docs explain the difference between match, fullmatch and contains.', 'Thanks for the great answer @user3136169, here is an example of how that might be done also removing NoneType values.', 'Write a Boolean function that checks the regex and use apply on the column', 'Using str  slice']"
158,How to display pandas DataFrame of floats using a format string for columns?,"I would like to display a pandas dataframe with a given format using print() and the IPython display(). For example:

df = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],
                  ...",https://stackoverflow.com/questions/20937538/how-to-display-pandas-dataframe-of-floats-using-a-format-string-for-columns,"['I would like to display a pandas dataframe with a given format using print() and the IPython display(). For example:', 'yields', ""If you don't want to modify the dataframe, you could use a custom formatter for that column."", 'As of Pandas 0.17 there is now a styling system which essentially provides formatted views of a DataFrame using Python format strings:', 'Similar to unutbu above, you could also use applymap as follows:', 'I like using pandas.apply() with python format().', 'You can also set locale to your region and set float_format to use a currency format. This will automatically set $ sign for currency in USA.', 'summary:']"
159,"Pandas column of lists, create a row for each list element","I have a dataframe where some cells contain lists of multiple values. Rather than storing multiple
values in a cell, I'd like to expand the dataframe so that each item in the list gets its own row (...",https://stackoverflow.com/questions/27263805/pandas-column-of-lists-create-a-row-for-each-list-element,"[""I have a dataframe where some cells contain lists of multiple values. Rather than storing multiple\nvalues in a cell, I'd like to expand the dataframe so that each item in the list gets its own row (with the same values in all other columns). So if I have:"", 'Result:', 'A bit longer than I expected:', 'Series and DataFrame methods define a .explode() method that explodes lists into separate rows. See the docs section on Exploding a list-like column.', 'you can also use pd.concat and pd.melt for this:', ""Trying to work through Roman Pekar's solution step-by-step to understand it better, I came up with my own solution, which uses melt to avoid some of the confusing stacking and index resetting. I can't say that it's obviously a clearer solution though:"", ""For those looking for a version of Roman Pekar's answer that avoids manual column naming:"", 'I found the easiest way was to:', 'Very late answer but I want to add this:', 'Try this in pandas >=0.25 version', ""Also very late, but here is an answer from Karvy1 that worked well for me if you don't have pandas >=0.25 version:  https://stackoverflow.com/a/52511166/10740287""]"
160,Applying function with multiple arguments to create a new pandas column,I want to create a new column in a pandas data frame by applying a function to two existing columns. Following this answer I've been able to create a new column when I only need one column as an ...,https://stackoverflow.com/questions/19914937/applying-function-with-multiple-arguments-to-create-a-new-pandas-column,"[""I want to create a new column in a pandas data frame by applying a function to two existing columns. Following this answer I've been able to create a new column when I only need one column as an argument:"", 'Alternatively, you can use numpy underlying function:', ""You can go with @greenAfrican example, if it's possible for you to rewrite your function. But if you don't want to rewrite your function, you can wrap it into anonymous function inside apply, like this:"", 'This solves the problem:', 'If you need to create multiple columns at once:', 'One more dict style clean syntax:']"
161,"python pandas: Remove duplicates by columns A, keeping the row with the highest value in column B","I have a dataframe with repeat values in column A.  I want to drop duplicates, keeping the row with the highest value in column B.

So this:

A B
1 10
1 20
2 30
2 40
3 10
Should turn into this:

A B
...",https://stackoverflow.com/questions/12497402/python-pandas-remove-duplicates-by-columns-a-keeping-the-row-with-the-highest,"['I have a dataframe with repeat values in column A.  I want to drop duplicates, keeping the row with the highest value in column B.', 'This takes the last. Not the maximum though:', 'The top answer is doing too much work and looks to be very slow for larger data sets. apply is slow and should be avoided if possible. ix is deprecated and should be avoided as well.', 'Simplest solution:', 'Try this:', 'I would sort the dataframe first with Column B descending, then drop duplicates for Column A and keep first', 'You can try this as well', ""I think in your case you don't really need a groupby. I would sort by descending order your B column, then drop duplicates at column A and if you want you can also have a new nice and\nclean index like that:"", ""Here's a variation I had to solve that's worth sharing: for each unique string in columnA I wanted to find the most common associated string in columnB."", 'When already given posts answer the question, I made a small change by adding the column name on which the max() function is applied for better code readability.', 'Easiest way to do this:', 'this also works:', ""I am not going to give you the whole answer (I don't think you're looking for the parsing and writing to file part anyway), but a pivotal hint should suffice: use python's set() function, and then sorted() or .sort() coupled with .reverse():""]"
162,Pandas get topmost n records within each group,"Suppose I have pandas DataFrame like this:

>>> df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})
>>> df
   id  value
0   1      1
1   1      2
2   1      3
3 ...",https://stackoverflow.com/questions/20069009/pandas-get-topmost-n-records-within-each-group,"['Suppose I have pandas DataFrame like this:', ""Did you try df.groupby('id').head(2)"", 'Since 0.14.1, you can now do nlargest and nsmallest on a groupby object:', 'Sometimes sorting the whole data ahead is very time consuming. \nWe can groupby first and doing topk for each group:']"
163,Format / Suppress Scientific Notation from Python Pandas Aggregation Results,"How can one modify the format for the output from a groupby operation in pandas that produces scientific notation for very large numbers? 

I know how to do string formatting in python but I'm at a ...",https://stackoverflow.com/questions/21137150/format-suppress-scientific-notation-from-python-pandas-aggregation-results,"['How can one modify the format for the output from a groupby operation in pandas that produces scientific notation for very large numbers?', 'Granted, the answer I linked in the comments is not very helpful. You can specify your own string converter like so.', ""Here is another way of doing it, similar to Dan Allan's answer but without the lambda function:"", 'You can use round function just to suppress scientific notation for specific dataframe:', 'If you want to style the output of a data frame in a jupyter notebook cell, you can set the display style on a per-dataframe basis:', 'If you would like to use the values, say as part of csvfile csv.writer, the numbers can be formatted before creating a list:', 'I had multiple dataframes with different floating point, so thx to Allans idea made dynamic length.']"
164,python dataframe pandas drop column using int,"I understand that to drop a column you use df.drop('column name', axis=1). Is there a way to drop a column using a numerical index instead of the column name?",https://stackoverflow.com/questions/20297317/python-dataframe-pandas-drop-column-using-int,"[""I understand that to drop a column you use df.drop('column name', axis=1). Is there a way to drop a column using a numerical index instead of the column name?"", 'You can delete column on i index like this:', 'Drop multiple columns like this:', 'If there are multiple columns with identical names, the solutions given here so far will remove all of the columns,  which may not be what one is looking for. This may be the case if one is trying to remove duplicate columns except one instance. The example below clarifies this situation:', 'If you have two columns with the same name. One simple way is to manually rename the columns like this:-', 'You need to identify the columns based on their position in dataframe. For example, if you want to drop (del) column number 2,3 and 5, it will be,', 'if you really want to do it with integers (but why?), then you could build a dictionary.', ""You can use the following line to drop the first two columns (or any column you don't need):"", ""You can simply supply columns parameter to df.drop command so you don't to specify axis in that case, like so"", 'Since there can be multiple columns with same name , we should first rename the columns.\nHere is code for the solution.']"
165,Drop all duplicate rows across multiple columns in Python Pandas,"The pandas drop_duplicates function is great for ""uniquifying"" a dataframe. However, one of the keyword arguments to pass is take_last=True or take_last=False, while I would like to drop all rows ...",https://stackoverflow.com/questions/23667369/drop-all-duplicate-rows-across-multiple-columns-in-python-pandas,"['The pandas drop_duplicates function is great for ""uniquifying"" a dataframe. However, one of the keyword arguments to pass is take_last=True or take_last=False, while I would like to drop all rows which are duplicates across a subset of columns. Is this possible?', 'This is much easier in pandas now with drop_duplicates and the keep parameter.', ""Just want to add to Ben's answer on drop_duplicates:"", 'If you want result to be stored in another dataset:', 'use groupby and filter', 'Actually, drop rows 0 and 1 only requires (any observations containing matched A and C is kept.):', 'Try these various things']"
166,Compare two DataFrames and output their differences side-by-side,"I am trying to highlight exactly what changed between two dataframes.

Suppose I have two Python Pandas dataframes:

""StudentRoster Jan-1"":
id   Name   score                    isEnrolled           ...",https://stackoverflow.com/questions/17095101/compare-two-dataframes-and-output-their-differences-side-by-side,"['I am trying to highlight exactly what changed between two dataframes.', 'The first part is similar to Constantine, you can get the boolean of which rows are empty*:', 'It is possible to use the DataFrame style property to highlight the background color of the cells where there is a difference.', ""This answer simply extends @Andy Hayden's, making it resilient to when numeric fields are nan, and wrapping it up into a function."", 'prints', 'I have faced this issue, but found an answer before finding this post :', 'If your two dataframes have the same ids in them, then finding out what changed is actually pretty easy. Just doing frame1 != frame2 will give you a boolean DataFrame where each True is data that has changed. From that, you could easily get the index of each changed row by doing changedids = frame1.index[np.any(frame1 != frame2,axis=1)].', 'A different approach using concat and drop_duplicates:', ""With pandas 1.1, you could essentially replicate Ted Petrou's output with a single function call. Example taken from the docs:"", ""After fiddling around with @journois's answer, I was able to get it to work using MultiIndex instead of Panel due to Panel's deprication."", 'Extending answer of @cge, which is pretty cool for more readability of result:', 'Here is another way using select and merge:', 'If you found this thread trying to compare data fames in tests, then take a look at assert_frame_equal method: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.testing.assert_frame_equal.html', 'A function that finds asymmetrical  difference between two data frames is implemented below:\n(Based on set difference for pandas)\nGIST: https://gist.github.com/oneryalcin/68cf25f536a25e65f0b3c84f9c118e03', 'import pandas as pd\nimport numpy as np']"
167,Select rows in pandas MultiIndex DataFrame,"What are the most common pandas ways to select/filter rows of a dataframe whose index is a MultiIndex?
Slicing based on a single value/label
Slicing based on multiple labels from one or more levels
...",https://stackoverflow.com/questions/53927460/select-rows-in-pandas-multiindex-dataframe,"['What are the most common pandas ways to select/filter rows of a dataframe whose index is a MultiIndex?', 'Note\nThis post will be structured in the following manner:', ""Recently I came across a use case where I had a 3+ level multi-index dataframe in which I couldn't make any of the solutions above produce the results I was looking for. It's quite possible that the above solutions do of course work for my use case, and I tried several, however I was unable to get them to work with the time I had available.""]"
168,NumPy or Pandas: Keeping array type as integer while having a NaN value,"Is there a preferred way to keep the data type of a numpy array fixed as int (or int64 or whatever), while still having an element inside listed as numpy.NaN?

In particular, I am converting an in-...",https://stackoverflow.com/questions/11548005/numpy-or-pandas-keeping-array-type-as-integer-while-having-a-nan-value,"['Is there a preferred way to keep the data type of a numpy array fixed as int (or int64 or whatever), while still having an element inside listed as numpy.NaN?', 'This capability has been added to pandas (beginning with version 0.24):\nhttps://pandas.pydata.org/pandas-docs/version/0.24/whatsnew/v0.24.0.html#optional-integer-na-support', ""NaN can't be stored in an integer array. This is a known limitation of pandas at the moment; I have been waiting for progress to be made with NA values in NumPy (similar to NAs in R), but it will be at least 6 months to a year before NumPy gets these features, it seems:"", 'If performance is not the main issue, you can store strings instead.', ""This is not a solution for all cases, but mine (genomic coordinates) I've resorted to using 0 as NaN"", 'Functionality to support NaN in integer series will be available in v0.24 upwards. There\'s information on this in the v0.24 ""What\'s New"" section, and more details under Nullable Integer Data Type.', 'This is now possible, since pandas v 0.24.0', 'Just wanted to add that in case you are trying to convert a float (1.143) vector to integer (1) that has NA converting to the new \'Int64\' dtype will give you an error. In order to solve this you have to round the numbers and then do "".astype(\'Int64\')""', 'If there are blanks in the text data, columns that would normally be integers will be cast to floats as float64 dtype because int64 dtype cannot handle nulls. This can cause inconsistent schema if you are loading multiple files some with blanks (which will end up as float64 and others without which will end up as int64']"
169,pandas get column average/mean,"I can't get the average or mean of a column in pandas. A have a dataframe. Neither of things I tried below gives me the average of the column weight

>>> allDF 
         ID           ...",https://stackoverflow.com/questions/31037298/pandas-get-column-average-mean,"[""I can't get the average or mean of a column in pandas. A have a dataframe. Neither of things I tried below gives me the average of the column weight"", 'If you only want the mean of the weight column, select the column (which is a Series) and call .mean():', 'Try df.mean(axis=0) , axis=0 argument calculates the column wise mean of the dataframe so the result will be axis=1 is row wise mean so you are getting multiple values.', 'Do try to give print (df.describe()) a shot. I hope it will be very helpful to get an overall description of your dataframe.', 'you can use', 'You can also access a column using the dot notation (also called attribute access) and then calculate its mean:', 'Mean for each column in  df :', 'Additionally if you want to get the round value after finding the mean.', 'You can use either of the two statements below:', ""You can simply go for:\n        df.describe()\nthat will provide you with all the relevant details you need, but to find the min, max or average value of a particular column (say 'weights' in your case), use:""]"
170,Logical operators for boolean indexing in Pandas,"I'm working with boolean index in Pandas.
The question is why the statement:

a[(a['some_column']==some_number) & (a['some_other_column']==some_other_number)]
works fine whereas

a[(a['...",https://stackoverflow.com/questions/21415661/logical-operators-for-boolean-indexing-in-pandas,"[""I'm working with boolean index in Pandas.\nThe question is why the statement:"", 'When you say', ""Python's and, or and not logical operators are designed to work with scalars. So Pandas had to do one better and override the bitwise operators to achieve vectorized (element-wise) version of this functionality."", 'Logical operators for boolean indexing in Pandas']"
171,Pandas create empty DataFrame with only column names,"I have a dynamic DataFrame which works fine, but when there are no data to be added into the DataFrame I get an error. And therefore I need a solution to create an empty DataFrame with only the column ...",https://stackoverflow.com/questions/44513738/pandas-create-empty-dataframe-with-only-column-names,"['I have a dynamic DataFrame which works fine, but when there are no data to be added into the DataFrame I get an error. And therefore I need a solution to create an empty DataFrame with only the column names.', 'You can create an empty DataFrame with either column names or an Index:', 'Are you looking for something like this?', 'df.to_html() has a columns parameter.']"
172,Splitting dictionary/list inside a Pandas Column into Separate Columns,"I have data saved in a postgreSQL database. I am querying this data using Python2.7 and turning it into a Pandas DataFrame. However, the last column of this dataframe has a dictionary (or list?) of ...",https://stackoverflow.com/questions/38231591/splitting-dictionary-list-inside-a-pandas-column-into-separate-columns,"['I have data saved in a postgreSQL database. I am querying this data using Python2.7 and turning it into a Pandas DataFrame. However, the last column of this dataframe has a dictionary (or list?) of values within it. The DataFrame looks like this:', ""To convert the string to an actual dict, you can do df['Pollutant Levels'].map(eval). Afterwards, the solution below can be used to convert the dict to different columns."", 'I know the question is quite old, but I got here searching for answers. There is actually a better (and faster) way now of doing this using json_normalize:', 'Try this:  The data returned from SQL has to converted into a Dict. \nor could it be  ""Pollutant Levels""  is now Pollutants\'', ""Merlin's answer is better and super easy, but we don't need a lambda function. The evaluation of dictionary can be safely ignored by either of the following two ways as illustrated below:"", ""I strongly recommend the method extract the column 'Pollutants':"", 'You can use join with pop + tolist. Performance is comparable to concat with drop + tolist, but some may find this syntax cleaner:', 'One line solution is following:', ""my_df = pd.DataFrame.from_dict(my_dict, orient='index', columns=['my_col'])"", ""I've concatenated those steps in a method, you have to pass only the dataframe and the column which contains the dict to expand:"", 'speed comparison for a large dataset of 10 million rows']"
173,How to get rid of “Unnamed: 0” column in a pandas DataFrame?,"I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0. 

file.csv
,A,B,C
0,1,2,3
1,4,5,6
2,7,8,9
The CSV is read with this:

pd.read_csv('...",https://stackoverflow.com/questions/36519086/how-to-get-rid-of-unnamed-0-column-in-a-pandas-dataframe,"['I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0.', ""It's the index column, pass index=False to not write it out, see the docs"", ""This issue most likely manifests because your CSV was saved along with its RangeIndex (which usually doesn't have a name). The fix would actually need to be done when saving the DataFrame, but this isn't always an option."", 'Another case that this might be happening is if your data was improperly written to your csv to have each row end with a comma. This will leave you with an unnamed column Unnamed: x at the end of your data when you try to read it into a df.', 'To get ride of all Unnamed columns, you can also use regex such as df.drop(df.filter(regex=""Unname""),axis=1, inplace=True)', ""Simply delete that column using: del df['column_name']"", 'You can do the following with Unnamed Columns:', 'Simple do this:']"
174,How to filter rows containing a string pattern from a Pandas dataframe [duplicate],"Assume we have a data frame in Python Pandas that looks like this:

df = pd.DataFrame({'vals': [1, 2, 3, 4], 'ids': [u'aball', u'bball', u'cnut', u'fball']})
Or, in table form:

ids    vals
aball   1
...",https://stackoverflow.com/questions/27975069/how-to-filter-rows-containing-a-string-pattern-from-a-pandas-dataframe,"['Assume we have a data frame in Python Pandas that looks like this:', 'Step-by-step explanation (from inner to outer):', 'If you want to set the column you filter on as a new index, you could also consider to use .filter; if you want to keep it as a separate column then str.contains is the way to go.']"
175,Find element's index in pandas Series,"I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)

I.e., I'd ...",https://stackoverflow.com/questions/18327624/find-elements-index-in-pandas-series,"[""I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)"", 'Though I admit that there should be a better way to do that, but this at least avoids iterating and looping through the object and moves it to the C level.', 'Converting to an Index, you can use get_loc', 'This works if you know 7 is there in advance. You can check this with\n(myseries==7).any()', ""I'm impressed with all the answers here.  This is not a new answer, just an attempt to summarize the timings of all these methods.  I considered the case of a series with 25 elements and assumed the general case where the index could contain any values and you want the index value corresponding to the search value which is towards the end of the series."", 'Another way to do this, although equally unsatisfying is:', 'If you use numpy, you can get an array of the indecies that your value is found:', 'you can use Series.idxmax()', ""Another way to do it that hasn't been mentioned yet is the tolist method:"", 'This is the most native and scalable approach I could find:', 'Often your value occurs at multiple indices:']"
176,Random row selection in Pandas dataframe,"Is there a way to select random rows from a DataFrame in Pandas.
In R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at ...",https://stackoverflow.com/questions/15923826/random-row-selection-in-pandas-dataframe,"['Is there a way to select random rows from a DataFrame in Pandas.', 'Something like this?', 'With pandas version 0.16.1 and up, there is now a DataFrame.sample method built-in:', 'As of v0.20.0, you can use pd.DataFrame.sample, which can be used to return a random sample of a fixed number rows, or a percentage of rows:', 'The best way to do this is with the sample function from the random module,', 'Actually this will give you repeated indices np.random.random_integers(0, len(df), N) where N is a large number.', 'Below line will randomly select n number of rows out of the total existing row numbers from the dataframe df without replacement.']"
177,How to loop over grouped Pandas dataframe?,"DataFrame:

  c_os_family_ss c_os_major_is l_customer_id_i
0      Windows 7                         90418
1      Windows 7                         90418
2      Windows 7                         90418
...",https://stackoverflow.com/questions/27405483/how-to-loop-over-grouped-pandas-dataframe,"['DataFrame:', ""df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)) does already return a dataframe, so you cannot loop over the groups anymore."", 'Here is an example of iterating over a pd.DataFrame grouped by the column atable. For an sample usecase, ""create"" statements for an SQL database are generated within the for loop:', 'You can iterate over the index values if your dataframe has already been created.']"
178,Pandas: Looking up the list of sheets in an excel file,"The new version of Pandas uses the following interface to load Excel files:

read_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])
but what if I don't know the sheets that are ...",https://stackoverflow.com/questions/17977540/pandas-looking-up-the-list-of-sheets-in-an-excel-file,"['The new version of Pandas uses the following interface to load Excel files:', 'You can still use the ExcelFile class (and the sheet_names attribute):', 'You should explicitly  specify the second parameter (sheetname) as None. like this:', ""This is the fastest way I have found, inspired by @divingTobi's answer. All The answers based on xlrd, openpyxl or pandas are slow for me, as they all load the whole file first."", ""Building on @dhwanil_shah 's answer, you do not need to extract the whole file. With zf.open it is possible to read from a zipped file directly."", ""I have tried xlrd, pandas, openpyxl and other such libraries and all of them seem to take exponential time as the file size increase as it reads the entire file. The other solutions mentioned above where they used 'on_demand' did not work for me. If you just want to get the sheet names initially, the following function works for xlsx files."", ""For a 5MB Excel file I'm working with, load_workbook without the read_only flag took 8.24s. With the read_only flag it only took 39.6 ms. If you still want to use an Excel library and not drop to an xml solution, that's much faster than the methods that parse the whole file.""]"
179,Replacing blank values (white space) with NaN in pandas,"I want to find all values in a Pandas dataframe that contain whitespace (any arbitrary amount) and replace those values with NaNs.

Any ideas how this can be improved?

Basically I want to turn this:

...",https://stackoverflow.com/questions/13445241/replacing-blank-values-white-space-with-nan-in-pandas,"['I want to find all values in a Pandas dataframe that contain whitespace (any arbitrary amount) and replace those values with NaNs.', 'I think df.replace() does the job, since pandas 0.13:', 'If you want to replace an empty string and records with only spaces, the correct answer is!:', 'How about:', 'I will did this:', 'Simplest of all solutions:', 'If you are exporting the data from the CSV file it can be as simple as this :', 'For a very fast and simple solution where you check equality against a single value, you can use the mask method.', ""These are all close to the right answer, but I wouldn't say any solve the problem while remaining most readable to others reading your code. I'd say that answer is a combination of BrenBarn's Answer and tuomasttik's comment below that answer. BrenBarn's answer utilizes isspace builtin, but does not support removing empty strings, as OP requested, and  I would tend to attribute that as the standard use case of replacing strings with null."", ""This worked for me.\nWhen I import my csv file I added na_values = ' '. Spaces are not included in the default NaN values."", 'This is not an elegant solution, but what does seem to work is saving to XLSX and then importing it back. The other solutions on this page did not work for me, unsure why.', 'This should work', 'you can also use a filter to do it.']"
180,Pandas DataFrame column to list [duplicate],"I am pulling a subset of data from a column based on conditions in another column being met.

I can get the correct values back but it is in pandas.core.frame.DataFrame.  How do I convert that to list?...",https://stackoverflow.com/questions/23748995/pandas-dataframe-column-to-list,"['I am pulling a subset of data from a column based on conditions in another column being met.', 'You can use the Series.to_list method.', ""I'd like to clarify a few things:"", 'You can use pandas.Series.tolist', 'The above solution is good if all the data is of same dtype. Numpy arrays are homogeneous containers. When you do df.values the output is an numpy array. So if the data has int and float in it then output will either have int or float and the columns will loose their original dtype. \nConsider df']"
181,pandas GroupBy columns with NaN (missing) values,"I have a DataFrame with many missing values in columns which I wish to groupby:

import pandas as pd
import numpy as np
df = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})

In [4]: df....",https://stackoverflow.com/questions/18429491/pandas-groupby-columns-with-nan-missing-values,"['I have a DataFrame with many missing values in columns which I wish to groupby:', 'This is mentioned in the Missing Data section of the docs:', 'From pandas 1.1 you have better control over this behavior, NA values are now allowed in the grouper using dropna=False:', ""Ancient topic, if someone still stumbles over this--another workaround is to convert via .astype(str) to string before grouping. That will conserve the NaN's."", 'I am not able to add a comment to M. Kiewisch since I do not have enough reputation points (only have 41 but need more than 50 to comment).', ""One small point to Andy Hayden's solution – it doesn't work (anymore?) because np.nan == np.nan yields False, so the replace function doesn't actually do anything."", ""All answers provided thus far result in potentially dangerous behavior as it is quite possible you select a dummy value that is actually part of the dataset. This is increasingly likely as you create groups with many attributes. Simply put, the approach doesn't always generalize well."", 'I answered this already, but some reason the answer was converted to a comment.  Nevertheless, this is the most efficient solution:']"
182,Can pandas automatically recognize dates?,"Today I was positively surprised by the fact that while reading data from a data file (for example) pandas is able to recognize types of values:

df = pandas.read_csv('test.dat', delimiter=r""\s+"", ...",https://stackoverflow.com/questions/17465045/can-pandas-automatically-recognize-dates,"['Today I was positively surprised by the fact that while reading data from a data file (for example) pandas is able to recognize types of values:', ""You should add parse_dates=True, or parse_dates=['column name'] when reading, thats usually enough to magically parse it. But there are always weird formats which need to be defined manually. In such a case you can also add a date parser function, which is the most flexible way possible."", ""Perhaps the pandas interface has changed since @Rutger answered, but in the version I'm using (0.15.2), the date_parser function receives a list of dates instead of a single value. In this case, his code should be updated like so:"", 'pandas read_csv method is great for parsing dates.  Complete documentation at http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html', 'You could use pandas.to_datetime() as recommended in the documentation for pandas.read_csv():', 'When merging two columns into a single datetime column, the accepted answer generates an error (pandas version 0.20.3), since the columns are sent to the date_parser function separately.', 'Yes - according to the pandas.read_csv documentation:', 'While loading csv file contain date column.We have two approach to to make pandas to \nrecognize date column i.e', 'If performance matters to you make sure you time:', ""In addition to what the other replies said, if you have to parse very large files with hundreds of thousands of timestamps, date_parser can prove to be a huge performance bottleneck, as it's a Python function called once per row. You can get a sizeable performance improvements by instead keeping the dates as text while parsing the CSV file and then converting the entire column into dates in one go:""]"
183,Pandas percentage of total with groupby,"This is obviously simple, but as a numpy newbe I'm getting stuck.

I have a CSV file that contains 3 columns, the State, the Office ID, and the Sales for that office.

I want to calculate the ...",https://stackoverflow.com/questions/23377108/pandas-percentage-of-total-with-groupby,"[""This is obviously simple, but as a numpy newbe I'm getting stuck."", ""Paul H's answer is right that you will have to make a second groupby object, but you can calculate the percentage in a simpler way -- just groupby the state_office and divide the sales column by its sum. Copying the beginning of Paul H's answer:"", 'You need to make a second groupby object that groups by the states, and then use the div method:', ""For conciseness I'd use the SeriesGroupBy:"", ""I think this needs benchmarking. Using OP's original DataFrame,"", '(This solution is inspired from this article https://pbpython.com/pandas_transform.html)', ""I know that this is an old question, but exp1orer's answer is very slow for datasets with a large number unique groups (probably because of the lambda).  I built off of their answer to turn it into an array calculation so now it's super fast! Below is the example code:"", 'I realize there are already good answers here.', 'The most elegant way to find percentages across columns or index is to use pd.crosstab.', 'You can sum the whole DataFrame and divide by the state total:', 'I think this would do the trick in 1 line:', ""Simple way I have used is a merge after the 2 groupby's then doing simple division."", 'Returns:', ""As someone who is also learning pandas I found the other answers a bit implicit as pandas hides most of the work behind the scenes. Namely in how the operation works by automatically matching up column and index names. This code should be equivalent to a step by step version of @exp1orer's accepted answer"", 'One-line solution:']"
184,Why were pandas merges in python faster than data.table merges in R in 2012?,"I recently came across the pandas library for python, which according to this benchmark performs very fast in-memory merges.  It's even faster than the data.table package in R (my language of choice ...",https://stackoverflow.com/questions/8991709/why-were-pandas-merges-in-python-faster-than-data-table-merges-in-r-in-2012,"[""I recently came across the pandas library for python, which according to this benchmark performs very fast in-memory merges.  It's even faster than the data.table package in R (my language of choice for analysis)."", 'It looks like Wes may have discovered a known issue in data.table when the number of unique strings (levels) is large: 10,000.', 'The reason pandas is faster is because I came up with a better algorithm, which is implemented very carefully using a fast hash table implementation - klib and in C/Cython to avoid the Python interpreter overhead for the non-vectorizable parts. The algorithm is described in some detail in my presentation: A look inside pandas design and development.', 'This topic is two years old but seems like a probable place for people to land when they search for comparisons of Pandas and data.table', ""There are great answers, notably made by authors of both tools that question asks about.\nMatt's answer explain the case reported in the question, that it was caused by a bug, and not an merge algorithm. Bug was fixed on the next day, more than a 7 years ago already.""]"
185,How to find which columns contain any NaN value in Pandas dataframe,"Given a pandas dataframe containing possible NaN values scattered here and there:

Question: How do I determine which columns contain NaN values? In particular, can I get a list of the column names ...",https://stackoverflow.com/questions/36226083/how-to-find-which-columns-contain-any-nan-value-in-pandas-dataframe,"['Given a pandas dataframe containing possible NaN values scattered here and there:', 'UPDATE: using Pandas 0.22.0', 'You can use df.isnull().sum(). It shows all columns and the total NaNs of each feature.', 'I had a problem where I had to many columns to visually inspect on the screen so a short list comp that filters and returns the offending columns is', ""In datasets having large number of columns its even better to see how many columns contain null values and how many don't."", 'i use these three lines of code to print out the column names which contain at least one null value:', 'Both of these should work:', 'This worked for me,']"
186,How to access pandas groupby dataframe by key,"How do I access the corresponding groupby dataframe in a groupby object by the key?

With the following groupby:

rand = np.random.RandomState(1)
df = pd.DataFrame({'A': ['foo', 'bar'] * 3,
           ...",https://stackoverflow.com/questions/14734533/how-to-access-pandas-groupby-dataframe-by-key,"['How do I access the corresponding groupby dataframe in a groupby object by the key?', 'You can use the get_group method:', ""Wes McKinney (pandas' author) in Python for Data Analysis provides the following recipe:"", 'Rather than', 'If you are looking for selective groupby objects then, do: gb_groups.keys(), and input desired key into the following key_list..', 'I was looking for a way to sample a few members of the GroupBy obj - had to address the posted question to get this done.']"
187,Turn Pandas Multi-Index into column,"I have a dataframe with 2 index levels:

                         value
Trial    measurement
    1              0        13
                   1         3
                   2         4
    2          ...",https://stackoverflow.com/questions/20110170/turn-pandas-multi-index-into-column,"['I have a dataframe with 2 index levels:', 'The reset_index() is a pandas DataFrame method that will transfer index values into the DataFrame as columns.  The default setting for the parameter is drop=False (which will keep the index values as columns).', ""This doesn't really apply to your case but could be helpful for others (like myself 5 minutes ago) to know. If one's multindex have the same name like this:"", 'As @cs95 mentioned in a comment, to drop only one level, use:', ""I ran into Karl's issue as well. I just found myself renaming the aggregated column then resetting the index."", 'There may be situations when df.reset_index() cannot be used (e.g., when you need the index, too). In this case, use index.get_level_values() to access index values directly:']"
188,Pandas: sum DataFrame rows for given columns,"I have the following DataFrame:

In [1]:

import pandas as pd
df = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})
df
Out [1]:
   a  b   c  d
0  1  2  dd  5
1  2  3  ee  ...",https://stackoverflow.com/questions/25748683/pandas-sum-dataframe-rows-for-given-columns,"['I have the following DataFrame:', 'You can just sum and set param axis=1 to sum the rows, this will ignore none numeric columns:', 'If you have just a few columns to sum, you can write:', 'Create a list of column names you want to add up.', 'This is a simpler way using iloc to select which columns to sum:', 'You can simply pass your dataframe into the following function:', 'Following syntax helped me when I have columns in sequence', 'The shortest and simpliest way here is to use']"
189,Efficient way to apply multiple filters to pandas DataFrame or Series,"I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together ...",https://stackoverflow.com/questions/13611065/efficient-way-to-apply-multiple-filters-to-pandas-dataframe-or-series,"['I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.', 'Pandas (and numpy) allow for boolean indexing, which will be much more efficient:', 'Chaining conditions creates long lines, which are discouraged by pep8.\nUsing the .query method forces to use strings, which is powerful but unpythonic and not very dynamic.', 'Simplest of All Solutions:', 'Since pandas 0.22 update, comparison options are available like:', 'Why not do this?', 'e can also select rows based on values of a column that are not in a list or any iterable. We will create boolean variable just like before, but now we will negate the boolean variable by placing ~ in the front.', 'If you want to check any/all of multiple columns for a value, you can do:']"
190,JSON to pandas DataFrame,"What I am trying to do is extract elevation data from a google maps API along a path specified by latitude and longitude coordinates as follows:

from urllib2 import Request, urlopen
import json

...",https://stackoverflow.com/questions/21104592/json-to-pandas-dataframe,"['What I am trying to do is extract elevation data from a google maps API along a path specified by latitude and longitude coordinates as follows:', 'I found a quick and easy solution to what I wanted using json_normalize() included in pandas 1.01.', 'Check this snip out.', 'You could first import your json data in a Python dictionnary :', 'Just a new version of the accepted answer, as python3.x does not support urllib2', 'Optimization of the accepted answer:', 'The problem is that you have several columns in the data frame that contain dicts with smaller dicts inside them. Useful Json is often heavily nested. I have been writing small functions that pull the info I want out into a new column. That way I have it in the format that I want to use.', 'Here is small utility class that converts JSON to DataFrame and back: Hope you find this helpful.', ""billmanH's solution helped me but didn't work until i switched from:"", 'Once you have the flattened DataFrame obtained by the accepted answer, you can make the columns a MultiIndex (""fancy multiline header"") like this:', ""I prefer a more generic method in which may be user doesn't prefer to give key 'results'. You can still flatten it by using a recursive approach of finding key having nested data or if you have key but your JSON is very nested. It is something like:""]"
191,"How to split data into 3 sets (train, validation and test)?","I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using train_test_split from sklearn.cross_validation, one can divide the data in two sets (train and test). However, I ...",https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test,"[""I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using train_test_split from sklearn.cross_validation, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data."", 'Numpy solution. We will shuffle the whole dataset first (df.sample(frac=1, random_state=42)) and then split our data set into the following parts:', ""Function was written to handle seeding of randomized set creation.  You should not rely on set splitting that doesn't randomize the sets."", 'However, one approach to dividing the dataset into train, test, cv with 0.6, 0.2, 0.2 would be to use the train_test_split method twice.', ""Here is a Python function that splits a Pandas dataframe into train, validation, and test dataframes with stratified sampling. It performs this split by calling scikit-learn's function train_test_split() twice."", 'It is very convenient to use train_test_split without performing reindexing after dividing to several sets and not writing some additional code. Best answer above does not mention that by separating two times using train_test_split not changing partition sizes won`t give initially intended partition:', 'In the case of supervised learning, you may want to split both X and y (where X is your input and y the ground truth output).\nYou just have to pay attention to shuffle X and y the same way before splitting.']"
192,Search for “does-not-contain” on a DataFrame in pandas,"I've done some searching and can't figure out how to filter a dataframe by df[""col""].str.contains(word), however I'm wondering if there is a way to do the reverse: filter a dataframe by that set's ...",https://stackoverflow.com/questions/17097643/search-for-does-not-contain-on-a-dataframe-in-pandas,"['I\'ve done some searching and can\'t figure out how to filter a dataframe by df[""col""].str.contains(word), however I\'m wondering if there is a way to do the reverse: filter a dataframe by that set\'s compliment. eg: to the effect of !(df[""col""].str.contains(word)).', 'You can use the invert (~) operator (which acts like a not for boolean data):', ""I was having trouble with the not (~) symbol as well, so here's another way from another StackOverflow thread:"", 'You can use Apply and Lambda to select rows where a column contains any thing in a list. For your scenario :', 'I had to get rid of the NULL values before using the command recommended by Andy above. An example:', 'I hope the answers are already posted', ""Additional to nanselm2's answer, you can use 0 instead of False:""]"
193,How to replace NaNs by preceding values in pandas DataFrame?,"Suppose I have a DataFrame with some NaNs:

>>> import pandas as pd
>>> df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])
>>> df
    0   1   2
0   1   2   3
1 ...",https://stackoverflow.com/questions/27905295/how-to-replace-nans-by-preceding-values-in-pandas-dataframe,"['Suppose I have a DataFrame with some NaNs:', 'You could use the fillna method on the DataFrame and specify the method as ffill (forward fill):', 'The accepted answer is perfect. I had a related but slightly different situation where I had to fill in forward but only within groups. In case someone has the same need, know that fillna works on a DataFrameGroupBy object.', ""You can use pandas.DataFrame.fillna with the method='ffill' option. 'ffill' stands for 'forward fill' and will propagate last valid observation forward. The alternative is 'bfill' which works the same way, but backwards."", ""One thing that I noticed when trying this solution is that if you have N/A at the start or the end of the array, ffill and bfill don't quite work.  You need both."", ""ffill now has it's own method pd.DataFrame.ffill"", 'Only one column version', 'Just agreeing with ffill method, but one extra info is that you can limit the forward fill with keyword argument limit.', 'In my case, we have time series from different devices but some devices could not send any value during some period. So we should create NA values for every device and time period and after that do fillna.', 'You can use fillna to remove or replace NaN values.']"
194,Pandas split column of lists into multiple columns,"I have a pandas DataFrame with one column:

import pandas as pd

df = pd.DataFrame(
    data={
        ""teams"": [
            [""SF"", ""NYG""],
            [""SF"", ""NYG""],
            [""SF"", ""NYG""],
      ...",https://stackoverflow.com/questions/35491274/pandas-split-column-of-lists-into-multiple-columns,"['I have a pandas DataFrame with one column:', 'You can use DataFrame constructor with lists created by to_list:', 'Much simpler solution:', 'This solution preserves the index of the df2 DataFrame, unlike any solution that uses tolist():', ""There seems to be a syntactically simpler way, and therefore easier to remember, as opposed to the proposed solutions. I'm assuming that the column is called 'meta' in a dataframe df:"", 'Based on the previous answers, here is another solution which returns the same result as df2.teams.apply(pd.Series) with a much faster run time:', ""The above solutions didn't work for me since I have nan observations in my dataframe. In my case df2[['team1','team2']] = pd.DataFrame(df2.teams.values.tolist(), index= df2.index) yields:"", 'list comprehension', ""Here's another solution using df.transform and df.set_index:""]"
195,Pandas dataframe fillna() only some columns in place,"I am trying to fill none values in a Pandas dataframe with 0's for only some subset of columns.

When I do:

import pandas as pd
df = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,...",https://stackoverflow.com/questions/38134012/pandas-dataframe-fillna-only-some-columns-in-place,"[""I am trying to fill none values in a Pandas dataframe with 0's for only some subset of columns."", 'You can select your desired columns and do it by assignment:', 'You can using dict , fillna with different value for different column', ""You can avoid making a copy of the object using Wen's solution and inplace=True:"", ""using the top answer produces a warning about making changes to a copy of a df slice. Assuming that you have other columns, a better way to do this is to pass a dictionary: \ndf.fillna({'A': 'NA', 'B': 'NA'}, inplace=True)"", ""Here's how you can do it all in one line:"", 'Or something like:', 'Sometimes this syntax wont work:']"
196,What is the most efficient way to create a dictionary of two pandas Dataframe columns?,"What is the most efficient way to organise the following pandas Dataframe:

data =

Position    Letter
1           a
2           b
3           c
4           d
5           e
into a dictionary like ...",https://stackoverflow.com/questions/17426292/what-is-the-most-efficient-way-to-create-a-dictionary-of-two-pandas-dataframe-co,"['What is the most efficient way to organise the following pandas Dataframe:', ""Speed comparion (using Wouter's method)"", 'I found a faster way to solve the problem, at least on realistically large datasets using:\ndf.set_index(KEY).to_dict()[VALUE]', ""In Python 3.6 the fastest way is still the WouterOvermeire one. Kikohs' proposal is slower than the other two options."", 'Explaining solution: dict(sorted(df.values.tolist()))']"
197,Rename Pandas DataFrame Index,"I've a csv file without header, with a DateTime index. I want to rename the index and column name, but with df.rename() only the column name is renamed. Bug? I'm on version 0.12.0

In [2]: df = pd....",https://stackoverflow.com/questions/19851005/rename-pandas-dataframe-index,"[""I've a csv file without header, with a DateTime index. I want to rename the index and column name, but with df.rename() only the column name is renamed. Bug? I'm on version 0.12.0"", ""The rename method takes a dictionary for the index which applies to index values.\nYou want to rename to index level's name:"", 'The currently selected answer does not mention the rename_axis method which can be used to rename the index and column levels.', 'or', 'In Pandas version 0.13 and greater the index level names are immutable (type FrozenList) and can no longer be set directly.  You must first use Index.rename() to apply the new index level names to the Index and then use DataFrame.reindex() to apply the new index to the DataFrame.  Examples:', 'You can also use Index.set_names as follows:', 'If you want to use the same mapping for renaming both columns and index you can do:', 'Is the only one that does the job for me (pandas 0.22.0).\nWithout the inplace=True, the name of the index is not set in my case.', 'For Single Index :', 'you can use index and columns attributes of pandas.DataFrame. NOTE: number of elements of list must match the number of rows/columns.']"
198,Pandas dataframe get first row of each group,"I have a pandas DataFrame like following.

df = pd.DataFrame({'id' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],
                'value'  : [""first"",""second"",""second"",""first"",
                            ""...",https://stackoverflow.com/questions/20067636/pandas-dataframe-get-first-row-of-each-group,"['I have a pandas DataFrame like following.', 'If you need id as column:', 'This will give you the second row of each group (zero indexed, nth(0) is the same as first()):', ""I'd suggest to use .nth(0) rather than .first() if you need to get the first row."", ""If you only need the first row from each group we can do with drop_duplicates, Notice the function default method keep='first'."", 'maybe this is what you want']"
199,check if variable is dataframe,"when my function f is called with a variable I want to check if var is a pandas dataframe:

def f(var):
    if var == pd.DataFrame():
        print ""do stuff""
I guess the solution might be quite ...",https://stackoverflow.com/questions/14808945/check-if-variable-is-dataframe,"['when my function f is called with a variable I want to check if var is a pandas dataframe:', 'Use isinstance, nothing else:', 'Use the built-in isinstance() function.']"
200,python pandas: apply a function with arguments to a series,"I want to apply a function with arguments to a series in python pandas:

x = my_series.apply(my_function, more_arguments_1)
y = my_series.apply(my_function, more_arguments_2)
...
The documentation ...",https://stackoverflow.com/questions/12182744/python-pandas-apply-a-function-with-arguments-to-a-series,"['I want to apply a function with arguments to a series in python pandas:', 'Newer versions of pandas do allow you to pass extra arguments (see the new documentation). So now you can do:', 'Steps:', 'You can pass any number of arguments to the function that apply is calling through either unnamed arguments, passed as a tuple to the args parameter, or through other keyword arguments internally captured as a dictionary by the kwds parameter.']"
201,Pandas convert dataframe to array of tuples,"I have manipulated some data using pandas and now I want to carry out a batch save back to the database. This requires me to convert the dataframe into an array of tuples, with each tuple ...",https://stackoverflow.com/questions/9758450/pandas-convert-dataframe-to-array-of-tuples,"['I have manipulated some data using pandas and now I want to carry out a batch save back to the database. This requires me to convert the dataframe into an array of tuples, with each tuple corresponding to a ""row"" of the dataframe.', 'How about:', 'As of 17.1, the above will return a list of namedtuples.', 'A generic way:', 'Motivation\nMany data sets are large enough that we need to concern ourselves with speed/efficiency.  So I offer this solution in that spirit.  It happens to also be succinct.', ""Here's a vectorized approach (assuming the dataframe, data_set to be defined as df instead) that returns a list of tuples as shown:"", 'The most efficient and easy way:', ""This answer doesn't add any answers that aren't already discussed, but here are some speed results. I think this should resolve questions that came up in the comments. All of these look like they are O(n), based on these three values."", 'Changing the data frames list into a list of tuples.', 'More pythonic way:']"
202,Modifying a subset of rows in a pandas dataframe,"Assume I have a pandas DataFrame with two columns, A and B. I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0. How would I achieve that?

I tried the ...",https://stackoverflow.com/questions/12307099/modifying-a-subset-of-rows-in-a-pandas-dataframe,"[""Assume I have a pandas DataFrame with two columns, A and B. I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0. How would I achieve that?"", 'Use .loc for label based indexing:', 'Here is from pandas docs on advanced indexing:', 'Starting from pandas 0.20 ix is deprecated. The right way is to use df.loc', ""For a massive speed increase, use NumPy's where function."", 'To replace multiples columns convert to numpy array using .values:']"
203,Pandas: Setting no. of max rows,"I have a problem viewing the following DataFrame: 

n = 100
foo = DataFrame(index=range(n))
foo['floats'] = np.random.randn(n)
foo
The problem is that it does not print all rows per default in ...",https://stackoverflow.com/questions/16424493/pandas-setting-no-of-max-rows,"['I have a problem viewing the following DataFrame:', 'Set display.max_rows:', 'Personally, I like setting the options directly with an assignment statement as it is easy to find via tab completion thanks to iPython. I find it hard to remember what the exact option names are, so this method works for me.', 'Does not work in Jupyter!\nInstead use:', ""It was already pointed in this comment and in this answer, but I'll try to give a more direct answer to the question:"", 'As @hanleyhansen noted in a comment, as of version 0.18.1, the display.height option is deprecated, and says ""use display.max_rows instead"". So you just have to configure it like this:', 'As in this answer to a similar question, there is no need to hack settings. It is much simpler to write:', 'to set unlimited number of rows use']"
204,pandas dataframe columns scaling with sklearn,"I have a pandas dataframe with mixed type columns, and I'd like to apply sklearn's min_max_scaler to some of the columns.  Ideally, I'd like to do these transformations in place, but haven't figured ...",https://stackoverflow.com/questions/24645153/pandas-dataframe-columns-scaling-with-sklearn,"[""I have a pandas dataframe with mixed type columns, and I'd like to apply sklearn's min_max_scaler to some of the columns.  Ideally, I'd like to do these transformations in place, but haven't figured out a way to do that yet.  I've written the following code that works:"", 'I am not sure if previous versions of pandas prevented this but now the following snippet works perfectly for me and produces exactly what you want without having to use apply', 'Like this?', ""As it is being mentioned in pir's comment - the .apply(lambda el: scale.fit_transform(el)) method will produce the following warning:"", 'This should work without depreciation warnings.', 'You can do it using  pandas only:', ""I know it's a very old comment, but still:"", ""(Tested for pandas 1.0.5)\nBased on @athlonshi answer (it had ValueError: could not convert string to float: 'big', on C column), full working example without warning:""]"
205,How to get the first column of a pandas DataFrame as a Series?,"I tried:

x=pandas.DataFrame(...)
s = x.take([0], axis=1)
And s gets a DataFrame, not a Series.",https://stackoverflow.com/questions/15360925/how-to-get-the-first-column-of-a-pandas-dataframe-as-a-series,"['I tried:', '===========================================================================', 'From v0.11+, ... use df.iloc.', 'You can get the first column as a Series by following code:', ""Isn't this the simplest way?"", 'This works great when you want to load a series from a csv file', 'where i is the position/number of the column(starting from 0).']"
206,How can I one hot encode in Python?,I have a machine learning classification problem with 80% categorical variables. Must I use one hot encoding if I want to use some classifier for the classification? Can i pass the data to a ...,https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python,"['I have a machine learning classification problem with 80% categorical variables. Must I use one hot encoding if I want to use some classifier for the classification? Can i pass the data to a classifier without the encoding?', ""Approach 1: You can use pandas' pd.get_dummies."", ""Much easier to use Pandas for basic one-hot encoding. If you're looking for more options you can use scikit-learn."", 'You can do it with numpy.eye and a using the array element selection mechanism:', 'Firstly, easiest way to one hot encode: use Sklearn.', 'One hot encoding with pandas is very easy:', 'You can use numpy.eye function.', 'pandas as has inbuilt function ""get_dummies"" to get one hot encoding of that particular column/s.', ""Here is a solution using DictVectorizer and the Pandas DataFrame.to_dict('records') method."", 'One-hot encoding requires bit more than converting the values to indicator variables. Typically ML process requires you to apply this coding several times to validation or test data sets and applying the model you construct to real-time observed data. You should store the mapping (transform) that was used to construct the model. A good solution would use the DictVectorizer or LabelEncoder (followed by get_dummies. Here is a function that you can use:', 'You can pass the data to catboost classifier without encoding. Catboost handles categorical variables itself by performing one-hot and target expanding mean encoding.', ""You can do the following as well. Note for the below you don't have to use pd.concat."", ""I know I'm late to this party, but the simplest way to hot encode a dataframe in an automated way is to use this function:"", 'I used this in my acoustic model:\nprobably this helps in ur model.', 'To add to other questions, let me provide how I did it with a Python 2.0 function using Numpy:', 'This works for me:', 'It can and it should be easy as :', ""Expanding @Martin Thoma's answer"", 'Here is a function to do one-hot-encoding without using numpy, pandas, or other packages. It takes a list of integers, booleans, or strings (and perhaps other types too).', 'Try this:', 'Here i tried with this approach :']"
207,"Pandas DataFrame: replace all values in a column, based on condition","I have a simple DataFrame like the following:
I want to select all values from the 'First Season' column and replace those that are over 1990 by 1. In this example, only Baltimore Ravens would have ...",https://stackoverflow.com/questions/31511997/pandas-dataframe-replace-all-values-in-a-column-based-on-condition,"['I have a simple DataFrame like the following:', 'You need to select that column:', 'A bit late to the party but still - I prefer using numpy where:', ""strange that nobody has this answer, the only missing part of your code is the ['First Season'] right after df and just remove your curly brackets inside."", ""for single condition, ie. ( 'employrate'] > 70 )"", 'Explanation:', 'We can update the First Season column in df with the following syntax:']"
208,Pandas read_csv from url,"I am using Python 3.4 with IPython and have the following code. I'm unable to read a csv-file from the given URL:

import pandas as pd
import requests

url=""https://github.com/cs109/2014_data/blob/...",https://stackoverflow.com/questions/32400867/pandas-read-csv-from-url,"[""I am using Python 3.4 with IPython and have the following code. I'm unable to read a csv-file from the given URL:"", 'From pandas 0.19.2 you can now just pass the url directly.', 'In the latest version of pandas (0.19.2) you can directly pass the url', 'As I commented you need to use a StringIO  object and decode i.e c=pd.read_csv(io.StringIO(s.decode(""utf-8""))) if using requests, you need to decode as .content returns bytes if you used .text you would just need to pass s as is s = requests.get(url).text c = pd.read_csv(StringIO(s)).', ""The problem you're having is that the output you get into the variable 's' is not a csv, but a html file. \nIn order to get the raw csv, you have to modify the url to:""]"
209,Find column whose name contains a specific string,"I have a dataframe with column names, and I want to find the one that contains a certain string, but does not exactly match it. I'm searching for 'spike' in column names like 'spike-2', 'hey spike', '...",https://stackoverflow.com/questions/21285380/find-column-whose-name-contains-a-specific-string,"[""I have a dataframe with column names, and I want to find the one that contains a certain string, but does not exactly match it. I'm searching for 'spike' in column names like 'spike-2', 'hey spike', 'spiked-in' (the 'spike' part is always continuous)."", 'Just iterate over DataFrame.columns, now this is an example in which you will end up with a list of column names that match:', 'This answer uses the DataFrame.filter method to do this without list comprehension:', ""You can also use df.columns[df.columns.str.contains(pat = 'spike')]"", 'You can also select by name, regular expression. Refer to: pandas.DataFrame.filter', 'You also can use this code:', 'Getting name and subsetting based on Start, Contains, and Ends:']"
210,Reading an Excel file in python using pandas,"I am trying to read an excel file this way :

newFile = pd.ExcelFile(PATH\FileName.xlsx)
ParsedData = pd.io.parsers.ExcelFile.parse(newFile)
which throws an error that says two arguments expected, I ...",https://stackoverflow.com/questions/17063458/reading-an-excel-file-in-python-using-pandas,"['I am trying to read an excel file this way :', 'Close: first you call ExcelFile, but then you call the .parse method and pass it the sheet name.', 'This is much simple and easy way.', 'Thought i should add here, that if you want to access rows or columns to loop through them, you do this:', 'I think this should satisfy your need:', 'You just need to feed the path to your file to pd.read_excel', 'Here is an updated method with syntax that is more common in python code. It also prevents you from opening the same file multiple times.', 'Loading an excel file without explicitly naming a sheet but instead giving the number of the sheet order (often one will simply load the first sheet) goes like:']"
211,How can I map True/False to 1/0 in a Pandas DataFrame?,"I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?",https://stackoverflow.com/questions/17383094/how-can-i-map-true-false-to-1-0-in-a-pandas-dataframe,"['I have a column in python pandas DataFrame that has boolean True/False values, but for further calculations I need 1/0 representation. Is there a quick pandas/numpy way to do that?', 'A succinct way to convert a single column of boolean values to a column of integers 1 or 0:', 'Just multiply your Dataframe by 1 (int)', 'True is 1 in Python, and likewise False is 0*:', 'You also can do this directly on Frames', 'You can use a transformation for your data frame:', 'Use Series.view for convert boolean to integers:', ""I had to map FAKE/REAL to 0/1 but couldn't find proper answer.""]"
212,Check if a value exists in pandas dataframe index,"I am sure there is an obvious way to do this but cant think of anything slick right now.

Basically instead of raising exception I would like to get True or False to see if a value exists in pandas df ...",https://stackoverflow.com/questions/23549231/check-if-a-value-exists-in-pandas-dataframe-index,"['I am sure there is an obvious way to do this but cant think of anything slick right now.', 'This should do the trick', 'Just for reference as it was something I was looking for, you can test for presence within the values or the index by appending the "".values"" method, e.g.', 'Multi index works a little different from single index.  Here are some methods for multi-indexed dataframe.', 'with DataFrame: df_data', 'Code below does not print boolean, but allows for dataframe subsetting by index... I understand this is likely not the most efficient way to solve the problem, but I (1) like the way this reads and (2) you can easily subset where df1 index exists in df2:']"
213,How can I plot separate Pandas DataFrames as subplots?,"I have a few Pandas DataFrames sharing the same value scale, but having different columns and indices. When invoking df.plot(), I get separate plot images. what I really want is to have them all in ...",https://stackoverflow.com/questions/22483588/how-can-i-plot-separate-pandas-dataframes-as-subplots,"[""I have a few Pandas DataFrames sharing the same value scale, but having different columns and indices. When invoking df.plot(), I get separate plot images. what I really want is to have them all in the same plot as subplots, but I'm unfortunately failing to come up with a solution to how and would highly appreciate some help."", 'You can manually create the subplots with matplotlib, and then plot the dataframes on a specific subplot using the ax keyword. For example for 4 subplots (2x2):', 'You can see e.gs. in the documentation demonstrating joris answer. Also from the documentation, you could also set subplots=True and layout=(,) within the pandas plot function:', 'You can use the familiar Matplotlib style calling a figure and subplot, but you simply need to specify the current axis using plt.gca(). An example:', 'You can plot multiple subplots of multiple pandas data frames using matplotlib with a simple trick of making a list of all data frame. Then using the for loop for plotting subplots.', 'You can use this:', ""You may not need to use Pandas at all. Here's a matplotlib plot of cat frequencies:"", 'Building on @joris response above, if you have already established a reference to the subplot, you can use the reference as well. For example,', 'Assumptions', 'Here is a working pandas subplot example, where modes is the column names of the dataframe.']"
214,pandas unique values multiple columns,"df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],
                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],
                   'Col3': np.random.random(5)})
What is the best ...",https://stackoverflow.com/questions/26977076/pandas-unique-values-multiple-columns,"[""What is the best way to return the unique values of 'Col1' and 'Col2'?"", 'pd.unique returns the unique values from an input array, or DataFrame column or index.', ""I have setup a DataFrame with a few simple strings in it's columns:"", 'Or:', 'An updated solution using numpy v1.13+ requires specifying the axis in np.unique if using multiple columns, otherwise the array is implicitly flattened.', 'Non-pandas solution: using set().', 'for those of us that love all things pandas, apply, and of course lambda functions:', ""here's another way"", ""The output will be\n['Mary', 'Joe', 'Steve', 'Bob', 'Bill']""]"
215,How to print a groupby object,"I want to print the result of grouping with Pandas.

I have a dataframe:

import pandas as pd
df = pd.DataFrame({'A': ['one', 'one', 'two', 'three', 'three', 'one'], 'B': range(6)})
print(df)

       ...",https://stackoverflow.com/questions/22691010/how-to-print-a-groupby-object,"['I want to print the result of grouping with Pandas.', 'Simply do:', ""If you're simply looking for a way to display it, you could use describe():"", 'I confirmed that the behavior of head() changes between version 0.12 and 0.13. That looks like a bug to me. I created an issue.', 'Another simple alternative:', 'In addition to previous answers:', 'Also, other simple alternative could be:', ""Thanks to Surya for good insights. I'd clean up his solution and simply do:"", 'In Jupyter Notebook, if you do the following, it prints a nice grouped version of the object. The apply method helps in creation of a multiindex dataframe.', 'Call list() on the GroupBy object', 'you cannot see the groupBy data directly by print statement but you can see by iterating over the group using for loop \ntry this code to see the group by data', ""df.groupby('key you want to group by').apply(print)"", 'I found a tricky way, just for brainstorm, see the code:', 'In python 3', 'to print all (or arbitrarily many) lines of the grouped df:']"
216,Pandas join issue: columns overlap but no suffix specified,"I have following 2 data frames:

df_a =

     mukey  DI  PI
0   100000  35  14
1  1000005  44  14
2  1000006  44  14
3  1000007  43  13
4  1000008  43  13

df_b = 
    mukey  niccdcd
0  190236        ...",https://stackoverflow.com/questions/26645515/pandas-join-issue-columns-overlap-but-no-suffix-specified,"['I have following 2 data frames:', ""Your error on the snippet of data you posted is a little cryptic, in that because there are no common values, the join operation fails because the values don't overlap it requires you to supply a suffix for the left and right hand side:"", 'The .join() function is using the index of the passed as argument dataset, so you should use set_index or use .merge function instead.', 'This error indicates that the two tables have the 1 or more column names that have the same column name. The error message translates to: ""I can see the same column in both tables but you haven\'t told me to rename either before bringing one of them in""', 'Mainly join is used exclusively to join based on the index,not on the attribute names,so change the attributes names in two different dataframes,then try to join,they will be joined,else this error is raised']"
217,ImportError: No module named dateutil.parser,"I am receiving the following error when importing pandas in a Python program

monas-mbp:book mona$ sudo pip install python-dateutil
Requirement already satisfied (use --upgrade to upgrade): python-...",https://stackoverflow.com/questions/20853474/importerror-no-module-named-dateutil-parser,"['I am receiving the following error when importing pandas in a Python program', 'On Ubuntu you may need to install the package manager pip first:', 'For Python 3:', 'You can find the dateutil package at https://pypi.python.org/pypi/python-dateutil. Extract it to somewhere and run the command:', 'For Python 3 above, use:', ""If you're using a virtualenv, make sure that you are running pip from within the virtualenv."", 'None of the solutions worked for me. If you are using PIP do:', 'In Ubuntu 18.04 for Python2:', ""i have same issues on my MacOS and it's work for me to try install python-dateutil"", 'If you are using Pipenv, you may need to add this to your Pipfile:', 'I had the similar problem. This is the stack trace:']"
218,pandas: multiple conditions while indexing data frame - unexpected behavior,"I am filtering rows in a dataframe by values in two columns.

For some reason the OR operator behaves like I would expect AND operator to behave and vice versa.

My test code:

import pandas as pd

df ...",https://stackoverflow.com/questions/22591174/pandas-multiple-conditions-while-indexing-data-frame-unexpected-behavior,"['I am filtering rows in a dataframe by values in two columns.', 'As you can see, the AND operator drops every row in which at least one\n  value equals -1. On the other hand, the OR operator requires both\n  values to be equal to -1 to drop them.', 'You can use query(), i.e.:', 'A little mathematical logic theory here:']"
219,python pandas remove duplicate columns,"What is the easiest way to remove duplicate columns from a dataframe?

I am reading a text file that has duplicate columns via:

import pandas as pd

df=pd.read_table(fname)
The column names are:

...",https://stackoverflow.com/questions/14984119/python-pandas-remove-duplicate-columns,"['What is the easiest way to remove duplicate columns from a dataframe?', ""There's a one line solution to the problem. This applies if some column names are duplicated and you wish to remove them:"", ""It sounds like you already know the unique column names. If that's the case, then df = df['Time', 'Time Relative', 'N2'] would work."", 'Transposing is inefficient for large DataFrames.  Here is an alternative:', ""If I'm not mistaken, the following does what was asked without the memory problems of the transpose solution and with fewer lines than @kalu 's function, keeping the first of any similarly named columns."", 'It looks like you were on the right path. Here is the one-liner you were looking for:', 'First step:- Read first row i.e all columns the remove all duplicate columns.', 'I ran into this problem where the one liner provided by the first answer worked well.  However, I had the extra complication where the second copy of the column had all of the data.  The first copy did not.', 'The way below will identify dupe columns to review what is going wrong building the dataframe originally.', ""Note that Gene Burinsky's answer (at the time of writing the selected answer) keeps the first of each duplicated column. To keep the last:"", 'Fast and easy way to drop the duplicated columns by their values:']"
220,How to load a tsv file into a Pandas DataFrame?,"I'm new to python and pandas.  I'm trying to get a tsv file loaded into a pandas DataFrame.  

This is what I'm trying and the error I'm getting:

>>> df1 = DataFrame(csv.reader(open('c:/~/...",https://stackoverflow.com/questions/9652832/how-to-load-a-tsv-file-into-a-pandas-dataframe,"[""I'm new to python and pandas.  I'm trying to get a tsv file loaded into a pandas DataFrame."", 'Note: As of 17.0 from_csv is discouraged: use pd.read_csv instead', 'As of 17.0 from_csv is discouraged.', 'Use read_table(filepath). The default separator is tab', 'Try this', 'open file, save as .csv and then apply', 'You can load the tsv file directly into pandas data frame by specifying delimitor and header.']"
221,Remove unwanted parts from strings in a column,"I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column.

Data looks like:

    time    result
1    09:00   +52A
2    10:00   +62B
3    11:00   +44a
4    12:00   +...",https://stackoverflow.com/questions/13682044/remove-unwanted-parts-from-strings-in-a-column,"['I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column.', '6 years after the original question was posted, pandas now has a good number of ""vectorised"" string functions that can succinctly perform these string manipulation operations.', ""i'd use the pandas replace function, very simple and powerful as you can use regex. Below i'm using the regex \\D to remove any non-digit characters but obviously you could get quite creative with regex."", 'In the particular case where you know the number of positions that you want to remove from the dataframe column, you can use string indexing inside a lambda function to get rid of that parts:', ""There's a bug here: currently cannot pass arguments to str.lstrip and str.rstrip:"", ""A very simple method would be to use the extract method to select all the digits. Simply supply it the regular expression '\\d+' which extracts any number of digits."", ""I often use list comprehensions for these types of tasks because they're often faster."", 'Suppose your DF is having those extra character in between numbers as well.The last entry.', 'Try this using regular expression:']"
222,Add missing dates to pandas dataframe,"My data can have multiple events on a given date or NO events on a date. I take these events, get a count by date and plot them.  However, when I plot them, my two series don't always match.    

idx =...",https://stackoverflow.com/questions/19324453/add-missing-dates-to-pandas-dataframe,"[""My data can have multiple events on a given date or NO events on a date. I take these events, get a count by date and plot them.  However, when I plot them, my two series don't always match."", 'You could use Series.reindex:', ""A quicker workaround is to use .asfreq().  This doesn't require creation of a new index to call within .reindex()."", ""One issue is that reindex will fail if there are duplicate values. Say we're working with timestamped data, which we want to index by date:"", 'An alternative approach is resample, which can handle duplicate dates in addition to missing dates.  For example:', ""Here's a nice method to fill in missing dates into a dataframe, with your choice of fill_value, days_back to fill in, and sort order (date_order) by which to sort the dataframe:""]"
223,Replacing Pandas or Numpy Nan with a None to use with MysqlDB,I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in ...,https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb,"[""I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType."", ""@bogatron has it right, you can use where, it's worth noting that you can do this natively in pandas:"", 'Credit goes to this guy here on this Github issue.', 'You can replace nan with None in your numpy array:', 'After stumbling around, this worked for me:', ""Just an addition to @Andy Hayden's answer:"", ""Another addition: be careful when replacing multiples and converting the type of the column back from object to float. If you want to be certain that your None's won't flip back to np.NaN's apply @andy-hayden's suggestion with using pd.where.\nIllustration of how replace can still go 'wrong':"", 'Quite old, yet I stumbled upon the very same issue.\nTry doing this:']"
224,Concatenate a list of pandas dataframes together,"I have a list of Pandas dataframes that I would like to combine into one Pandas dataframe.  I am using Python 2.7.10 and Pandas 0.16.2

I created the list of dataframes from:

import pandas as pd
dfs =...",https://stackoverflow.com/questions/32444138/concatenate-a-list-of-pandas-dataframes-together,"['I have a list of Pandas dataframes that I would like to combine into one Pandas dataframe.  I am using Python 2.7.10 and Pandas 0.16.2', 'Given that all the dataframes have the same columns, you can simply concat them:', 'If the dataframes DO NOT all have the same columns try the following:', 'You also can do it with functional programming:', 'concat also works nicely with a list comprehension pulled using the ""loc"" command against an existing dataframe']"
225,what is the most efficient way of counting occurrences in pandas?,"I have a large (about 12M rows) dataframe df with say:

df.columns = ['word','documents','frequency']
So the following ran in a timely fashion:

word_grouping = df[['word','frequency']].groupby('word'...",https://stackoverflow.com/questions/20076195/what-is-the-most-efficient-way-of-counting-occurrences-in-pandas,"['I have a large (about 12M rows) dataframe df with say:', ""I think df['word'].value_counts() should serve. By skipping the groupby machinery, you'll save some time. I'm not sure why count should be much slower than max. Both take some time to avoid missing values. (Compare with size.)"", ""When you want to count the frequency of categorical data in a column in pandas dataFrame use: df['Column_Name'].value_counts()"", ""Just an addition to the previous answers. Let's not forget that when dealing with real data there might be null values, so it's useful to also include those in the counting by using the option dropna=False (default is True)""]"
226,Multiple aggregations of the same column using pandas GroupBy.agg(),"Is there a pandas built-in way to apply two different aggregating functions f1, f2 to the same column df[""returns""], without having to call agg() multiple times?

Example dataframe:

import pandas as ...",https://stackoverflow.com/questions/12589481/multiple-aggregations-of-the-same-column-using-pandas-groupby-agg,"['Is there a pandas built-in way to apply two different aggregating functions f1, f2 to the same column df[""returns""], without having to call agg() multiple times?', 'You can simply pass the functions as a list:', 'TLDR; Pandas groupby.agg has a new, easier syntax for specifying (1) aggregations on multiple columns, and (2) multiple aggregations on a column. So, to do this for pandas >= 0.25, use', 'Would something like this work:']"
227,Python pandas: fill a dataframe row by row,"The simple task of adding a row to a pandas.DataFrame object seems to be hard to accomplish. There are 3 stackoverflow questions relating to this, none of which give a working answer.

Here is what I'...",https://stackoverflow.com/questions/17091769/python-pandas-fill-a-dataframe-row-by-row,"['The simple task of adding a row to a pandas.DataFrame object seems to be hard to accomplish. There are 3 stackoverflow questions relating to this, none of which give a working answer.', ""df['y'] will set a column"", ""My approach was, but I can't guarantee that this is the fastest solution."", 'This is a simpler version', 'If your input rows are lists rather than dictionaries, then the following is a simple solution:']"
228,Removing index column in pandas when reading a csv,"I have the following code which imports a CSV file.  There are 3 columns and I want to set the first two of them to variables.  When I set the second column to the variable ""efficiency"" the index ...",https://stackoverflow.com/questions/20107570/removing-index-column-in-pandas-when-reading-a-csv,"['I have the following code which imports a CSV file.  There are 3 columns and I want to set the first two of them to variables.  When I set the second column to the variable ""efficiency"" the index column is also tacked on.  How can I get rid of the index column?', ""DataFrames and Series always have an index. Although it displays alongside the column(s), it is not a column, which is why del df['index'] did not work."", 'When reading to and from your CSV file include the argument index=False so for example:', 'df.reset_index(drop=True, inplace=True)', 'You can set one of the columns as an index in case it is an ""id"" for example. \nIn this case the index column will be replaced by one of the columns you have chosen.', 'If your problem is same as mine where you just want to reset the column headers from 0 to column size. Do', ""you can specify which column is an index in your csv file by using index_col parameter of from_csv function\nif this doesn't solve you problem please provide example of your data"", ""One thing that i do is df=df.reset_index() \nthen df=df.drop(['index'],axis=1)""]"
229,how to check the dtype of a column in python pandas,"I need to use different functions to treat numeric columns and string columns. What I am doing now is really dumb:

allc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)
for ...",https://stackoverflow.com/questions/22697773/how-to-check-the-dtype-of-a-column-in-python-pandas,"['I need to use different functions to treat numeric columns and string columns. What I am doing now is really dumb:', 'You can access the data-type of a column with dtype:', 'In pandas 0.20.2 you can do:', 'I know this is a bit of an old thread but with pandas 19.02, you can do:', 'Asked question title is general, but authors use case stated in the body of the question is specific. So any other answers may be used.', 'If you want to mark the type of a dataframe column as a string, you can do:', 'To check the data types after, for example, an import from a file']"
230,pandas: How do I split text in a column into multiple rows?,I'm working with a large csv file and the next to last column has a string of text that I want to split by a specific delimiter. I was wondering if there is a simple way to do this using pandas or ...,https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows,"[""I'm working with a large csv file and the next to last column has a string of text that I want to split by a specific delimiter. I was wondering if there is a simple way to do this using pandas or python?"", 'This splits the Seatblocks by space and gives each its own row.', 'Differently from Dan, I consider his answer quite elegant... but unfortunately it is also very very inefficient. So, since the question mentioned ""a large csv file"", let me suggest to try in a shell Dan\'s solution:', 'Another similar solution with chaining is use reset_index and rename:', 'Another approach would be like this:', 'Can also use groupby() with no need to join and stack().', 'This seems a far easier method than those suggested elsewhere in this thread.']"
231,Use .corr to get the correlation between two columns,"I have the following pandas dataframe Top15:
    

I create a column that estimates the number of citable documents per person:

Top15['PopEst'] = Top15['Energy Supply'] / Top15['Energy Supply per ...",https://stackoverflow.com/questions/42579908/use-corr-to-get-the-correlation-between-two-columns,"['I have the following pandas dataframe Top15:', 'Without actual data it is hard to answer the question but I guess you are looking for something like this:', 'I ran into the same issue.\nIt appeared Citable Documents per Person was a float, and python skips it somehow by default. All the other columns of my dataframe were in numpy-formats, so I solved it by converting the columnt to np.float64', 'My solution would be after converting data to numerical type:', 'It works like this:', 'If you want the correlations between all pairs of columns, you could do something like this:', 'When you call this:', ""I solved this problem by changing the data type. If you see the 'Energy Supply per Capita' is a numerical type while the 'Citable docs per Capita' is an object type. I converted the column to float using astype. I had the same problem with some np functions: count_nonzero and sum worked while mean and std didn't."", ""changing 'Citable docs per Capita' to numeric before correlation will solve the problem.""]"
232,How do I get a list of all the duplicate items using pandas in python?,"I have a list of items that likely has some export issues.  I would like to get a list of the duplicate items so I can manually compare them.  When I try to use pandas duplicated method, it only ...",https://stackoverflow.com/questions/14657241/how-do-i-get-a-list-of-all-the-duplicate-items-using-pandas-in-python,"['I have a list of items that likely has some export issues.  I would like to get a list of the duplicate items so I can manually compare them.  When I try to use pandas duplicated method, it only returns the first duplicate.  Is there a a way to get all of the duplicates and not just the first one?', 'Method #1: print all rows where the ID is one of the IDs in duplicated:', ""With Pandas version 0.17, you can set 'keep = False' in the duplicated function to get all the duplicate items."", ""it'll return all duplicated rows back to you."", 'As I am unable to comment, hence posting as a separate answer', 'This worked for me', 'Using an element-wise logical or and setting the take_last argument of the pandas duplicated method to both True and False you can obtain a set from your dataframe that includes all of the duplicates.', 'This may not be a solution to the question, but to illustrate examples:', 'sort(""ID"") does not seem to be working now, seems deprecated as per sort doc, so use sort_values(""ID"") instead to sort after duplicate filter, as following:', 'For my database duplicated(keep=False) did not work until the column was sorted.', ""df[df.duplicated(['ID'])==True].sort_values('ID')""]"
233,iterating row by row through a pandas dataframe [duplicate],"I'm looking to iterate row by row through a pandas DataFrame.  The way I'm doing it so far is as follows:

for i in df.index:
    do_something(df.ix[i])
Is there a more performant and/or more ...",https://stackoverflow.com/questions/10729210/iterating-row-by-row-through-a-pandas-dataframe,"['I have a DataFrame from Pandas:', 'DataFrame.iterrows is a generator which yields both the index and row (as a Series):', 'Iteration in Pandas is an anti-pattern and is something you should only do when you have exhausted every other option. You should not use any function with ""iter"" in its name for more than a few thousand rows or you will have to get used to a lot of waiting.', 'First consider if you really need to iterate over rows in a DataFrame. See this answer for alternatives.', 'You should use df.iterrows(). Though iterating row-by-row is not especially efficient since Series objects have to be created.', 'While iterrows() is a good option, sometimes itertuples() can be much faster:', 'You can also use df.apply() to iterate over rows and access multiple columns for a function.', 'You can use the df.iloc function as follows:', 'I was looking for How to iterate on rows and columns and ended here so:', 'If you really have to iterate a Pandas dataframe, you will probably want to avoid using iterrows(). There are different methods and the usual iterrows() is far from being the best. itertuples() can be 100 times faster.', 'You can write your own iterator that implements namedtuple', 'To loop all rows in a dataframe you can use:', 'Sometimes a useful pattern is:', 'To loop all rows in a dataframe and use values of each row conveniently, namedtuples can be converted to ndarrays. For example:', 'For both viewing and modifying values, I would use iterrows(). In a for loop and by using tuple unpacking (see the example: i, row), I use the row for only viewing the value and use i with the loc method when I want to modify values. As stated in previous answers, here you should not modify something you are iterating over.', ""There is a way to iterate throw rows while getting a DataFrame in return, and not a Series. I don't see anyone mentioning that you can pass index as a list for the row to be returned as a DataFrame:"", 'cs95 shows that Pandas vectorization far outperforms other Pandas methods for computing stuff with dataframes.', 'There are so many ways to iterate over the rows in Pandas dataframe. One very simple and intuitive way is:', ""You can also do NumPy indexing for even greater speed ups. It's not really iterating but works much better than iteration for certain applications."", 'In short', 'This example uses iloc to isolate each digit in the data frame.', ""Some libraries (e.g. a Java interop library that I use) require values to be passed in a row at a time, for example, if streaming data. To replicate the streaming nature, I 'stream' my dataframe values one by one, I wrote the below, which comes in handy from time to time."", 'Along with the great answers in this post I am going to propose Divide and Conquer approach, I am not writing this answer to abolish the other great answers but to fulfill them with another approach which was working efficiently for me. It has two steps of splitting and merging the pandas dataframe:', 'The easiest way, use the apply function']"
234,datetime dtypes in pandas read_csv,"I'm reading in a csv file with multiple datetime columns.  I'd need to set the data types upon reading in the file, but datetimes appear to be a problem.  For instance:

headers = ['col1', 'col2', '...",https://stackoverflow.com/questions/21269399/datetime-dtypes-in-pandas-read-csv,"[""I'm reading in a csv file with multiple datetime columns.  I'd need to set the data types upon reading in the file, but datetimes appear to be a problem.  For instance:"", 'There is no datetime dtype to be set for read_csv as csv files can only contain strings, integers and floats.', 'There is a parse_dates parameter for read_csv which allows you to define the names of the columns you want treated as dates or datetimes:', 'You might try passing actual types instead of strings.', 'I tried using the dtypes=[datetime, ...] option, but']"
235,Normalize data in pandas,"Suppose I have a pandas data frame df: 

I want to calculate the column wise mean of a data frame.

This is easy: 

df.apply(average) 
then the column wise range max(col) - min(col). This is easy ...",https://stackoverflow.com/questions/12525722/normalize-data-in-pandas,"['Suppose I have a pandas data frame df:', ""If you don't mind importing the sklearn library, I would recommend the method talked on this blog."", ""You can use apply for this, and it's a bit neater:"", 'Slightly modified from: Python Pandas Dataframe: Normalize data between 0.01 and 0.99? but from some of the comments thought it was relevant (sorry if considered a repost though...)', 'This is how you do it column-wise:']"
236,How to form tuple column from two columns in Pandas,"I've got a Pandas DataFrame and I want to combine the 'lat' and 'long' columns to form a tuple.

<class 'pandas.core.frame.DataFrame'>
Int64Index: 205482 entries, 0 to 209018
Data columns:
Month ...",https://stackoverflow.com/questions/16031056/how-to-form-tuple-column-from-two-columns-in-pandas,"[""I've got a Pandas DataFrame and I want to combine the 'lat' and 'long' columns to form a tuple."", 'Get comfortable with zip. It comes in handy when dealing with column data.', 'Pandas has the itertuples method to do exactly this:', ""I'd like to add df.values.tolist(). (as long as you don't mind to get a column of lists rather than tuples)""]"
237,How to add multiple columns to pandas dataframe in one assignment?,I'm new to pandas and trying to figure out how to add multiple columns to pandas simultaneously.  Any help here is appreciated.  Ideally I would like to do this in one step rather than multiple ...,https://stackoverflow.com/questions/39050539/how-to-add-multiple-columns-to-pandas-dataframe-in-one-assignment,"[""I'm new to pandas and trying to figure out how to add multiple columns to pandas simultaneously.  Any help here is appreciated.  Ideally I would like to do this in one step rather than multiple repeated steps..."", ""I would have expected your syntax to work too. The problem arises because when you create new columns with the column-list syntax (df[[new1, new2]] = ...), pandas requires that the right hand side be a DataFrame (note that it doesn't actually matter if the columns of the DataFrame have the same names as the columns you are creating)."", 'You could use assign with a dict of column names and values.', 'With the use of concat:', 'use of list comprehension, pd.DataFrame and pd.concat', 'if adding a lot of missing columns (a, b, c ,....) with the same value, here 0, i did this:', ""Just want to point out that option2 in @Matthias Fripp's answer"", 'If you just want to add empty new columns, reindex will do the job', 'I am not comfortable using ""Index"" and so on...could come up as below', ""You could instantiate the values from a dictionary if you wanted different values for each column & you don't mind making a dictionary on the line before.""]"
238,add a string prefix to each value in a string column using Pandas,"I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).
I already figured out how to kind-of do this and I am currently using:

df.ix[(df['col'] !...",https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas,"['I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).\nI already figured out how to kind-of do this and I am currently using:', 'Example:', 'As an alternative, you can also use an apply combined with format (or better with f-strings) which I find slightly more readable if one e.g. also wants to add a suffix or manipulate the element itself:', 'You can use pandas.Series.map :', ""If you load you table file with dtype=str \nor convert column type to string df['a'] = df['a'].astype(str) \nthen you can use such approach:"", 'Another solution with .loc:']"
239,Conditional Replace Pandas,"I have a DataFrame, and I want to replace the values in a particular column that exceed a value with zero. I had thought this was a way of achieving this:
df[df.my_channel > 20000].my_channel = 0

...",https://stackoverflow.com/questions/21608228/conditional-replace-pandas,"['I have a DataFrame, and I want to replace the values in a particular column that exceed a value with zero. I had thought this was a way of achieving this:', '.ix indexer works okay for pandas version prior to 0.20.0, but since pandas 0.20.0, the .ix indexer is deprecated, so you should avoid using it. Instead, you can use .loc or iloc indexers. You can solve this problem by:', 'Try', 'np.where function works as follows:', 'The reason your original dataframe does not update is because chained indexing may cause you to modify a copy rather than a view of your dataframe. The docs give this advice:', 'I would use lambda function on a Series of a DataFrame like this:', 'Try this:']"
240,How to unnest (explode) a column in a pandas DataFrame?,"I have the following DataFrame where one of the columns is an object (list type cell):

df=pd.DataFrame({'A':[1,2],'B':[[1,2],[1,2]]})
df
Out[458]: 
   A       B
0  1  [1, 2]
1  2  [1, 2]
My expected ...",https://stackoverflow.com/questions/53218931/how-to-unnest-explode-a-column-in-a-pandas-dataframe,"['I have the following DataFrame where one of the columns is an object (list type cell):', ""I know object columns type makes the data hard to convert with a pandas function. When I received the data like this, the first thing that came to mind was to 'flatten' or unnest the columns ."", 'Option 1', 'Exploding a list-like column has been simplified significantly in pandas 0.25 with the addition of the explode() method:', 'One alternative is to apply the meshgrid recipe over the rows of the columns to unnest:', 'My 5 cents:', 'Assume there are multiple columns with different length objects within it', 'Because normally sublist length are different and join/merge is far more computational expensive. I retested the method for different length sublist and more normal columns.', 'I generalized the problem a bit to be applicable to more columns.', 'Something pretty not recommended (at least work in this case):', 'Any opinions on this method I thought of? or is doing both concat and melt considered too ""expensive""?', 'I have another good way to solves this when you have more than one column to explode.', 'In my case with more than one column to explode, and with variables lengths for the arrays that needs to be unnested.', ""Below is a simple function for horizontal explode, based on @BEN_YO's answer.""]"
241,How to keep index when using pandas merge,"I would like to merge two DataFrames, and keep the index from the first frame as the index on the merged dataset.  However, when I do the merge, the resulting DataFrame has integer index.  How can I ...",https://stackoverflow.com/questions/11976503/how-to-keep-index-when-using-pandas-merge,"['I would like to merge two DataFrames, and keep the index from the first frame as the index on the merged dataset.  However, when I do the merge, the resulting DataFrame has integer index.  How can I specify that I want to keep the index from the left data frame?', 'Note that for some left merge operations, you may end up with more rows than in a when there are multiple matches between a and b. In this case, you may need to drop duplicates.', 'You can make a copy of index on left dataframe and do merge.', 'There is a non-pd.merge solution using Series.map and DataFrame.set_index.', 'This allows to preserve the index of df1', ""Think I've come up with a different solution. I was joining the left table on index value and the right table on a column value based off index of left table. What I did was a normal merge:"", 'another simple option is to rename the index to what was before:']"
242,How to estimate how much memory a Pandas' DataFrame will need?,"I have been wondering... If I am reading, say, a 400MB csv file into a pandas dataframe (using read_csv or read_table), is there any way to guesstimate how much memory this will need? Just trying to ...",https://stackoverflow.com/questions/18089667/how-to-estimate-how-much-memory-a-pandas-dataframe-will-need,"['I have been wondering... If I am reading, say, a 400MB csv file into a pandas dataframe (using read_csv or read_table), is there any way to guesstimate how much memory this will need? Just trying to get a better feel of data frames and  memory...', 'df.memory_usage() will return how many bytes each column occupies:', ""Here's a comparison of the different methods - sys.getsizeof(df) is simplest."", 'I thought I would bring some more data to the discussion.', 'You have to do this in reverse.', 'If you know the dtypes of your array then you can directly compute the number of bytes that it will take to store your data + some for the Python objects themselves. A useful attribute of numpy arrays is nbytes. You can get the number of bytes from the arrays in a pandas DataFrame by doing', ""Yes there is. Pandas will store your data in 2 dimensional numpy ndarray structures grouping them by dtypes. ndarray is basically a raw C array of data with a small header. So you can estimate it's size just by multiplying the size of the dtype it contains with the dimensions of the array."", 'This I believe this gives the in-memory size any object in python. Internals need to be checked with regard to pandas and numpy']"
243,pandas convert some columns into rows,"So my dataset has some information by location for n dates. The problem is each date is actually a different column header. For example the CSV looks like

location    name    Jan-2010    Feb-2010    ...",https://stackoverflow.com/questions/28654047/pandas-convert-some-columns-into-rows,"['So my dataset has some information by location for n dates. The problem is each date is actually a different column header. For example the CSV looks like', 'UPDATE\nFrom v0.20, melt is a first order function, you can now use', 'Use set_index with stack for MultiIndex Series, then for DataFrame add reset_index with rename:', 'I guess I found a simpler solution', ""You can add a prefix to your year columns and then feed directly to pd.wide_to_long. I won't pretend this is efficient, but it may in certain situations be more convenient than pd.melt, e.g. when your columns already have an appropriate prefix.""]"
244,Find maximum value of a column and return the corresponding row values using Pandas,"Using Python Pandas I am trying to find the Country & Place with the maximum value.

This returns the maximum value:

data.groupby(['Country','Place'])['Value'].max()
But how do I get the ...",https://stackoverflow.com/questions/15741759/find-maximum-value-of-a-column-and-return-the-corresponding-row-values-using-pan,"['', 'Assuming df has a unique index, this gives the row with the maximum value:', 'This will return the entire row with max value', ""The country and place is the index of the series, if you don't need the index, you can set as_index=False:"", 'I think the easiest way to return a row with the maximum value is by getting its index. argmax() can be used to return the index of the row with the largest value.', ""Use the index attribute of DataFrame. Note that I don't type all the rows in the example."", 'In order to print the Country and Place with maximum value, use the following line of code.', 'You can use:', 'My solution for finding maximum values in columns:', ""I'd recommend using nlargest for better performance and shorter code. import pandas"", 'import pandas\ndf is the data frame you create.', 'I encountered a similar error while trying to import data using pandas, The first column on my dataset had spaces before the start of the words. I removed the spaces and it worked like a charm!!']"
245,Find the column name which has the maximum value for each row,"I have a DataFrame like this one:

In [7]:
frame.head()
Out[7]:
Communications and Search   Business    General Lifestyle
0   0.745763    0.050847    0.118644    0.084746
0   0.333333    0.000000    0....",https://stackoverflow.com/questions/29919306/find-the-column-name-which-has-the-maximum-value-for-each-row,"['I have a DataFrame like this one:', 'You can use idxmax with axis=1 to find the column with the greatest value on each row:', ""And if you want to produce a column containing the name of the column with the maximum value but considering only a subset of columns then you use a variation of @ajcr's answer:"", 'You could apply on dataframe and get argmax() of each row via axis=1']"
246,How to show all of columns name on pandas dataframe?,"I have a dataframe that consist of hundreds of columns, and I need to see all column names.

What I did:

In[37]:
data_all2.columns
The output is:

Out[37]:
Index(['customer_id', 'incoming', '...",https://stackoverflow.com/questions/49188960/how-to-show-all-of-columns-name-on-pandas-dataframe,"['I have a dataframe that consist of hundreds of columns, and I need to see all column names.', 'You can globally set printing options. I think this should work:', 'To obtain all the column names of a DataFrame, df_data in this example, you just need to use the command df_data.columns.values.\nThis will show you a list with all the Column names of your Dataframe', ""In the interactive console, it's easy to do:"", 'This will do the trick. Note the use of display() instead of print.', 'What worked for me was the following:', ""The easiest way I've found is just"", 'To get all column name you can iterate over the data_all2.columns.', 'If you just want to see all the columns you can do something of this sort as a quick fix', 'A quick and dirty solution would be to convert it to a string', 'I had lots of duplicate column names, and once I ran', 'you can try this', ""Not a conventional answer, but I guess you could transpose the dataframe to look at the rows instead of the columns. I use this because I find looking at rows more 'intuitional' than looking at columns:"", 'This is my way. I never try for hundred columns. But I think it works', ""I know it is a repetition but I always end up copy pasting and modifying YOLO's answer:""]"
247,How to write to an existing excel file without overwriting data (using pandas)?,"I use pandas to write to excel file in the following fashion:

import pandas

writer = pandas.ExcelWriter('Masterfile.xlsx') 

data_filtered.to_excel(writer, ""Main"", cols=['Diff1', 'Diff2'])

writer....",https://stackoverflow.com/questions/20219254/how-to-write-to-an-existing-excel-file-without-overwriting-data-using-pandas,"['I use pandas to write to excel file in the following fashion:', 'Pandas docs says it uses openpyxl for xlsx files. Quick look through the code in ExcelWriter gives a clue that something like this might work out:', 'Here is a helper function:', 'With openpyxlversion 2.4.0 and pandasversion 0.19.2, the process @ski came up with gets a bit simpler:', 'Starting in pandas 0.24 you can simplify this with the mode keyword argument of ExcelWriter:', ""I know this is an older thread, but this is the first item you find when searching, and the above solutions don't work if you need to retain charts in a workbook that you already have created. In that case, xlwings is a better option - it allows you to write to the excel book and keeps the charts/chart data."", 'Old question, but I am guessing some people still search for this - so...', 'There is a better solution in pandas 0.24:', 'This works perfectly fine only thing is that formatting of the master file(file to which we add new sheet) is lost.', 'The ""keep_date_col"" hope help you']"
248,"How to test if a string contains one of the substrings in a list, in pandas?","Is there any function that would be the equivalent of a combination of df.isin() and df[col].str.contains()? 

For example, say I have the series
s = pd.Series(['cat','hat','dog','fog','pet']), and I ...",https://stackoverflow.com/questions/26577516/how-to-test-if-a-string-contains-one-of-the-substrings-in-a-list-in-pandas,"['Is there any function that would be the equivalent of a combination of df.isin() and df[col].str.contains()?', 'One option is just to use the regex | character to try to match each of the substrings in the words in your Series s (still using str.contains).', 'You can use str.contains alone with a regex pattern using OR (|):', 'Here is a one line lambda that also works:']"
249,A column-vector y was passed when a 1d array was expected,"I need to fit RandomForestRegressor from sklearn.ensemble.
forest = ensemble.RandomForestRegressor(**RF_tuned_parameters)
model = forest.fit(train_fold, train_y)
yhat = model.predict(test_fold)

This ...",https://stackoverflow.com/questions/34165731/a-column-vector-y-was-passed-when-a-1d-array-was-expected,"['I need to fit RandomForestRegressor from sklearn.ensemble.', 'Change this line:', ""I also encountered this situation when I was trying to train a KNN classifier. but it seems that the warning was gone after I changed:\nknn.fit(X_train,y_train)\nto\nknn.fit(X_train, np.ravel(y_train,order='C'))"", 'I had the same problem. The problem was that the labels were in a column format while it expected it in a row.\nuse np.ravel()', 'use below code:', 'Another way of doing this is to use ravel', 'With neuraxle, you can easily solve this :', 'Y = y.values[:,0]']"
250,Pandas groupby: How to get a union of strings,"I have a dataframe like this:

   A         B       C
0  1  0.749065    This
1  2  0.301084      is
2  3  0.463468       a
3  4  0.643961  random
4  1  0.866521  string
5  2  0.120737       !
Calling ...",https://stackoverflow.com/questions/17841149/pandas-groupby-how-to-get-a-union-of-strings,"['I have a dataframe like this:', 'When you apply your own function, there is not automatic exclusions of non-numeric columns. This is slower, though, than the application of .sum() to the groupby', 'You can use the apply method to apply an arbitrary function to the grouped data.  So if you want a set, apply set.  If you want a list, apply list.', 'You may be able to use the aggregate (or agg) function to concatenate the values. (Untested code)', 'You could try this:', ""Since pandas version 0.25.0 we have named aggregations where we can groupby, aggregate and at the same time assign new names to our columns. This way we won't get the MultiIndex columns, and the column names make more sense given the data they contain:"", 'a simple solution would be :', ""If you'd like to overwrite column B in the dataframe, this should work:"", ""Following @Erfan's good answer, most of the times in an analysis of aggregate values you want the unique possible combinations of these existing character values:""]"
251,How to convert SQL Query result to PANDAS Data Structure?,"Any help on this problem will be greatly appreciated.

So basically I want to run a query to my SQL database and store the returned data as Pandas data structure.

I have attached code for query.

I ...",https://stackoverflow.com/questions/12047193/how-to-convert-sql-query-result-to-pandas-data-structure,"['Any help on this problem will be greatly appreciated.', ""Here's the shortest code that will do the job:"", 'Edit: Mar. 2015', ""If you are using SQLAlchemy's ORM rather than the expression language, you might find yourself wanting to convert an object of type sqlalchemy.orm.query.Query to a Pandas data frame."", 'pandas now has a read_sql function. You definitely want to use that instead.', 'For those that works with the mysql connector you can use this code as a start. (Thanks to @Daniel Velkov)', ""Here's the code I use. Hope this helps."", 'This is a short and crisp answer to your problem:', 'Like Nathan, I often want to dump the results of a sqlalchemy or sqlsoup Query into a Pandas data frame.  My own solution for this is:', 'resoverall is a sqlalchemy ResultProxy object. You can read more about it in the sqlalchemy docs, the latter explains basic usage of working with Engines and Connections. Important here is that resoverall is dict like.', ""Simply use pandas and pyodbc together. You'll have to modify your connection string (connstr) according to your database specifications."", 'This question is old, but I wanted to add my two-cents. I read the question as "" I want to run a query to my [my]SQL database and store the returned data as Pandas data structure [DataFrame].""', 'Here is mine. Just in case if you are using ""pymysql"":', 'pandas.io.sql.write_frame is DEPRECATED.\nhttps://pandas.pydata.org/pandas-docs/version/0.15.2/generated/pandas.io.sql.write_frame.html', 'Long time from last post but maybe it helps someone...', 'best way I do this', 'If the result type is ResultSet, you should convert it to dictionary first. Then the DataFrame columns will be collected automatically.']"
252,How do I find numeric columns in Pandas?,"Let's say df is a pandas DataFrame.
I would like to find all columns of numeric type.
Something like:

isNumeric = is_numeric(df)",https://stackoverflow.com/questions/25039626/how-do-i-find-numeric-columns-in-pandas,"[""Let's say df is a pandas DataFrame.\nI would like to find all columns of numeric type.\nSomething like:"", 'You could use select_dtypes method of DataFrame. It includes two parameters include and exclude. So isNumeric would look like:', 'You can use the undocumented function _get_numeric_data() to filter only numeric columns:', 'Simple one-line answer to create a new dataframe with only numeric columns:', 'Simple one-liner:', 'Following codes will return list of names of the numeric columns of a data set.', 'This is another simple code for finding numeric column in pandas data frame,', 'We can include and exclude data types as per the requirement as below:', 'Please see the below code:', 'Adapting this answer, you could do']"
253,How to create a DataFrame of random integers with Pandas?,"I know that if I use randn,

import pandas as pd
import numpy as np
df = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))
gives me what I am looking for, but with elements from a normal ...",https://stackoverflow.com/questions/32752292/how-to-create-a-dataframe-of-random-integers-with-pandas,"['I know that if I use randn,', 'numpy.random.randint accepts a third argument (size) , in which you can specify the size of the output array. You can use this to create your DataFrame -', 'The recommended way to create random integers with NumPy these days is to use numpy.random.Generator.integers. (documentation)']"
254,When should I (not) want to use pandas apply() in my code?,"I have seen many answers posted to questions on Stack Overflow involving the use of the Pandas method apply. I have also seen users commenting under them saying that ""apply is slow, and should be ...",https://stackoverflow.com/questions/54432583/when-should-i-not-want-to-use-pandas-apply-in-my-code,"['I have seen many answers posted to questions on Stack Overflow involving the use of the Pandas method apply. I have also seen users commenting under them saying that ""apply is slow, and should be avoided"".', 'We start by addressing the questions in the OP, one by one.', 'The below chart suggests when to consider apply1. Green means possibly efficient; red avoid.', ""For axis=1 (i.e. row-wise functions) then you can just use the following function in lieu of apply. I wonder why this isn't the pandas behavior. (Untested with compound indexes, but it does appear to be much faster than apply)"", 'Are there ever any situations where apply is good?\nYes, sometimes.']"
255,Format y axis as percent,"I have an existing plot that was created with pandas like this:

df['myvar'].plot(kind='bar')
The y axis is format as float and I want to change the y axis to percentages.  All of the solutions I ...",https://stackoverflow.com/questions/31357611/format-y-axis-as-percent,"['I have an existing plot that was created with pandas like this:', 'This is a few months late, but I have created PR#6251 with matplotlib to add a new PercentFormatter class. With this class you just need one line to reformat your axis (two if you count the import of matplotlib.ticker):', 'pandas dataframe plot will return the ax for you, And then you can start to manipulate the axes whatever you want.', ""Jianxun's solution did the job for me but broke the y value indicator at the bottom left of the window."", 'For those who are looking for the quick one-liner:', 'I propose an alternative method using seaborn', ""I'm late to the game but I just realize this: ax can be replaced with plt.gca() for those who are not using axes and just subplots.""]"
256,Right way to reverse pandas.DataFrame?,"Here is my code:

import pandas as pd

data = pd.DataFrame({'Odd':[1,3,5,6,7,9], 'Even':[0,2,4,6,8,10]})

for i in reversed(data):
    print(data['Odd'], data['Even'])
When I run this code, i get the ...",https://stackoverflow.com/questions/20444087/right-way-to-reverse-pandas-dataframe,"['Here is my code:', 'or simply:', 'You can reverse the rows in an even simpler way:', 'None of the existing answers resets the index after reversing the dataframe.', 'This works:']"
257,Print very long string completely in pandas dataframe,"I am struggling with the seemingly very simple thing.I have a pandas data frame containing very long string.

df = pd.DataFrame({'one' : ['one', 'two', 
      'This is very long string very long ...",https://stackoverflow.com/questions/29902714/print-very-long-string-completely-in-pandas-dataframe,"['I am struggling with the seemingly very simple thing.I have a pandas data frame containing very long string.', 'You can use options.display.max_colwidth to specify you want to see more in the default representation:', ""Use pd.set_option('display.max_colwidth', -1) for automatic linebreaks and multi-line cells."", 'Another, pretty simple approach is to call  list function:', 'Another easier way to print the whole string is to call values on the dataframe.', 'Is this what you meant to do ?', 'Just add the following line to your code before print.', 'The way I often deal with the situation you describe is to use the .to_csv() method and write to stdout:', 'I have created a small utility function, this works well for me', ""If you're using jupyter notebook, you can also print pandas dataframe as HTML table, which will print full strings."", 'In the newer version of pandas, use:']"
258,Ignoring NaNs with str.contains,"I want to find rows that contain a string, like so:

DF[DF.col.str.contains(""foo"")]
However, this fails because some elements are NaN:
  ValueError: cannot index with vector containing NA / NaN ...",https://stackoverflow.com/questions/28311655/ignoring-nans-with-str-contains,"['I want to find rows that contain a string, like so:', ""There's a flag for that:"", 'In addition to the above answers, I would say for columns having no single word name, you may use:-', ""I'm not 100% on why (actually came here to search for the answer), but this also works, and doesn't require replacing all nan values."", 'You can also patern :', 'DF[DF.col.str.contains(""foo"").fillna(False)]']"
259,Combine Date and Time columns using python pandas,"I have a pandas dataframe with the following columns;

Date              Time
01-06-2013      23:00:00
02-06-2013      01:00:00
02-06-2013      21:00:00
02-06-2013      22:00:00
02-06-2013      23:00:...",https://stackoverflow.com/questions/17978092/combine-date-and-time-columns-using-python-pandas,"['I have a pandas dataframe with the following columns;', ""It's worth mentioning that you may have been able to read this in directly e.g. if you were using read_csv using parse_dates=[['Date', 'Time']]."", 'The accepted answer works for columns that are of datatype string. For completeness: I come across this question when searching how to do this when the columns are of datatypes: date and time.', 'You can use this to merge date and time into the same column of dataframe.', 'You can cast the columns if the types are different (datetime and timestamp or str) and use to_datetime :', ""I don't have enough reputation to comment on jka.ne so:"", 'The answer really depends on what your column types are. In my case, I had datetime and timedelta.', 'You can also convert to datetime without string concatenation, by combining datetime and timedelta objects. Combined with pd.DataFrame.pop, you can remove the source series simultaneously:', 'First make sure to have the right data types:', 'Use the  combine function:', 'My dataset had 1second resolution data for a few days and parsing by the suggested methods here was very slow. Instead I used:', 'DATA:']"
260,Get total of Pandas column,"Target

I have a Pandas data frame, as shown below, with multiple columns and would like to get the total of column, MyColumn.
Data Frame - df:

print df

           X           MyColumn  Y           ...",https://stackoverflow.com/questions/41286569/get-total-of-pandas-column,"['Target', 'You should use sum:', 'Another option you can go with here:', 'Similar to getting the length of a dataframe, len(df), the following worked for pandas and blaze:', 'There are two ways to sum of a column', 'As other option, you can do something like below']"
261,What rules does Pandas use to generate a view vs a copy?,"I'm confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original.

If I have, for example,

df = pd.DataFrame(np....",https://stackoverflow.com/questions/23296282/what-rules-does-pandas-use-to-generate-a-view-vs-a-copy,"[""I'm confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original."", ""Here's the rules, subsequent override:""]"
262,How to suppress Pandas Future warning ?,"When I run the program, Pandas gives 'Future warning' like below every time.

D:\Python\lib\site-packages\pandas\core\frame.py:3581: FutureWarning: rename with inplace=True  will return None from ...",https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning,"[""When I run the program, Pandas gives 'Future warning' like below every time."", 'Found this on github...', ""@bdiamante's answer may only partially help you.  If you still get a message after you've suppressed warnings, it's because the pandas library itself is printing the message.  There's not much you can do about it unless you edit the Pandas source code yourself.  Maybe there's an option internally to suppress them, or a way to override things, but I couldn't find one."", 'Warnings are annoying. As mentioned in other answers, you can suppress them using:']"
263,Python: Convert timedelta to int in a dataframe,I would like to create a column in a pandas data frame that is an integer representation of the number of days in a timedelta column.  Is it possible to use 'datetime.days' or do I need to do ...,https://stackoverflow.com/questions/25646200/python-convert-timedelta-to-int-in-a-dataframe,"[""I would like to create a column in a pandas data frame that is an integer representation of the number of days in a timedelta column.  Is it possible to use 'datetime.days' or do I need to do something more manual?"", 'Use the dt.days attribute. Access this attribute via:', 'You could do this, where td is your series of timedeltas.  The division converts the nanosecond deltas into day deltas, and the conversion to int drops to whole days.', 'Timedelta objects have read-only instance attributes .days, .seconds, and .microseconds.', 'If the question isn\'t just ""how to access an integer form of the timedelta?"" but ""how to convert the timedelta column in the dataframe to an int?"" the answer might be a little different. In addition to the .dt.days accessor you need either df.astype or pd.to_numeric']"
264,Pandas DataFrame to List of Lists,"It's easy to turn a list of lists into a pandas dataframe:

import pandas as pd
df = pd.DataFrame([[1,2,3],[3,4,5]])
But how do I turn df back into a list of lists?

lol = df.what_to_do_now?
print ...",https://stackoverflow.com/questions/28006793/pandas-dataframe-to-list-of-lists,"[""It's easy to turn a list of lists into a pandas dataframe:"", 'You could access the underlying array and call its tolist method:', 'If the data has column and index labels that you want to preserve, there are a few options.', 'I wanted to preserve the index, so I adapted the original answer to this solution:', ""I don't know if it will fit your needs, but you can also do:"", 'Maybe something changed but this gave back a list of ndarrays which did what I needed.', ""Note: I have seen many cases on Stack Overflow where converting a Pandas Series or DataFrame to a NumPy array or plain Python lists is entirely unecessary. If you're new to the library, consider double-checking whether the functionality you need is already offered by those Pandas objects."", 'We can use the DataFrame.iterrows() function to iterate over each of the rows of the given Dataframe and construct a list out of the data of each row:', 'This is very simple:', '""df.values"" returns a numpy array. This does not preserve the data types. An integer might be converted to a float.']"
265,Pretty Printing a pandas dataframe,"How can I print a pandas dataframe as a nice text-based table, like the following?

+------------+---------+-------------+
| column_one | col_two |   column_3  |
+------------+---------+-------------+
...",https://stackoverflow.com/questions/18528533/pretty-printing-a-pandas-dataframe,"['How can I print a pandas dataframe as a nice text-based table, like the following?', ""I've just found a great tool for that need, it is called tabulate."", 'A simple approach is to output as html, which pandas does out of the box:', 'If you want an inbuilt function to dump your data into some github markdown, you now have one. Take a look at to_markdown:', 'If you are in Jupyter notebook, you could run the following code to interactively display the dataframe in a well formatted table.', ""You can use prettytable to render the table as text. The trick is to convert the data_frame to an in-memory csv file and have prettytable read it. Here's the code:"", ""I used Ofer's answer for a while and found it great in most cases. Unfortunately, due to inconsistencies between pandas's to_csv and prettytable's from_csv, I had to use prettytable in a different way."", ""Following up on Mark's answer, if you're not using Jupyter for some reason, e.g. you want to do some quick testing on the console, you can use the DataFrame.to_string method, which works from -- at least -- Pandas 0.12 (2014) onwards."", ""Maybe you're looking for something like this:"", 'I wanted a paper printout of a dataframe but I wanted to add some results and comments as well on the same page.\nI have worked through the above and I could not get what I wanted. I ended up using\nfile.write(df1.to_csv()) and file.write("",,,blah,,,,,,blah"") statements to get my extras on the page.\nWhen I opened the csv file it went straight to a spreadsheet which printed everything in the right pace and format.']"
266,How do I release memory used by a pandas dataframe?,"I have a really large csv file that I opened in pandas as follows....

import pandas
df = pandas.read_csv('large_txt_file.txt')
Once I do this my memory usage increases by 2GB, which is expected ...",https://stackoverflow.com/questions/39100971/how-do-i-release-memory-used-by-a-pandas-dataframe,"['I have a really large csv file that I opened in pandas as follows....', ""Reducing memory usage in Python is difficult, because Python does not actually release memory back to the operating system. If you delete objects, then the memory is available to new Python objects, but not free()'d back to the system (see this question)."", ""As noted in the comments, there are some things to try: gc.collect (@EdChum) may clear stuff, for example. At least from my experience, these things sometimes work and often don't."", 'This solves the problem of releasing the memory for me!!!', 'del df will not be deleted if there are any reference to the df at the time of deletion. So you need to to delete all the references to it with del df to release the memory.', 'It seems there is an issue with glibc that affects the memory allocation in Pandas: https://github.com/pandas-dev/pandas/issues/2659']"
267,How to delete the last row of data of a pandas dataframe,"I think this should be simple, but I tried a few ideas and none of them worked:

last_row = len(DF)
DF = DF.drop(DF.index[last_row])  #<-- fail!
I tried using negative indices but that also lead ...",https://stackoverflow.com/questions/26921651/how-to-delete-the-last-row-of-data-of-a-pandas-dataframe,"['I think this should be simple, but I tried a few ideas and none of them worked:', 'To drop last n rows:', 'where n is the last number of rows to drop.', ""Since index positioning in Python is 0-based, there won't actually be an element in index at the location corresponding to len(DF). You need that to be last_row = len(DF) - 1:"", 'Surprised nobody brought this one up:', 'The Output of stats:', 'drop returns a new array so that is why it choked in the og post; I had a similar requirement to rename some column headers and deleted some rows due to an ill formed csv file converted to Dataframe, so after reading this post I used:', 'For more complex DataFrames that have a Multi-Index (say ""Stock"" and ""Date"") and one wants to remove the last row for each Stock not just the last row of the last Stock, then the solution reads:']"
268,How to change the datetime format in pandas,"My dataframe has a DOB column (example format 1/1/2016) which by default gets converted to pandas dtype 'object': DOB   object

Converting this to date format with df['DOB'] = pd.to_datetime(df['DOB'])...",https://stackoverflow.com/questions/38067704/how-to-change-the-datetime-format-in-pandas,"[""My dataframe has a DOB column (example format 1/1/2016) which by default gets converted to pandas dtype 'object': DOB   object"", 'You can use dt.strftime if you need to convert datetime to other formats (but note that then dtype of column will be object (string)):', 'Changing the format but not changing the type:', 'The below code worked for me instead of the previous one - try it out !', 'Compared to the first answer, I will recommend to use dt.strftime() first, then pd.to_datetime(). In this way, it will still result in the datetime data type.', 'There is a difference between', ""You can try this it'll convert the date format to DD-MM-YYYY:"", ""Below code changes to 'datetime' type and also formats in the given format string. Works well!"", 'Below is the code worked for me, And we need to be very careful for format. Below link will be definitely useful for knowing your exiting format and changing into desired format(Follow strftime() and strptime() Format Codes on below link):']"
269,How to add title to seaborn boxplot,"Seems pretty Googleable but haven't been able to find something online that works.

I've tried both sns.boxplot('Day', 'Count', data= gg).title('lalala') and sns.boxplot('Day', 'Count', data= gg)....",https://stackoverflow.com/questions/42406233/how-to-add-title-to-seaborn-boxplot,"[""Seems pretty Googleable but haven't been able to find something online that works."", 'Seaborn box plot returns a matplotlib axes instance. Unlike pyplot itself, which has a method plt.title(), the corresponding argument for an axes is ax.set_title(). Therefore you need to call', 'Try adding this at the end of your code:', ""sns.boxplot() function returns Axes(matplotlib.axes.Axes) object. please refer the documentation\nyou can add title using 'set' method as below:"", 'For a single boxplot:', "".set_title('') can be used to add title to Seaborn Plot""]"
270,Could pandas use column as index?,"I have a spreadsheet like this:

Locality    2005    2006    2007    2008    2009

ABBOTSFORD  427000  448000  602500  600000  638500
ABERFELDIE  534000  600000  735000  710000  775000
AIREYS ...",https://stackoverflow.com/questions/38542419/could-pandas-use-column-as-index,"['I have a spreadsheet like this:', 'Yes, with set_index you can make Locality your row index.', ""You can change the index as explained already using set_index.\nYou don't need to manually swap rows with columns, there is a transpose (data.T) method in pandas that does it for you:"", 'You can set the column index using index_col parameter available while reading from spreadsheet in Pandas.']"
271,Creating dataframe from a dictionary where entries have different lengths,"Say I have a dictionary with 10 key-value pairs. Each entry holds a numpy array. However, the length of the array is not the same for all of them.

How can I create a dataframe where each column holds ...",https://stackoverflow.com/questions/19736080/creating-dataframe-from-a-dictionary-where-entries-have-different-lengths,"['Say I have a dictionary with 10 key-value pairs. Each entry holds a numpy array. However, the length of the array is not the same for all of them.', 'In Python 3.x:', ""Here's a simple way to do that:"", 'A way of tidying up your syntax, but still do essentially the same thing as these other answers, is below:', ""While this does not directly answer the OP's question. I found this to be an excellent solution for my case when I had unequal arrays and I'd like to share:"", 'You can also use pd.concat along axis=1 with a list of pd.Series objects:', 'Both the following lines work perfectly :', ""If you don't want it to show NaN and you have two particular lengths, adding a 'space' in each remaining cell would also work."", '', 'pd.DataFrame([my_dict]) will do!']"
272,How to drop rows from pandas data frame that contains a particular string in a particular column? [duplicate],"I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.

For example, I want to drop all rows which have the string ""XYZ"" as a ...",https://stackoverflow.com/questions/28679930/how-to-drop-rows-from-pandas-data-frame-that-contains-a-particular-string-in-a-p,"['I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.', ""pandas has vectorized string operations, so you can just filter out the rows that contain the string you don't want:"", 'If your string constraint is not just one string you can drop those corresponding rows with:', 'This will only work if you want to compare exact strings.\nIt will not work in case you want to check if the column string contains any of the strings in the list.', 'Slight modification to the code. Having na=False will skip empty values. Otherwise you can get an error TypeError: bad operand type for unary ~: float', 'Reference: https://chrisalbon.com/python/data_wrangling/pandas_dropping_column_and_rows/', 'The below code will give you list of all the rows:-', 'if you do not want to delete all NaN, use']"
273,How to convert a Scikit-learn dataset to a Pandas dataset?,"How do I convert data from a Scikit-learn Bunch object to a Pandas DataFrame?

from sklearn.datasets import load_iris
import pandas as pd
data = load_iris()
print(type(data))
data1 = pd. # Is there a ...",https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset,"['How do I convert data from a Scikit-learn Bunch object to a Pandas DataFrame?', 'Manually, you can use pd.DataFrame constructor, giving a numpy array (data) and a list of the names of the columns (columns).\nTo have everything in one DataFrame, you can concatenate the features and the target into one numpy array with np.c_[...] (note the []):', 'This tutorial maybe of interest: http://www.neural.cz/dataset-exploration-boston-house-pricing.html', ""TOMDLt's solution is not generic enough for all the datasets in scikit-learn. For example it does not work for the boston housing dataset. I propose a different solution which is more universal. No need to use numpy as well."", 'Took me 2 hours to figure this out', 'Just as an alternative that I could wrap my head around much easier:', 'Otherwise use seaborn data sets which are actual pandas data frames:', 'This works for me.', 'Other way to combine features and target variables can be using np.column_stack (details)', 'Basically what you need is the ""data"", and you have it in the scikit bunch, now you need just the ""target"" (prediction) which is also in the bunch.', 'As of version 0.23, you can directly return a DataFrame using the as_frame argument. \nFor example, loading the iris data set:', 'You can use the parameter as_frame=True to get pandas dataframes.', 'This is easy method worked for me.', 'Working off the best answer and addressing my comment, here is a function for the conversion', 'Whatever TomDLT answered it may not work for some of you because', 'There might be a better way but here is what I have done in the past and it works quite well:', 'This snippet is only syntactic sugar built upon what TomDLT and rolyat have already contributed and explained. The only differences would be that load_iris will return a tuple instead of a dictionary and the columns names are enumerated.', 'One of the best ways:', ""I took couple of ideas from your answers and I don't know how to make it shorter :)"", 'The API is a little cleaner than the responses suggested. Here, using as_frame and being sure to include a response column as well.', ""Here's another integrated method example maybe helpful.""]"
274,python pandas dataframe to dictionary,"I've a two columns dataframe, and intend to convert it to python dictionary - the first column will be the key and the second will be the value. Thank you in advance. 

Dataframe:

    id    value
0   ...",https://stackoverflow.com/questions/18695605/python-pandas-dataframe-to-dictionary,"[""I've a two columns dataframe, and intend to convert it to python dictionary - the first column will be the key and the second will be the value. Thank you in advance."", 'See the docs for to_dict. You can use it like this:', 'If you want a simple way to preserve duplicates, you could use groupby:', 'The answers by joris in this thread and by punchagan in the duplicated thread are very elegant, however they will not give correct results if the column used for the keys contains any duplicated value.', 'Simplest solution:', 'in some versions  the code below might not work', ""You can use 'dict comprehension'"", 'Another (slightly shorter) solution for not losing duplicate entries:', ""I found this question while trying to make a dictionary out of three columns of a pandas dataframe. In my case the dataframe has columns A, B and C (let's say A and B are the geographical coordinates of longitude and latitude and C the country region/state/etc, which is more or less the case)."", 'You need a list as a dictionary value. This code will do the trick.', 'this is my sloution, a basic loop', 'This is my solution:']"
275,Converting strings to floats in a DataFrame,How to covert a DataFrame column containing strings and NaN values to floats. And there is another column whose values are strings and floats; how to convert this entire column to floats.,https://stackoverflow.com/questions/16729483/converting-strings-to-floats-in-a-dataframe,"['How to covert a DataFrame column containing strings and NaN values to floats. And there is another column whose values are strings and floats; how to convert this entire column to floats.', 'NOTE: pd.convert_objects has now been deprecated. You should use pd.Series.astype(float) or pd.to_numeric as described in other\n  answers.', 'You can try df.column_name = df.column_name.astype(float). As for the NaN values, you need to specify how they should be converted, but you can use the .fillna method to do it.', ""In a newer version of pandas (0.17 and up), you can use to_numeric function. It allows you to convert the whole dataframe or just individual columns. It also gives you an ability to select how to treat stuff that can't be converted to numeric values:"", ""you have to replace empty strings ('') with np.nan before converting to float. ie:"", 'Here is an example']"
276,Change one value based on another value in pandas,"I'm trying to reprogram my Stata code into Python for speed improvements, and I was pointed in the direction of PANDAS.  I am, however, having a hard time wrapping my head around how to process the ...",https://stackoverflow.com/questions/19226488/change-one-value-based-on-another-value-in-pandas,"[""I'm trying to reprogram my Stata code into Python for speed improvements, and I was pointed in the direction of PANDAS.  I am, however, having a hard time wrapping my head around how to process the data."", ""One option is to use Python's slicing and indexing features to logically evaluate the places where your condition holds and overwrite the data there."", 'You can use map, it can map vales from a dictonairy or even a custom function.', 'The original question addresses a specific narrow use case. For those who need more generic answers here are some examples:', ""This question might still be visited often enough that it's worth offering an addendum to Mr Kassies' answer. The dict built-in class can be sub-classed so that a default is returned for 'missing' keys. This mechanism works well for pandas. But see below.""]"
277,Read a zipped file as a pandas DataFrame,"I'm trying to unzip a csv file and pass it into pandas so I can work on the file.
The code I have tried so far is: 

import requests, zipfile, StringIO
r = requests.get('http://data.octo.dc.gov/feeds/...",https://stackoverflow.com/questions/18885175/read-a-zipped-file-as-a-pandas-dataframe,"[""I'm trying to unzip a csv file and pass it into pandas so I can work on the file.\nThe code I have tried so far is:"", 'If you want to read a zipped or a tar.gz file into pandas dataframe, the read_csv methods includes this particular implementation.', 'I think you want to open the ZipFile, which returns a file-like object,  rather than read:', ""It seems you don't even have to specify the compression any more. The following snippet loads the data from filename.zip into df."", 'For ""zip"" files, you can use import zipfile and your code will be working simply with these lines:', 'https://www.kaggle.com/jboysen/quick-gz-pandas-tutorial']"
278,Making heatmap from pandas DataFrame,"I have a dataframe generated from Python's Pandas package. How can I generate  heatmap using DataFrame from pandas package. 

import numpy as np 
from pandas import *

Index= ['aaa','bbb','ccc','ddd','...",https://stackoverflow.com/questions/12286607/making-heatmap-from-pandas-dataframe,"[""I have a dataframe generated from Python's Pandas package. How can I generate  heatmap using DataFrame from pandas package."", 'You want matplotlib.pcolor:', 'For people looking at this today, I would recommend the Seaborn heatmap() as documented here.', 'If you don\'t need a plot per say, and you\'re simply interested in adding color to represent the values in a table format, you can use the style.background_gradient() method of the pandas data frame. This method colorizes the HTML table that is displayed when viewing pandas data frames in e.g. the JupyterLab Notebook and the result is similar to using ""conditional formatting"" in spreadsheet software:', 'Useful sns.heatmap api is here.  Check out the parameters, there are a good number of them.  Example:', 'If you want an interactive heatmap from a Pandas DataFrame and you are running a Jupyter notebook, you can try the interactive Widget Clustergrammer-Widget, see interactive notebook on NBViewer here, documentation here', ""Please note that the authors of seaborn only want seaborn.heatmap to work with categorical dataframes. It's not general.""]"
279,Convert unix time to readable date in pandas dataframe,"I have a dataframe with unix times and prices in it. I want to convert the index column so that it shows in human readable dates. 

So for instance I have date as 1349633705 in the index column but I'...",https://stackoverflow.com/questions/19231871/convert-unix-time-to-readable-date-in-pandas-dataframe,"['I have a dataframe with unix times and prices in it. I want to convert the index column so that it shows in human readable dates.', 'These appear to be seconds since epoch.', 'If you try using:', 'Assuming we imported pandas as pd and df is our dataframe', 'Alternatively, by changing a line of the above code:']"
280,"Convert row to column header for Pandas DataFrame,","The data I have to work with is a bit messy.. It has header names inside of its data. How can I choose a row from an existing pandas dataframe and make it (rename it to) a column header?

I want to do ...",https://stackoverflow.com/questions/26147180/convert-row-to-column-header-for-pandas-dataframe,"['The data I have to work with is a bit messy.. It has header names inside of its data. How can I choose a row from an existing pandas dataframe and make it (rename it to) a column header?', 'Set the column labels to equal the values in the 2nd row (index location 1):', ""This works (pandas v'0.19.2'):"", 'It would be easier to recreate the data frame.\nThis would also interpret the columns types from scratch.', 'You can specify the row index in the read_csv or read_html constructors via the header parameter which represents Row number(s) to use as the column names, and the start of the data. This has the advantage of automatically dropping all the preceding rows which supposedly are junk.', 'To rename the header without reassign df:']"
281,Assign pandas dataframe column dtypes,"I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)

import pandas as pd
...",https://stackoverflow.com/questions/21197774/assign-pandas-dataframe-column-dtypes,"[""I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)"", 'Since 0.17, you have to use the explicit conversions:', 'For those coming from Google (etc.) such as myself:', 'you can set the types explicitly with pandas DataFrame.astype(dtype, copy=True, raise_on_error=True, **kwargs) and pass in a dictionary with the dtypes you want to dtype', 'Another way to set the column types is to first construct a numpy record array with your desired types, fill it out and then pass it to a DataFrame constructor.', ""facing similar problem to you. In my case I have 1000's of files from cisco logs that I need to parse manually."", ""You're better off using typed np.arrays, and then pass the data and column names as a dictionary.""]"
282,Appending a list or series to a pandas DataFrame as a row?,So I have initialized an empty pandas DataFrame and I would like to iteratively append lists (or Series) as rows in this DataFrame. What is the best way of doing this?,https://stackoverflow.com/questions/26309962/appending-a-list-or-series-to-a-pandas-dataframe-as-a-row,"['So I have initialized an empty pandas DataFrame and I would like to iteratively append lists (or Series) as rows in this DataFrame. What is the best way of doing this?', ""Sometimes it's easier to do all the appending outside of pandas, then, just create the DataFrame in one shot."", ""Here's a simple and dumb solution:"", 'Could you do something like this?', ""Following onto Mike Chirico's answer...  if you want to append a list after the dataframe is already populated..."", ""Here's a function that, given an already created dataframe, will append a list as a new row. This should probably have error catchers thrown in, but if you know exactly what you're adding then it shouldn't be an issue."", ""If you want to add a Series and use the Series' index as columns of the DataFrame, you only need to append the Series between brackets:"", 'Converting the list to a data frame within the append function works, also when applied in a loop', 'simply use loc:', ""As mentioned here - https://kite.com/python/answers/how-to-append-a-list-as-a-row-to-a-pandas-dataframe-in-python, you'll need to first convert the list to a series then append the series to dataframe."", 'The simplest way:']"
283,Select Pandas rows based on list index,"I have a dataframe df :

   20060930  10.103       NaN     10.103   7.981
   20061231  15.915       NaN     15.915  12.686
   20070331   3.196       NaN      3.196   2.710
   20070630   7.907       ...",https://stackoverflow.com/questions/19155718/select-pandas-rows-based-on-list-index,"['I have a dataframe df :', 'should do the trick!\nWhen I index with data frames I always use the .ix() method. Its so much easier and more flexible...', 'you can also use iloc:', 'Another way (although it is a longer code) but it is faster than the above codes. Check it using %timeit function:', 'For large datasets, it is memory efficient to read only selected rows via the skiprows parameter.']"
284,extract column value based on another column pandas dataframe,"I am kind of getting stuck on extracting value of one variable conditioning on another variable. For example, the following dataframe:

A  B
p1 1
p1 2
p3 3
p2 4
How can I get the value of A when B=3? ...",https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe,"['I am kind of getting stuck on extracting value of one variable conditioning on another variable. For example, the following dataframe:', 'You could use loc to get series which satisfying your condition and then iloc to get first element:', 'You can try query, which is less typing:', ""df[df['B']==3]['A'], assuming df is your pandas.DataFrame."", ""Use df[df['B']==3]['A'].values if you just want item itself without the brackets"", 'I have also worked on this clausing and extraction operations for my assignment.', ""It's easier for me to think in these terms, but borrowing from other answers.  The value you want is located in the series:""]"
285,Insert a row to pandas dataframe,"I have a dataframe:

s1 = pd.Series([5, 6, 7])
s2 = pd.Series([7, 8, 9])

df = pd.DataFrame([list(s1), list(s2)],  columns =  [""A"", ""B"", ""C""])

   A  B  C
0  5  6  7
1  7  8  9

[2 rows x 3 columns]
...",https://stackoverflow.com/questions/24284342/insert-a-row-to-pandas-dataframe,"['I have a dataframe:', 'Just assign row to a particular index, using loc:', 'Not sure how you were calling concat() but it should work as long as both objects are of the same type. Maybe the issue is that you need to cast your second vector to a dataframe? Using the df that you defined the following works for me:', 'One way to achieve this is', 'I put together a short function that allows for a little more flexibility when inserting a row:', 'We can use numpy.insert. This has the advantage of flexibility. You only need to specify the index you want to insert to.', ""this might seem overly simple but its incredible that a simple insert new row function isn't built in. i've read a lot about appending a new df to the original, but i'm wondering if this would be faster."", 'Below would be the best way to insert a row into pandas dataframe without sorting and reseting an index:', 'It is pretty simple to add a row into a pandas DataFrame:', 'concat() seems to be a bit faster than last row insertion and reindexing.\nIn case someone would wonder about the speed of two top approaches:', 'You can simply append the row to the end of the DataFrame, and then adjust the index.', 'The simplest way add a row in a pandas data frame is:']"
286,Python Pandas merge only certain columns,"Is it possible to only merge some columns? I have a DataFrame df1 with columns x, y, z, and df2 with columns x, a ,b, c, d, e, f, etc.

I want to merge the two DataFrames on x, but I only want to ...",https://stackoverflow.com/questions/17978133/python-pandas-merge-only-certain-columns,"['Is it possible to only merge some columns? I have a DataFrame df1 with columns x, y, z, and df2 with columns x, a ,b, c, d, e, f, etc.', 'You could merge the sub-DataFrame (with just those columns):', 'You want to use TWO brackets, so if you are doing a VLOOKUP sort of action:', 'If you want to drop column(s) from the target data frame, but the column(s) are required for the join, you can do the following:', 'You can use .loc to select the specific columns with all rows and then pull that. An example is below:', 'This is to merge selected columns from two tables.']"
287,"What is dtype('O'), in pandas?","I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get;

dtype('O')...",https://stackoverflow.com/questions/37561991/what-is-dtypeo-in-pandas,"[""I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get;"", 'It means:', 'What is dtype?', 'It means ""a python object"", i.e. not one of the builtin scalar types supported by numpy.', ""'O' stands for object.""]"
288,Return multiple columns from pandas apply(),"I have a pandas DataFrame, df_test.  It contains a column 'size' which represents size in bytes.  I've calculated KB, MB, and GB using the following code:

df_test = pd.DataFrame([
    {'dir': '/Users/...",https://stackoverflow.com/questions/23586510/return-multiple-columns-from-pandas-apply,"[""I have a pandas DataFrame, df_test.  It contains a column 'size' which represents size in bytes.  I've calculated KB, MB, and GB using the following code:"", 'This is an old question, but for completeness, you can return a Series from the applied function that contains the new data, preventing the need to iterate three times.  Passing axis=1 to the apply function applies the function sizes to each row of the dataframe, returning a series to add to a new dataframe.  This series, s, contains the new values, as well as the original data.', 'Use apply and zip will 3 times fast than Series way.', 'Some of the current replies work fine, but I want to offer another, maybe more ""pandifyed"" option. This works for me with the current pandas 0.23 (not sure if it will work in previous versions):', 'Just another readable way. This code will add three new columns and its values, returning series without use parameters in the apply function.', 'Really cool answers! Thanks Jesse and jaumebonet! Just some observation in regards to:', ""The performance between the top answers is significantly varied, and Jesse & famaral42 have already discussed this, but it is worth sharing a fair comparison between the top answers, and elaborating on a subtle but important detail of Jesse's answer: the argument passed in to the function, also affects performance."", 'I believe the 1.1 version breaks the behavior suggested in the top answer here.', 'Generally, to return multiple values, this is what I do', 'It gives a new dataframe with two columns from the original one.']"
289,Set order of columns in pandas dataframe,"Is there a way to reorder columns in pandas dataframe based on my personal preference (i.e. not alphabetically or numerically sorted, but more like following certain conventions)?

Simple example:

...",https://stackoverflow.com/questions/41968732/set-order-of-columns-in-pandas-dataframe,"['Is there a way to reorder columns in pandas dataframe based on my personal preference (i.e. not alphabetically or numerically sorted, but more like following certain conventions)?', 'Just select the order yourself by typing in the column names. Note the double brackets:', 'You can use this:', 'Here is a solution I use very often. When you have a large data set with tons of columns, you definitely do not want to manually rearrange all the columns.', ""You could also do something like df = df[['x', 'y', 'a', 'b']]"", 'Construct it with a list instead of a dictionary', 'You can also use OrderedDict:', ""Add the 'columns' parameter:"", 'Try indexing (so you want a generic solution not only for this, so index order can be just what you want):', 'I find this to be the most straightforward and working:']"
290,Drop columns whose name contains a specific string from pandas DataFrame,"I have a pandas dataframe with the following column names:

Result1, Test1, Result2, Test2, Result3, Test3, etc...

I want to drop all the columns whose name contains the word ""Test"". The numbers of ...",https://stackoverflow.com/questions/19071199/drop-columns-whose-name-contains-a-specific-string-from-pandas-dataframe,"['I have a pandas dataframe with the following column names:', 'Here is one way to do this:', 'In recent versions of pandas, you can use string methods on the index and columns. Here, str.startswith seems like a good fit.', ""You can filter out the columns you DO want using 'filter'"", 'This can be done neatly in one line with:', 'Use the DataFrame.select method:', 'This method does everything in place. Many of the other answers create copies and are not as efficient:', ""Don't drop. Catch the opposite of what you want."", 'the shortest way to do is is :', 'Question states \'I want to drop all the columns whose name contains the word ""Test"".\'', ""Solution when dropping a list of column names containing regex. I prefer this approach because I'm frequently editing the drop list. Uses a negative filter regex for the drop list.""]"
291,Output data from all columns in a dataframe in pandas [duplicate],"I have a csv file with the name params.csv. I opened up ipython qtconsole and created a pandas dataframe using:

import pandas
paramdata = pandas.read_csv('params.csv', names=paramnames)
where, ...",https://stackoverflow.com/questions/11361985/output-data-from-all-columns-in-a-dataframe-in-pandas,"['I have a csv file with the name params.csv. I opened up ipython qtconsole and created a pandas dataframe using:', 'There is too much data to be displayed on the screen, therefore a summary is displayed instead.', 'Use:', 'I know this is an old question, but I have just had a similar problem and I think what I did would work for you too.', 'In ipython, I use this to print a part of the dataframe that works quite well (prints the first 100 rows):', 'you can also use DataFrame.head(x) / .tail(x) to display the first / last x rows of the DataFrame.', ""I'm coming to python from R, and R's head() function wraps lines in a really convenient way for looking at data:"", 'you can use sequence slicing syntax i.e']"
292,Run an OLS regression with Pandas Data Frame,"I have a pandas data frame and I would like to able to predict the values of column A from the values in columns B and C. Here is a toy example:

import pandas as pd
df = pd.DataFrame({""A"": [10,20,30,...",https://stackoverflow.com/questions/19991445/run-an-ols-regression-with-pandas-data-frame,"['I have a pandas data frame and I would like to able to predict the values of column A from the values in columns B and C. Here is a toy example:', ""I think you can almost do exactly what you thought would be ideal, using the statsmodels package which was one of pandas' optional dependencies before pandas' version 0.20.0 (it was used for a few things in pandas.stats.)"", 'Note: pandas.stats has been removed with 0.20.0', ""I don't know if this is new in sklearn or pandas, but I'm able to pass the data frame directly to sklearn without converting the data frame to a numpy array or any other data types."", 'This would require me to reformat the data into lists inside lists, which seems to defeat the purpose of using pandas in the first place.', 'Statsmodels kan build an OLS model with column references directly to a pandas dataframe.']"
293,cartesian product in pandas,"I have two pandas dataframes:

from pandas import DataFrame
df1 = DataFrame({'col1':[1,2],'col2':[3,4]})
df2 = DataFrame({'col3':[5,6]})     
What is the best practice to get their cartesian product (...",https://stackoverflow.com/questions/13269890/cartesian-product-in-pandas,"['I have two pandas dataframes:', 'If you have a key that is repeated for each row, then you can produce a cartesian product using merge (like you would in SQL).', ""Use pd.MultiIndex.from_product as an index in an otherwise empty dataframe, then reset its index, and you're done."", ""This won't win a code golf competition, and borrows from the previous answers - but clearly shows how the key is added, and how the join works. This creates 2 new data frames from lists, then adds the key to do the cartesian product on."", ""Minimal code needed for this one. Create a common 'key' to cartesian merge the two:"", 'With method chaining:', 'As an alternative, one can rely on the cartesian product provided by itertools: itertools.product, which avoids creating a temporary key or modifying the index:', ""If you have no overlapping columns, don't want to add one, and the indices of the data frames can be discarded, this may be easier:"", 'Here is a helper function to perform a simple Cartesian product with two data frames. The internal logic handles using an internal key, and avoids mangling any columns that happen to be named ""key"" from either side.', 'You could start by taking the Cartesian product of df1.col1 and df2.col3, then merge back to df1 to get col2.', 'You can use numpy as it could be faster. Suppose you have two series as follows,', 'I find using pandas MultiIndex to be the best tool for the job. If you have a list of lists lists_list, call pd.MultiIndex.from_product(lists_list) and iterate over the result (or use it in DataFrame index).']"
294,Pandas every nth row,Dataframe.resample() works only with timeseries data. I cannot find a way of getting every nth row from non-timeseries data. What is the best method?,https://stackoverflow.com/questions/25055712/pandas-every-nth-row,"['Dataframe.resample() works only with timeseries data. I cannot find a way of getting every nth row from non-timeseries data. What is the best method?', ""I'd use iloc, which takes a row/column slice, both based on integer position and following normal python syntax."", ""Though @chrisb's accepted answer does answer the question, I would like to add to it the following."", 'There is an even simpler solution to the accepted answer that involves directly invoking df.__getitem__.', ""I had a similar requirement, but I wanted the n'th item in a particular group. This is how I solved it."", 'A solution I came up with when using the index was not viable ( possibly the multi-Gig .csv was too large, or I missed some technique that would allow me to reindex without crashing ).\nWalk through one row at a time and add the nth row to a new dataframe.']"
295,Are for-loops in pandas really bad? When should I care?,"Are for loops really ""bad""? If not, in what situation(s) would they be better than using a more conventional ""vectorized"" approach?1

I am familiar with the concept of ""vectorization"", and how pandas ...",https://stackoverflow.com/questions/54028199/are-for-loops-in-pandas-really-bad-when-should-i-care,"['Are for loops really ""bad""? If not, in what situation(s) would they be better than using a more conventional ""vectorized"" approach?1', 'TLDR; No, for loops are not blanket ""bad"", at least, not always. It is probably more accurate to say that some vectorized operations are slower than iterating, versus saying that iteration is faster than some vectorized operations. Knowing when and why is key to getting the most performance out of your code. In a nutshell, these are the situations where it is worth considering an alternative to vectorized pandas functions:', 'In short']"
296,How do I combine two data frames?,"I'm using Pandas data frames. I have a initial data frame, say D. I extract two data frames from it like this:
A = D[D.label == k]
B = D[D.label != k]

I want to combine A and B so I can have them as ...",https://stackoverflow.com/questions/12850345/how-do-i-combine-two-data-frames,"[""I'm using Pandas data frames. I have a initial data frame, say D. I extract two data frames from it like this:"", 'I believe you can use the append method', 'You can also use pd.concat, which is particularly helpful when you are joining more than two dataframes:', 'Thought to add this here in case someone finds it useful. @ostrokach already mentioned how you can merge the data frames across rows which is', ""There's another solution for the case that you are working with big data and need to concatenate multiple datasets. concat can get performance-intensive, so if you don't want to create a new df each time, you can instead use a list comprehension:"", 'If you want to update/replace the values of first dataframe df1 with the values of second dataframe df2. you can do it by following steps —', '1st dataFrame']"
297,Converting a column within pandas dataframe from int to string,"I have a dataframe in pandas with mixed int and str data columns. I want to concatenate first the columns within the dataframe. To do that I have to convert an int column to str. 
I've tried to do as ...",https://stackoverflow.com/questions/17950374/converting-a-column-within-pandas-dataframe-from-int-to-string,"[""I have a dataframe in pandas with mixed int and str data columns. I want to concatenate first the columns within the dataframe. To do that I have to convert an int column to str. \nI've tried to do as follows:"", 'Convert a series', 'Change data type of DataFrame column:', 'Warning: Both solutions given ( astype() and apply() ) do not preserve NULL values in either the nan or the None form.', 'Use the following code:', 'Just for an additional reference.']"
298,pandas: best way to select all columns whose names start with X,"I have a DataFrame:

import pandas as pd
import numpy as np

df = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8],
                   'foo.fighters': [0, 1, np.nan, 0, 0, 0],
                  ...",https://stackoverflow.com/questions/27275236/pandas-best-way-to-select-all-columns-whose-names-start-with-x,"['I have a DataFrame:', 'Just perform a list comprehension to create your columns:', ""Now that pandas' indexes support string operations, arguably the simplest and best way to select columns beginning with 'foo' is just:"", 'The simplest way is to use str directly on column names, there is no need for pd.Series', ""Based on @EdChum's answer, you can try the following solution:"", 'You can try the regex here to filter out the columns starting with ""foo""', 'My solution. It may be slower on performance:', 'Another option for the selection of the desired entries is to use map:', 'In my case I needed a list of prefixes']"
299,How to read a .xlsx file using the pandas Library in iPython?,"I want to read a .xlsx file using the Pandas Library of python and port the data to a postgreSQL table. 

All I could do up until now is:

import pandas as pd
data = pd.ExcelFile(""*File Name*"")
Now I ...",https://stackoverflow.com/questions/16888888/how-to-read-a-xlsx-file-using-the-pandas-library-in-ipython,"['I want to read a .xlsx file using the Pandas Library of python and port the data to a postgreSQL table.', 'I usually create a dictionary containing a DataFrame for every sheet:', ""DataFrame's read_excel method is like read_csv method:"", ""Instead of using a sheet name, in case you don't know or can't open the excel file to check in ubuntu (in my case, Python 3.6.7, ubuntu 18.04), I use the parameter index_col (index_col=0 for the first sheet)"", 'Assign spreadsheet filename to file', 'If you use read_excel() on a file opened using the function open(), make sure to add rb to the open function to avoid encoding errors']"
300,How to “select distinct” across multiple data frame columns in pandas?,"I'm looking for a way to do the equivalent to the SQL 

SELECT DISTINCT col1, col2 FROM dataframe_table
The pandas sql comparison doesn't have anything about distinct.

.unique() only works for a ...",https://stackoverflow.com/questions/30530663/how-to-select-distinct-across-multiple-data-frame-columns-in-pandas,"[""I'm looking for a way to do the equivalent to the SQL"", 'You can use the drop_duplicates method to get the unique rows in a DataFrame:', ""I've tried different solutions. First was:"", 'There is no unique method for a df, if the number of unique values for each column were the same then the following would work: df.apply(pd.Series.unique) but if not then you will get an error. Another approach would be to store the values in a dict which is keyed on the column name:', ""To solve a similar problem, I'm using groupby:"", 'I think use drop duplicate sometimes will not so useful depending dataframe.', 'You can take the sets of the columns and just subtract the smaller set from the larger set:']"
